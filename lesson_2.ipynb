{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3e89c5b3",
   "metadata": {},
   "source": [
    "<img align=\"left\" src=\"https://ithaka-labs.s3.amazonaws.com/static-files/images/tdm/tdmdocs/tapi-logo-small.png\" />\n",
    "\n",
    "This notebook is free for educational reuse under [Creative Commons CC BY License](https://creativecommons.org/licenses/by/4.0/).\n",
    "\n",
    "Created by [Erik Fredner](https://fredner.org) for the 2024 Text Analysis Pedagogy Institute, with support from [Constellate](https://constellate.org).\n",
    "\n",
    "For questions/comments/improvements, email erik@fredner.org<br />\n",
    "____"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68f932d1",
   "metadata": {},
   "source": [
    "# Automated Text Classification Using LLMs\n",
    "\n",
    "This is lesson 2 of 3 in the educational series on using large language models (LLMs) for text classification. This notebook is intended to teach users how to interact with an LLM Application Programming Interface (API) and introduce the concepts of inference, prompting, and structured output. \n",
    "\n",
    "**Skills:** \n",
    "* Python\n",
    "* Text analysis\n",
    "* Text classification\n",
    "* LLMs\n",
    "* JSON\n",
    "* APIs\n",
    "\n",
    "**Audience:**\n",
    "Researchers\n",
    "\n",
    "**Use case:**\n",
    "Tutorial\n",
    "\n",
    "**Difficulty:**\n",
    "Intermediate. This assumes users are familiar with Python and have been programming for 6+ months. Code makes up a larger part of the notebook and basic concepts related to Python are not explained.\n",
    "\n",
    "**Completion time:**\n",
    "90 minutes\n",
    "\n",
    "**Knowledge Required:** \n",
    "* Python basics (variables, flow control, functions, lists, dictionaries)\n",
    "* `pandas` basics\n",
    "\n",
    "**Knowledge Recommended:**\n",
    "* Experience using LLMs (e.g., ChatGPT)\n",
    "\n",
    "**Learning Objectives:**\n",
    "After this lesson, learners will be able to:\n",
    "\n",
    "1. Describe how to evaluate automated LLM classifications.\n",
    "2. Create data to evaluate LLM classifications.\n",
    "3. Characterize the [F-score](https://en.wikipedia.org/wiki/F-score).\n",
    "4. Combine the ideas above to evaluate multiple prompts.\n",
    "\n",
    "**Research Pipeline:**\n",
    "1. Play with LLMs if you have not already.\n",
    "2. Test using a chatbot interface for an LLM (like ChatGPT) to perform relevant classifications for your research.\n",
    "3. Evaluate initial results.\n",
    "4. Learn how to interact with an API through this notebook.\n",
    "5. Modify your initial experiments based on what we cover."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "157c0555",
   "metadata": {},
   "source": [
    "# Required Python Libraries\n",
    "\n",
    "* [OpenAI](https://pypi.org/project/openai/) to interact with the OpenAI API for ChatGPT."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ff5ae40-a08c-4614-be41-660fc2945ba7",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install --upgrade openai tiktoken python-dotenv\n",
    "%pip install pexpect==4.9.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5480e2a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Import Libraries ###\n",
    "\n",
    "from openai import OpenAI\n",
    "import pandas as pd\n",
    "from IPython.display import clear_output\n",
    "import random\n",
    "from dotenv import load_dotenv\n",
    "import random\n",
    "import json\n",
    "import numpy as np\n",
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dedd148",
   "metadata": {},
   "source": [
    "# Required Data\n",
    "\n",
    "**Data Format:** \n",
    "* Comma-separated values (.csv)\n",
    "\n",
    "**Data Source:**\n",
    "* 500 randomly sampled *Jeopardy!* questions, including their category, clue, and answer\n",
    "* Questions transcribed from episodes of the show by archivists at the [*J-Archive!*](https://j-archive.com)\n",
    "* Questions extracted and posted publicly [on GitHub](https://github.com/amwagner19/jarchive-clues)\n",
    "* Extraneous columns for the course dropped and IDs reindexed\n",
    "\n",
    "**Data Quality/Bias:**\n",
    "* This data reproduces a small random subset of the questions recorded by the [*J-Archive!*](https://j-archive.com) archivists\n",
    "* *J-Archive!* is a well-regarded fan site, but it has not recorded every clue on every game (e.g., unasked questions)\n",
    "* Any biases reflected in the form and content of the questions reflect those of the *Jeopardy!* writers\n",
    "\n",
    "## Download Required Data\n",
    "\n",
    "The dataset for this class is small enough to distribute with [the git repository for the course](https://github.com/erikfredner/tap-2024), which you can clone."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "019627d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"data.csv\", index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07a0cac1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aae479e2",
   "metadata": {},
   "source": [
    "# Review of Lesson 1\n",
    "\n",
    "1. Why classify texts?\n",
    "   1. Examples from business\n",
    "   2. Examples from scholarship\n",
    "2. Good, bad, and ugly of using LLMs for text classification\n",
    "3. ChatGPT website vs. API\n",
    "4. Calling the API\n",
    "5. Model options\n",
    "6. Model costs\n",
    "7. JSON mode"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f53edaa2",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "- Last time, we talked about text classification.\n",
    "- But we didn't have any texts to classify.\n",
    "- Today, we're going to change that with a type of text that non-LLM methods would struggle to classify: *Jeopardy!* questions.\n",
    "\n",
    "## Why *Jeopardy*?\n",
    "\n",
    "- I just finished [research](https://fredner.org/jeopardy/) for an essay I am writing about *Jeopardy!* questions and the literary canon.\n",
    "  - I'm teaching the methods used for that project here.\n",
    "- Non-LLM methods struggle with short, dense, allusive texts like quiz questions, so this suggests a set of classifications that LLMs can perform that other methods struggle with."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "853ebfe8",
   "metadata": {},
   "source": [
    "# Types of classifications\n",
    "\n",
    "When classifying texts, classification problems can fall into one of several categories. Here are some of the most common examples:\n",
    "\n",
    "- Binary classification: Classifying texts into one of two categories.\n",
    "  - e.g., classifying emails as Spam or Not Spam\n",
    "- Multi-class classification: Classifying texts into one of three or more categories.\n",
    "  - e.g., classifying newspaper articles as politics, business, arts, etc.\n",
    "- Multi-label classification: Labeling texts with one or more classifications.\n",
    "  - e.g., classifying novels with one or more genre labels: `['fantasy', 'romance']`, for example\n",
    "- Hierarchical classification: Classifying texts as part of both classes and subclasses\n",
    "  - e.g., classifying research papers:\n",
    "```python\n",
    "{\n",
    "    \"field\": \"literary studies\",\n",
    "    \"subfields\": [\"american literature\", \"nineteenth-century\"],\n",
    "}\n",
    "```\n",
    "- Ordinal classification: Classifying a text in a way that ranks or orders it.\n",
    "  - e.g., Attempting to infer star values (i.e., rankings from 1-5) from unstarred movie reviews\n",
    "\n",
    "\n",
    "## Which are we going to do?\n",
    "\n",
    "In this brief class, we're going to focus on the simplest category---**binary classification**---with a little bit of **ordinal classification**, too."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bcaaab1",
   "metadata": {},
   "source": [
    "# How do you evaluate an LLM's classifications?\n",
    "\n",
    "- Neither humans nor LLMs classify texts perfectly.\n",
    "- How well do humans agree with each other?\n",
    "- How well do the LLM's judgments align with researcher judgments?\n",
    "\n",
    "The first thing that we need to do is create [**gold-standard data**](https://simmering.dev/blog/gold-data/) that we can use to evaluate the model. In our case, this is going to be the results of human judgments classifying our questions.\n",
    "\n",
    "In some cases, there might already exist classification labels that you could use (e.g., librarians' categorizations of books)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c592608",
   "metadata": {},
   "source": [
    "## Creating data to evaluate the classification\n",
    "\n",
    "We're going to get a sense for the challenge of classifying *Jeopardy* questions by doing it ourselves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b388b6b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make sure you have loaded the data\n",
    "df.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6d03182",
   "metadata": {},
   "source": [
    "- Everyone gets their own sample of questions from our 500 question sample.\n",
    "- There will be overlap with our answers, which we want to measure agreement.\n",
    "- To keep the list of possible labels small, we are going to label questions as belonging to one of five categories that correspond to the most frequent topics in *Jeopardy*:\n",
    "  - History\n",
    "  - Geography\n",
    "  - Literature\n",
    "  - Science\n",
    "  - Other\n",
    "- You can look up information about questions if you're unsure how to classify them.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79772f11",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_df = df.sample(10).copy()\n",
    "my_df.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbca8566",
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_jeopardy_questions(my_df):\n",
    "    # Initialize a list to store the categorizations\n",
    "    categorizations = []\n",
    "\n",
    "    # List of valid categories\n",
    "    categories = [\"History\", \"Geography\", \"Literature\", \"Science\", \"Other\"]\n",
    "\n",
    "    for index, row in my_df.iterrows():\n",
    "        # Print the CATEGORY, CLUE, and ANSWER for the current row\n",
    "        print(f\"ID: {row['ID']}\")\n",
    "        print(f\"CATEGORY: {row['CATEGORY']}\")\n",
    "        print(f\"CLUE: {row['CLUE']}\")\n",
    "        print(f\"ANSWER: {row['ANSWER']}\")\n",
    "\n",
    "        # Display category options with corresponding numbers\n",
    "        print(\"Please classify the question into one of the following categories:\")\n",
    "        for i, category in enumerate(categories, 1):\n",
    "            print(f\"{i}: {category}\")\n",
    "\n",
    "        # Ask the user for a valid categorization\n",
    "        while True:\n",
    "            try:\n",
    "                category_index = int(\n",
    "                    input(\"Enter the number corresponding to the category: \")\n",
    "                )\n",
    "                if 1 <= category_index <= len(categories):\n",
    "                    selected_category = categories[category_index - 1]\n",
    "                    # Save the categorization along with the row ID\n",
    "                    categorizations.append(\n",
    "                        {\n",
    "                            \"ID\": row[\"ID\"],\n",
    "                            \"CATEGORY\": row[\"CATEGORY\"],\n",
    "                            \"CLUE\": row[\"CLUE\"],\n",
    "                            \"ANSWER\": row[\"ANSWER\"],\n",
    "                            \"CLASSIFICATION\": selected_category,\n",
    "                        }\n",
    "                    )\n",
    "                    break\n",
    "                else:\n",
    "                    print(\"Invalid number. Please try again.\")\n",
    "            except ValueError:\n",
    "                print(\"Invalid input. Please enter a number.\")\n",
    "\n",
    "        # Clear the output in the Jupyter notebook\n",
    "        clear_output(wait=True)\n",
    "\n",
    "    # Convert the categorizations to a DataFrame for further use if needed\n",
    "    categorizations_df = pd.DataFrame(categorizations)\n",
    "\n",
    "    # Return the categorizations DataFrame\n",
    "    return categorizations_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c43b62b-46b8-459e-8f98-ce70cb1dbef0",
   "metadata": {},
   "source": [
    "# Classification time!\n",
    "\n",
    "- Running the cell below will ask you to categorize *Jeopardy* questions.\n",
    "- Occasionally, you may not see the box to enter your classifications.\n",
    "- If you run into trouble with the script, you may need to interrupt the kernel. You can do that...\n",
    "    - by clicking the Stop ⬛️ icon in the ribbon above.\n",
    "    - by pressing `ii` on your keyboard\n",
    "    - Or by going to the menu bar: `Kernel > Interrupt kernel`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "277e1d71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: fix bug with incomplete output (mabye caused by URL strings)\n",
    "clear_output()\n",
    "categorized_df = classify_jeopardy_questions(my_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9beba83b-d7ad-459b-92c2-74af80f58d2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# here's what you made\n",
    "categorized_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05d00161",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate a random id to distinguish your data from others'\n",
    "random_id = random.randint(1, 100000)\n",
    "categorized_df.to_csv(f\"classified_jeopardy_{random_id}.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9614461",
   "metadata": {},
   "source": [
    "Now, we have saved your categorizations to a CSV file in your Constellate workspace.\n",
    "\n",
    "## Next steps\n",
    "\n",
    "1. Right-click your classification file (`classified_jeopardy_...csv`), and select `Download`. That will download the file to the `~/Downloads` folder on your computer.\n",
    "2. Navigate to your `Downloads` on your computer, and find your `.csv` file\n",
    "3. Upload your `.csv` to [this Dropbox folder](https://www.dropbox.com/request/ryB9Bh9QefqASXRaZfPU) for us to combine our data together:\n",
    "\n",
    "<https://www.dropbox.com/request/ryB9Bh9QefqASXRaZfPU>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cae142fa",
   "metadata": {},
   "source": [
    "## Discussion of classification experience\n",
    "\n",
    "- Was this more difficult than you anticipated?\n",
    "- How did you decide how to categorize a given question?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ec4c405",
   "metadata": {},
   "source": [
    "# Evaluating our classifications\n",
    "\n",
    "We will evaluate our classifications using the majority of evidence in the data.\n",
    "\n",
    "**N.B.:** The code below will only work on my machine. I will add the classifications to the course repository ASAP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd00c246",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "PATH = \"/Users/erik/Dropbox/File requests/2024 TAPI classifications\"\n",
    "csvs = [f for f in os.listdir(PATH) if f.endswith(\".csv\")]\n",
    "# create a stacked dataframe with the csvs\n",
    "df = pd.concat([pd.read_csv(os.path.join(PATH, f)) for f in csvs], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7f7f79a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop everything except ID and CLASSIFICATION\n",
    "df = df[[\"ID\", \"CLASSIFICATION\"]]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34fcbfca",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_vals = df.groupby(\"ID\").value_counts().unstack().fillna(0)\n",
    "df_vals.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7515351a",
   "metadata": {},
   "source": [
    "For simplicity's sake, I am going to focus on questions where one categorization was the clear \"winner\" (i.e., exclude ties)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1c9f91a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_row_ties(row):\n",
    "    max_value = row.max()\n",
    "    max_count = (row == max_value).sum()\n",
    "    if max_count > 1:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "\n",
    "# Apply the function to each row and create a new column for the results\n",
    "df_vals[\"Tie?\"] = df_vals.apply(check_row_ties, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3fd1277",
   "metadata": {},
   "outputs": [],
   "source": [
    "# How many ties did we have?\n",
    "df_vals[\"Tie?\"].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32a284b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the ids for rows without ties\n",
    "gold_ids = df_vals[~df_vals[\"Tie?\"]].index.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac96e337",
   "metadata": {},
   "outputs": [],
   "source": [
    "gold_df = df_vals[df_vals.index.isin(gold_ids)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f85ab849",
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop the tie column\n",
    "gold_df.drop(\"Tie?\", axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f03e0b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "gold_melted = gold_df.reset_index().melt(\n",
    "    id_vars=[\"ID\"], var_name=\"CLASSIFICATION\", value_name=\"Value\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57144840",
   "metadata": {},
   "outputs": [],
   "source": [
    "gold_labels = gold_melted.loc[\n",
    "    gold_melted.groupby(\"ID\")[\"Value\"].idxmax(), [\"ID\", \"CLASSIFICATION\"]\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27794f98",
   "metadata": {},
   "outputs": [],
   "source": [
    "gold_labels.set_index(\"ID\", inplace=True)\n",
    "gold_labels.to_csv(\"gold_labels.csv\")\n",
    "gold_labels.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b4c5168-6503-4c42-a12b-b08f316f8bc9",
   "metadata": {},
   "source": [
    "> In your menu bar, you may run Git / Pull from Remote to download `gold_labels.csv` \n",
    "> \n",
    "> Note that this will overwrite *all* changes you have made to the files here."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08f850a3",
   "metadata": {},
   "source": [
    "# Using our gold-standard data\n",
    "\n",
    "- We created human-labeled data indicating correct results for our classifications.\n",
    "  - (In a real research setting, this would be a more meticulous and expert-driven process. For this class, it's fine.)\n",
    "- Now we need to test the LLM on these classification tasks.\n",
    "- We will evaluate its performance using the data we created and a simple statistic called the [F-score](https://en.wikipedia.org/wiki/F-score)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b13d8b6",
   "metadata": {},
   "source": [
    "## Getting one basic classification result\n",
    "\n",
    "- We talked earlier about different types of classification.\n",
    "- The data we have just created is suitable for either binary or multi-class classification.\n",
    "- The type of classification we get back from the LLM will be determined by the prompt we give the model.\n",
    "- Let's start with a simple prompt:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "637a8791",
   "metadata": {},
   "outputs": [],
   "source": [
    "# binary classification prompt\n",
    "system_prompt = \"\"\"Determine whether the following Jeopardy question is about Literature.\n",
    "Respond in JSON like so: {\"Literature\": True}\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfc2b72b-f06d-497d-b42d-04d12a10f654",
   "metadata": {},
   "source": [
    "Because we created multi-class labels, we can treat everything that is not labeled literature as `False`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e3f8e47",
   "metadata": {},
   "source": [
    "> **N.B.** Remember to instruct the model to respond in JSON *even if you activate JSON mode*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f3e5984",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load questions\n",
    "data = pd.read_csv(\"data.csv\")\n",
    "# load labels\n",
    "gold_labels = pd.read_csv(\"gold_labels.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec46391d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_prompt(row):\n",
    "    prompt = f\"\"\"Category: {row['CATEGORY'].values[0]}\\nClue: {row['CLUE'].values[0]}\\nAnswer: {row['ANSWER'].values[0]}\"\"\"\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1f99ec5",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = make_prompt(data.sample(1))\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f163ca51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remember, this uses the API key in your .env file:\n",
    "# if you didn't run the code in lesson 1, you probably don't have an .env file\n",
    "load_dotenv()\n",
    "client = OpenAI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97f0e258",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_completion(\n",
    "    system_prompt, prompt, print_prompt=True, client=client, model=\"gpt-4o\", json=True\n",
    "):\n",
    "    completion = client.chat.completions.create(\n",
    "        model=model,\n",
    "        response_format={\"type\": \"json_object\"} if json else None,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": prompt},\n",
    "        ],\n",
    "    )\n",
    "    if print_prompt:\n",
    "        print(f\"System prompt: {system_prompt}\\n{'-' * 80}\")\n",
    "        print(f\"User prompt: {prompt}\\n{'-' * 80}\")\n",
    "        print(f\"Assistant response: {completion.choices[0].message.content}\\n{'*' * 80}\")\n",
    "\n",
    "    return completion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47bdfbc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "c = make_completion(system_prompt, prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e44b4fab",
   "metadata": {},
   "source": [
    "Ok, that returns a binary response.\n",
    "\n",
    "Now we can put these things together and see how text classifications can quickly become data.\n",
    "\n",
    "We'll start with one row that we know has a gold-standard label:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62a15541-91ac-46ef-8cd5-94a19e1268f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = dict()\n",
    "\n",
    "test_id = gold_labels.sample(1)['ID'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15b13f2d-29ae-4aba-b43f-0ab98dd0551a",
   "metadata": {},
   "outputs": [],
   "source": [
    "row = data[data['ID'].isin(test_id)]\n",
    "row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ee723a3-f0dc-4df2-9439-3dc50403ff79",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = make_prompt(row)\n",
    "c = make_completion(system_prompt, prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e727fa9-8a81-4e4c-bf90-0bd69c4af535",
   "metadata": {},
   "outputs": [],
   "source": [
    "output[test_id[0]] = json.loads(c.choices[0].message.content)\n",
    "output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a75cb99",
   "metadata": {},
   "source": [
    "Now we can put all of this together to automatically process a batch of questions using the `gold_ids`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c29fc726",
   "metadata": {},
   "outputs": [],
   "source": [
    "l = list()\n",
    "\n",
    "for idx, gold_row in gold_labels.sample(3).iterrows():\n",
    "    # get row values\n",
    "    gold_id = gold_row[\"ID\"]\n",
    "    gold_classification = gold_row[\"CLASSIFICATION\"]\n",
    "    \n",
    "    question_row = data[data['ID'] == gold_id]\n",
    "    prompt = make_prompt(question_row)\n",
    "    c = make_completion(system_prompt, prompt)\n",
    "\n",
    "    # make output\n",
    "    d = dict()\n",
    "    d[\"ID\"] = gold_id\n",
    "    d[\"CLASSIFICATION\"] = gold_classification\n",
    "    d.update(json.loads(c.choices[0].message.content))\n",
    "    l.append(d)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "448f526e-bdad-443e-ac75-cff0121985df",
   "metadata": {},
   "source": [
    "For demonstration purposes, the loop above only `sample`s 3 rows.\n",
    "\n",
    "This structure would be sufficient to do all of the points for which we have values, however."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e951b82-dc4e-4503-b259-6e6b88fd31dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = pd.DataFrame(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e269f9e9-6d94-4a57-b14c-3886de16360a",
   "metadata": {},
   "outputs": [],
   "source": [
    "output.columns = [\"ID\", \"GOLD_CLASSIFICATION\", \"LLM_IS_LITERATURE\"]\n",
    "output.set_index(\"ID\", inplace=True)\n",
    "output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa5a0678",
   "metadata": {},
   "source": [
    "## Comparing LLM classifications to human classifications"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb55d556",
   "metadata": {},
   "source": [
    "The most basic way of answering this question: How often do humans and the LLM agree on binary classification (Literature vs. Not Literature)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "213ff902-5daa-4b93-b9c2-e38c8ead1899",
   "metadata": {},
   "outputs": [],
   "source": [
    "output[\"GOLD_IS_LITERATURE\"] = output[\"GOLD_CLASSIFICATION\"] == \"Literature\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00351245-4fe3-493d-b436-fb467a8e2774",
   "metadata": {},
   "outputs": [],
   "source": [
    "output[\"GOLD_LLM_AGREE\"] = (output[\"GOLD_IS_LITERATURE\"] == output[\"LLM_IS_LITERATURE\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cecd7e0-59cd-4b64-a01a-e2d1782bdcde",
   "metadata": {},
   "outputs": [],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f397bac6",
   "metadata": {},
   "source": [
    "This would be the most basic way of measuring the success rate of your classification: How often does the model output match gold-standard data?\n",
    "\n",
    "However, there is a more sophisticated and widely used solution for binary classification: the F-score.\n",
    "\n",
    "## The F-Score\n",
    "\n",
    "To understand the F-score, you need to understand two related concepts: precision and recall.\n",
    "\n",
    "### Precision\n",
    "\n",
    "Precision measures how many of the items the model identified as `True` were really `True` according to the gold standard data. That is compared against the number of items identified as `True` that were `False` according to the gold standard data, which are known as \"False positives.\"\n",
    "\n",
    "> Precision answers the question: \"How many retrieved items were relevant?\"\n",
    "\n",
    "$Precision = \\frac{True \\ Positives}{True \\ Positives + False \\ Positives}$\n",
    "\n",
    "### Recall\n",
    "\n",
    "Recall measures how many values that ought to have been `True` were labeled `True`.\n",
    "\n",
    "> Recall answers the question: \"How many relevant items were retrieved?\"\n",
    "\n",
    "$Recall = \\frac{True \\ Positives}{True \\ Positives + False \\ Negatives}$\n",
    "\n",
    "### F-score (aka F1)\n",
    "\n",
    "The F score is the harmonic mean of precision and recall.\n",
    "\n",
    "$F_{1}= 2 \\times \\frac{Precision \\times Recall}{Precision + Recall}$\n",
    "\n",
    "## How to calculate\n",
    "\n",
    "There is an easy way to calculate this score using [sklearn](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.f1_score.html):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71709276",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example with fake data\n",
    "y_true = [0, 1, 0, 0, 1, 1]\n",
    "y_pred = [0, 1, 1, 0, 1, 1]\n",
    "f1_score(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c2d5d80-4323-4013-a24c-b584cbf30fa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fa2c2fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Because Python stores True as 1 and False as 0, we can directly use the columns:\n",
    "f1_score(output[\"GOLD_IS_LITERATURE\"], output[\"LLM_IS_LITERATURE\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a18f9ca7",
   "metadata": {},
   "source": [
    "You can also use the F-score to evaluate multi-class classifications.\n",
    "\n",
    "We'll stick with binary for now for simplicity's sake."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a47fc2d5",
   "metadata": {},
   "source": [
    "# Putting it all together\n",
    "\n",
    "Here is how the things we learned today will come together:\n",
    "\n",
    "1. We have our texts to classify (*Jeopardy!* questions, though you could use any texts in any format, as long as you associate an ID with the text.)\n",
    "2. We created gold-standard (i.e., human-labeled, and, ideally, expert-labeled) classifications for testing.\n",
    "3. We wrote prompts and set up API calls that output structured classifications as JSON.\n",
    "4. We import that JSON into a more familiar data structure (a `pandas` dataframe or another two-dimensional data structure).\n",
    "5. We can systematically evaluate the quality of our classifications using the F score.\n",
    "\n",
    "## Next steps\n",
    "\n",
    "There are two big remaining steps we will do for our classification. The steps above are necessary; these are nice to have, and they demonstrate some principles of interacting with LLMs.\n",
    "\n",
    "1. Quantifying uncertainty\n",
    "\n",
    "As you experienced doing classifications by hand, some judgments were easier to make than others. We can ask the LLM to express its confidence in its judgments numerically.\n",
    "\n",
    "This is useful because it allows us to sort automatically labeled data by confidence for review. Low confidence classifications deserve higher priority for manual review (and possible correction) by researchers. (You could also do some clever massaging of the F score by penalizing confident but wrong classifications, while lessening the penalty for low confidence wrong classifications.)\n",
    "\n",
    "2. Prompt engineering\n",
    "\n",
    "We can use the F score to systematically test and evaluate variations of our `system` and `user` prompts to see which prompts produce the most accurate classifications."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cc0d4e7",
   "metadata": {},
   "source": [
    "# Quantifying uncertainty\n",
    "\n",
    "There is a very easy way and a slightly more complex way to quantify our uncertainty in these classifications.\n",
    "\n",
    "## The very easy way\n",
    "\n",
    "This requires a small modification of the system prompt:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef7c42f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = \"\"\"Determine whether the following Jeopardy question is about Literature.\n",
    "Express your confidence in your classification as a percentage from 50 to 100, where 50 is guessing and 100 is certain.\n",
    "Respond in JSON like so:\n",
    "{\"Literature\": True,\n",
    "\"Confidence\": 95}\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63a49388",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = make_prompt(data.sample(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "100a32ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "c = make_completion(system_prompt, prompt)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6253d692-f88e-4c78-914b-5b9233d117e0",
   "metadata": {},
   "source": [
    "## The not-very-hard way\n",
    "\n",
    "One feature of the API we did not discuss last time is called `logprobs`. [Here](https://platform.openai.com/docs/api-reference/chat/create) is the documentation.\n",
    "\n",
    "[OpenAI recommends using `logprobs` to assess the confidence of text classifications.](https://cookbook.openai.com/examples/using_logprobs)\n",
    "\n",
    "When `logprobs` (which refers to the log probabilities of each *generated* token) is switched on, the API returns its probabilities of generating each token like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2944c505-d356-4f4c-85c1-21832fd571c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = \"\"\"Determine whether the following Jeopardy question is about Literature.\n",
    "Respond in JSON like so:\n",
    "{\"Literature\": True}\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24989539-8bbc-42d4-92e6-87622d153dda",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da1d812b-59a4-4272-aadf-d0663cb21081",
   "metadata": {},
   "outputs": [],
   "source": [
    "completion = client.chat.completions.create(\n",
    "  model=\"gpt-4o\",\n",
    "  logprobs=True, # new\n",
    "  top_logprobs=2, # new: ask the API to return the two most probable tokens for each token generated\n",
    "  messages=[\n",
    "    {\"role\": \"system\", \"content\": system_prompt},\n",
    "    {\"role\": \"user\", \"content\": prompt}\n",
    "  ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f11e11b-b6d0-475d-a41b-a29501877911",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(completion.choices[0].logprobs.to_json())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44f003b4-3cb3-4eec-8871-bc5686eb4b0d",
   "metadata": {},
   "source": [
    "The closer `logprob`s are to 0, the more confident the model is in its response. As a reminder:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53038761-1028-4b75-ae0c-7be91efcb0b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "math.log(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3521c4e-a43f-43e2-8e3f-f3d4f8bd5ef0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# i.e., a 95% probability\n",
    "math.log(0.95)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9690e717-2cdc-4b5b-8183-393687f0c702",
   "metadata": {},
   "source": [
    "In the example used above, the `logprob` of the token `\"True\"` was:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cffa5d07-1367-4167-b669-6b94e951091f",
   "metadata": {},
   "outputs": [],
   "source": [
    "logprob = -0.026302502\n",
    "\n",
    "print(f\"The model was {round((math.exp(logprob) * 100), 2)}% confident in this classification.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd568c9d-5895-4356-ae4e-8687e9c6a7da",
   "metadata": {},
   "source": [
    "So, the model's stated confidence in its classification may differ from the underlying `logprob` value for the resulting classification token.\n",
    "\n",
    "For the purposes of this class, we are going to use the model's stated confidence in the response as it is simpler to extract and manipulate.\n",
    "\n",
    "But if you are doing a project where precise probabilities matter, `logprob` is the way to go."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eca42822",
   "metadata": {},
   "source": [
    "# Exercises\n",
    "\n",
    "1. Using what we have learned today, try writing a multi-class classification `system` prompt for *Jeopardy* questions that will output structured JSON with the predefined options we used to create the gold standard data.\n",
    "2. How is the F score different from merely calculating the percentage of the time that the gold-standard data and the classification model agree?\n",
    "3. Write a paragraph explaining how you could apply these techniques to a set of texts than *Jeopardy* questions."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
