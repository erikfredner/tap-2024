{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3e89c5b3",
   "metadata": {},
   "source": [
    "<img align=\"left\" src=\"https://ithaka-labs.s3.amazonaws.com/static-files/images/tdm/tdmdocs/tapi-logo-small.png\" />\n",
    "\n",
    "This notebook is free for educational reuse under [Creative Commons CC BY License](https://creativecommons.org/licenses/by/4.0/).\n",
    "\n",
    "Created by [Erik Fredner](https://fredner.org) for the 2024 Text Analysis Pedagogy Institute, with support from [Constellate](https://constellate.org).\n",
    "\n",
    "For questions/comments/improvements, email erik@fredner.org<br />\n",
    "____"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68f932d1",
   "metadata": {},
   "source": [
    "# Automated Text Classification Using LLMs\n",
    "\n",
    "This is lesson 2 of 3 in the educational series on using large language models (LLMs) for text classification. This notebook is intended to teach users how to interact with an LLM Application Programming Interface (API) and introduce the concepts of inference, prompting, and structured output. \n",
    "\n",
    "**Skills:** \n",
    "* Python\n",
    "* Text analysis\n",
    "* Text classification\n",
    "* LLMs\n",
    "* JSON\n",
    "* APIs\n",
    "\n",
    "**Audience:**\n",
    "Researchers\n",
    "\n",
    "**Use case:**\n",
    "Tutorial\n",
    "\n",
    "**Difficulty:**\n",
    "Intermediate. This assumes users are familiar with Python and have been programming for 6+ months. Code makes up a larger part of the notebook and basic concepts related to Python are not explained.\n",
    "\n",
    "**Completion time:**\n",
    "90 minutes\n",
    "\n",
    "**Knowledge Required:** \n",
    "* Python basics (variables, flow control, functions, lists, dictionaries)\n",
    "\n",
    "**Knowledge Recommended:**\n",
    "* Experience using LLMs (e.g., ChatGPT)\n",
    "\n",
    "**Learning Objectives:**\n",
    "After this lesson, learners will be able to:\n",
    "\n",
    "1. Describe how to evaluate automated LLM classifications.\n",
    "2. Create data to evaluate LLM classifications.\n",
    "3. Characterize the [F-score](https://en.wikipedia.org/wiki/F-score).\n",
    "4. Combine the ideas above to evaluate multiple prompts.\n",
    "\n",
    "**Research Pipeline:**\n",
    "1. Play with LLMs if you have not already.\n",
    "2. Test using a chatbot interface for an LLM (like ChatGPT) to perform relevant classifications for your research.\n",
    "3. Evaluate initial results.\n",
    "4. Learn how to interact with an API through this notebook.\n",
    "5. Modify your initial experiments based on what we cover."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "157c0555",
   "metadata": {},
   "source": [
    "# Required Python Libraries\n",
    "\n",
    "* [OpenAI](https://pypi.org/project/openai/) to interact with the OpenAI API for ChatGPT."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5480e2a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Import Libraries ###\n",
    "\n",
    "from openai import OpenAI\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dedd148",
   "metadata": {},
   "source": [
    "# Required Data\n",
    "\n",
    "**Data Format:** \n",
    "* Comma-separated values (.csv)\n",
    "\n",
    "**Data Source:**\n",
    "* 500 randomly sampled *Jeopardy!* questions, including their category, clue, and answer\n",
    "* Questions transcribed from episodes of the show by archivists at the [*J-Archive!*](https://j-archive.com)\n",
    "* Questions extracted and posted publicly [on GitHub](https://github.com/amwagner19/jarchive-clues)\n",
    "* Extraneous columns for the course dropped and IDs reindexed\n",
    "\n",
    "**Data Quality/Bias:**\n",
    "* This data reproduces a small random subset of the questions recorded by the [*J-Archive!*](https://j-archive.com) archivists\n",
    "* *J-Archive!* is a well-regarded fan site, but it has not recorded every clue on every game (e.g., unasked questions)\n",
    "* Any biases reflected in the form and content of the questions reflect those of the *Jeopardy!* writers\n",
    "\n",
    "## Download Required Data\n",
    "\n",
    "The dataset for this class is small enough to distribute with [the git repository for the course](https://github.com/erikfredner/tap-2024), which you can clone."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "019627d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"data.csv\", index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "07a0cac1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CATEGORY</th>\n",
       "      <th>CLUE</th>\n",
       "      <th>ANSWER</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ID</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>132</th>\n",
       "      <td>FARMING</td>\n",
       "      <td>The main type of farming on the western grassl...</td>\n",
       "      <td>cattle ranching</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>416</th>\n",
       "      <td>GEOGRAPHY</td>\n",
       "      <td>Founded by Chinese miners in 1857, it's Malays...</td>\n",
       "      <td>Kuala Lumpur</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>275</th>\n",
       "      <td>THUMB ENCHANTED EVENING</td>\n",
       "      <td>The \"thumb\" on the geographic mitten that is M...</td>\n",
       "      <td>Lake Huron</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>224</th>\n",
       "      <td>U.S. CITIES</td>\n",
       "      <td>The Smith Brothers made their first cough drop...</td>\n",
       "      <td>Poughkeepsie</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>381</th>\n",
       "      <td>MOVIE PAIRS</td>\n",
       "      <td>Comic strip characters played on screen by Art...</td>\n",
       "      <td>Dagwood &amp; Blondie</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    CATEGORY  \\\n",
       "ID                             \n",
       "132                  FARMING   \n",
       "416                GEOGRAPHY   \n",
       "275  THUMB ENCHANTED EVENING   \n",
       "224              U.S. CITIES   \n",
       "381              MOVIE PAIRS   \n",
       "\n",
       "                                                  CLUE             ANSWER  \n",
       "ID                                                                         \n",
       "132  The main type of farming on the western grassl...    cattle ranching  \n",
       "416  Founded by Chinese miners in 1857, it's Malays...       Kuala Lumpur  \n",
       "275  The \"thumb\" on the geographic mitten that is M...         Lake Huron  \n",
       "224  The Smith Brothers made their first cough drop...       Poughkeepsie  \n",
       "381  Comic strip characters played on screen by Art...  Dagwood & Blondie  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aae479e2",
   "metadata": {},
   "source": [
    "# Review of Lesson 1\n",
    "\n",
    "1. Why classify texts?\n",
    "   1. Examples from business\n",
    "   2. Examples from scholarship\n",
    "2. Good, bad, and ugly of using LLMs for text classification\n",
    "3. ChatGPT website vs. API\n",
    "4. Calling the API\n",
    "5. Model options\n",
    "6. Model costs\n",
    "7. JSON mode"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f53edaa2",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "- Last time, we talked about text classification.\n",
    "- But we didn't have any texts to classify.\n",
    "- Today, we're going to change that with a type of text that non-LLM methods would struggle to classify: *Jeopardy!* questions.\n",
    "\n",
    "## Why *Jeopardy*?\n",
    "\n",
    "- I just finished [research](https://fredner.org/jeopardy/) for an essay I am writing about *Jeopardy!* questions and the literary canon.\n",
    "  - I'm teaching the methods used for that project here.\n",
    "- Non-LLM methods struggle with short, dense, allusive texts like quiz questions, so this suggests a set of classifications that LLMs can perform that other methods struggle with."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "853ebfe8",
   "metadata": {},
   "source": [
    "# Types of classifications\n",
    "\n",
    "When classifying texts, classification problems can fall into one of several categories. Here are some of the most common examples:\n",
    "\n",
    "- Binary classification: Classifying texts into one of two categories.\n",
    "  - e.g., classifying emails as Spam or Not Spam\n",
    "- Multi-class classification: Classifying texts into one of three or more categories.\n",
    "  - e.g., classifying newspaper articles as politics, business, arts, etc.\n",
    "- Multi-label classification: Labeling texts with one or more classifications.\n",
    "  - e.g., classifying novels with one or more genre labels: `['fantasy', 'romance']`, for example\n",
    "- Hierarchical classification: Classifying texts as part of both classes and subclasses\n",
    "  - e.g., classifying research papers:\n",
    "```python\n",
    "{\n",
    "    \"field\": \"literary studies\",\n",
    "    \"subfields\": [\"american literature\", \"nineteenth-century\"],\n",
    "}\n",
    "```\n",
    "- Ordinal classification: Classifying a text in a way that ranks or orders it.\n",
    "  - e.g., Attempting to infer star values (i.e., rankings from 1-5) from unstarred movie reviews\n",
    "\n",
    "\n",
    "## Which are we going to do?\n",
    "\n",
    "In this brief class, we're going to focus on the simplest category---**binary classification**---with a little bit of **ordinal classification**, too."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bcaaab1",
   "metadata": {},
   "source": [
    "# How do you evaluate an LLM's classifications?\n",
    "\n",
    "- Neither humans nor LLMs classify texts perfectly.\n",
    "- How well do humans agree with each other?\n",
    "- How well do the LLM's judgments align with researcher judgments?\n",
    "\n",
    "The first thing that we need to do is create [**gold-standard data**](https://simmering.dev/blog/gold-data/) that we can use to evaluate the model. In our case, this is going to be the results of human judgments classifying our questions.\n",
    "\n",
    "In some cases, there might already exist classification labels that you could use (e.g., librarians' categorizations of books)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c592608",
   "metadata": {},
   "source": [
    "## Creating data to evaluate the classification\n",
    "\n",
    "We're going to get a sense for the challenge of classifying *Jeopardy* questions by doing it ourselves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b388b6b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CATEGORY</th>\n",
       "      <th>CLUE</th>\n",
       "      <th>ANSWER</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ID</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>DIRECTORS</td>\n",
       "      <td>Yilmaz Guney, accused of murder, directed his ...</td>\n",
       "      <td>in prison</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199</th>\n",
       "      <td>5-LETTER WORDS</td>\n",
       "      <td>An archaic interjection used to express surpri...</td>\n",
       "      <td>marry</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>356</th>\n",
       "      <td>ONE GIANT LEAP</td>\n",
       "      <td>His important experiments included launching t...</td>\n",
       "      <td>Robert Goddard</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>428</th>\n",
       "      <td>TOP 40 MATH</td>\n",
       "      <td>Tennessee Ernie Ford's \"Tons\" times Eddie Mone...</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>406</th>\n",
       "      <td>ROCK ART</td>\n",
       "      <td>She's painted the covers for many of her own a...</td>\n",
       "      <td>Joni Mitchell</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           CATEGORY                                               CLUE  \\\n",
       "ID                                                                       \n",
       "24        DIRECTORS  Yilmaz Guney, accused of murder, directed his ...   \n",
       "199  5-LETTER WORDS  An archaic interjection used to express surpri...   \n",
       "356  ONE GIANT LEAP  His important experiments included launching t...   \n",
       "428     TOP 40 MATH  Tennessee Ernie Ford's \"Tons\" times Eddie Mone...   \n",
       "406        ROCK ART  She's painted the covers for many of her own a...   \n",
       "\n",
       "             ANSWER  \n",
       "ID                   \n",
       "24        in prison  \n",
       "199           marry  \n",
       "356  Robert Goddard  \n",
       "428              32  \n",
       "406   Joni Mitchell  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# make sure you have loaded the data\n",
    "df.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6d03182",
   "metadata": {},
   "source": [
    "- Everyone gets their own sample of 50 questions from our 500 question sample.\n",
    "- There will be overlap with our answers, which we want to measure agreement.\n",
    "- To keep the list of possible labels small, we are going to label questions as belonging to one of five categories that correspond to the most frequent topics in *Jeopardy*:\n",
    "  - History\n",
    "  - Geography\n",
    "  - Literature\n",
    "  - Science\n",
    "  - Other\n",
    "- You can look up information about questions if you're unsure how to classify them. (Just don't ask ChatGPT.)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "79772f11",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_df = df.sample(50).copy()\n",
    "my_df.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "23f12125",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>CATEGORY</th>\n",
       "      <th>CLUE</th>\n",
       "      <th>ANSWER</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>410</td>\n",
       "      <td>A SPECIAL TRAIN CAR</td>\n",
       "      <td>&lt;a href=\"https://www.j-archive.com/media/2023-...</td>\n",
       "      <td>a dome car</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>440</td>\n",
       "      <td>THE WORLD OF SPORTS</td>\n",
       "      <td>The Canadian version of this sport has 12 men ...</td>\n",
       "      <td>football</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>276</td>\n",
       "      <td>&lt;a href=\"https://www.j-archive.com/media/2022-...</td>\n",
       "      <td>(&lt;a href=\"https://www.j-archive.com/media/2022...</td>\n",
       "      <td>the aviator</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>116</td>\n",
       "      <td>TOUGH HISTORY</td>\n",
       "      <td>Name for the style seen here after a king who ...</td>\n",
       "      <td>Louis XIV</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>34</td>\n",
       "      <td>THE 18th CENTURY</td>\n",
       "      <td>In 1789 General Nguyen Hue led an attack on Ch...</td>\n",
       "      <td>Tet</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    ID                                           CATEGORY  \\\n",
       "0  410                                A SPECIAL TRAIN CAR   \n",
       "1  440                                THE WORLD OF SPORTS   \n",
       "2  276  <a href=\"https://www.j-archive.com/media/2022-...   \n",
       "3  116                                      TOUGH HISTORY   \n",
       "4   34                                   THE 18th CENTURY   \n",
       "\n",
       "                                                CLUE       ANSWER  \n",
       "0  <a href=\"https://www.j-archive.com/media/2023-...   a dome car  \n",
       "1  The Canadian version of this sport has 12 men ...     football  \n",
       "2  (<a href=\"https://www.j-archive.com/media/2022...  the aviator  \n",
       "3  Name for the style seen here after a king who ...    Louis XIV  \n",
       "4  In 1789 General Nguyen Hue led an attack on Ch...          Tet  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dbca8566",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from IPython.display import clear_output\n",
    "\n",
    "\n",
    "def classify_jeopardy_questions(my_df):\n",
    "    # Initialize a list to store the categorizations\n",
    "    categorizations = []\n",
    "\n",
    "    # List of valid categories\n",
    "    categories = [\"History\", \"Geography\", \"Literature\", \"Science\", \"Other\"]\n",
    "\n",
    "    for index, row in my_df.iterrows():\n",
    "        # Print the CATEGORY, CLUE, and ANSWER for the current row\n",
    "        print(f\"ID: {row['ID']}\")\n",
    "        print(f\"CATEGORY: {row['CATEGORY']}\")\n",
    "        print(f\"CLUE: {row['CLUE']}\")\n",
    "        print(f\"ANSWER: {row['ANSWER']}\")\n",
    "\n",
    "        # Display category options with corresponding numbers\n",
    "        print(\"Please classify the question into one of the following categories:\")\n",
    "        for i, category in enumerate(categories, 1):\n",
    "            print(f\"{i}: {category}\")\n",
    "\n",
    "        # Ask the user for a valid categorization\n",
    "        while True:\n",
    "            try:\n",
    "                category_index = int(\n",
    "                    input(\"Enter the number corresponding to the category: \")\n",
    "                )\n",
    "                if 1 <= category_index <= len(categories):\n",
    "                    selected_category = categories[category_index - 1]\n",
    "                    # Save the categorization along with the row ID\n",
    "                    categorizations.append(\n",
    "                        {\"ID\": row[\"ID\"], \"Category\": selected_category}\n",
    "                    )\n",
    "                    break\n",
    "                else:\n",
    "                    print(\"Invalid number. Please try again.\")\n",
    "            except ValueError:\n",
    "                print(\"Invalid input. Please enter a number.\")\n",
    "\n",
    "        # Clear the output in the Jupyter notebook\n",
    "        clear_output(wait=True)\n",
    "\n",
    "    # Convert the categorizations to a DataFrame for further use if needed\n",
    "    categorizations_df = pd.DataFrame(categorizations)\n",
    "\n",
    "    # Return the categorizations DataFrame\n",
    "    return categorizations_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "277e1d71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# categorized_df = classify_jeopardy_questions(my_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05d00161",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "# generate a random id to distinguish your data from others'\n",
    "random_id = random.randint(1, 10000)\n",
    "categorized_df.to_csv(f\"classified_jeopardy_{random_id}.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9614461",
   "metadata": {},
   "source": [
    "Now, we have saved your categorizations to a CSV file in your Constellate workspace.\n",
    "\n",
    "## Next steps\n",
    "\n",
    "1. Right-click your classification file (`classified_jeopardy_...csv`), and select `Download`. That will download the file to the `~/Downloads` folder on your computer.\n",
    "2. Navigate to your `Downloads` on your computer, and find your `.csv` file\n",
    "3. Upload your `.csv` to [this Dropbox folder](https://www.dropbox.com/request/ryB9Bh9QefqASXRaZfPU) for us to combine our data together:\n",
    "\n",
    "<https://www.dropbox.com/request/ryB9Bh9QefqASXRaZfPU>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cae142fa",
   "metadata": {},
   "source": [
    "## Discussion of classification experience\n",
    "\n",
    "- Was this more difficult than you anticipated?\n",
    "- How did you decide how to categorize a given question?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ec4c405",
   "metadata": {},
   "source": [
    "# Evaluating our classifications\n",
    "\n",
    "We will evaluate our classifications using the majority of evidence in the data.\n",
    "\n",
    "**N.B.:** The code below will only work on my machine for now. I will add the classifications to the course repository ASAP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fd00c246",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "PATH = \"/Users/erik/Dropbox/File requests/2024 TAPI classifications\"\n",
    "csvs = [f for f in os.listdir(PATH) if f.endswith(\".csv\")]\n",
    "# create a stacked dataframe with the csvs\n",
    "df = pd.concat([pd.read_csv(os.path.join(PATH, f)) for f in csvs], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e7f7f79a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Category</th>\n",
       "      <th>Geography</th>\n",
       "      <th>Literature</th>\n",
       "      <th>Other</th>\n",
       "      <th>Science</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ID</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>112</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>208</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>216</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>236</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>267</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>344</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>364</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Category  Geography  Literature  Other  Science\n",
       "ID                                             \n",
       "43              0.0         0.0    1.0      0.0\n",
       "66              2.0         0.0    0.0      0.0\n",
       "112             0.0         0.0    1.0      0.0\n",
       "208             0.0         0.0    1.0      0.0\n",
       "216             0.0         1.0    0.0      0.0\n",
       "236             0.0         0.0    0.0      1.0\n",
       "267             1.0         0.0    0.0      0.0\n",
       "344             0.0         0.0    0.0      1.0\n",
       "364             1.0         0.0    0.0      0.0"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_vals = df.groupby(\"ID\").value_counts().unstack().fillna(0)\n",
    "df_vals"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7515351a",
   "metadata": {},
   "source": [
    "For simplicity's sake, we are going to focus on questions where one categorization was the clear \"winner\" (i.e., exclude ties)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f1c9f91a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_row_ties(row):\n",
    "    max_value = row.max()\n",
    "    max_count = (row == max_value).sum()\n",
    "    if max_count > 1:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "\n",
    "# Apply the function to each row and create a new column for the results\n",
    "df_vals[\"Tie?\"] = df_vals.apply(check_row_ties, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a3fd1277",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# How many ties did we have?\n",
    "df_vals[\"Tie?\"].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "32a284b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the ids for rows without ties\n",
    "gold_ids = df_vals[~df_vals[\"Tie?\"]].index.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ac96e337",
   "metadata": {},
   "outputs": [],
   "source": [
    "gold_df = df_vals[df_vals.index.isin(gold_ids)]\n",
    "# drop the tie column\n",
    "gold_df.drop(\"Tie?\", axis=1, inplace=True)\n",
    "gold_df.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7f03e0b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "gold_melted = gold_df.melt(id_vars=[\"ID\"], var_name=\"Category\", value_name=\"Value\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "57144840",
   "metadata": {},
   "outputs": [],
   "source": [
    "gold_labels = gold_melted.loc[\n",
    "    gold_melted.groupby(\"ID\")[\"Value\"].idxmax(), [\"ID\", \"Category\"]\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "27794f98",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Category</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ID</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>Other</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>Geography</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>112</th>\n",
       "      <td>Other</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>208</th>\n",
       "      <td>Other</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>216</th>\n",
       "      <td>Literature</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>236</th>\n",
       "      <td>Science</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>267</th>\n",
       "      <td>Geography</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>344</th>\n",
       "      <td>Science</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>364</th>\n",
       "      <td>Geography</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Category\n",
       "ID             \n",
       "43        Other\n",
       "66    Geography\n",
       "112       Other\n",
       "208       Other\n",
       "216  Literature\n",
       "236     Science\n",
       "267   Geography\n",
       "344     Science\n",
       "364   Geography"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gold_labels.set_index(\"ID\", inplace=True)\n",
    "gold_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08f850a3",
   "metadata": {},
   "source": [
    "# Using our gold-standard data\n",
    "\n",
    "- We created human-labeled data indicating correct results for our classifications.\n",
    "  - (In a real research setting, this would be a more meticulous and expert-driven process. For this class, it's fine.)\n",
    "- Now we need to test the LLM on these classification tasks.\n",
    "- We will evaluate its performance using the data we created and a simple statistic called the [F-score](https://en.wikipedia.org/wiki/F-score)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b13d8b6",
   "metadata": {},
   "source": [
    "## Getting one basic classification result\n",
    "\n",
    "- We talked earlier about types of classification.\n",
    "- The data we have just created is suitable for either binary or multi-class classification.\n",
    "- The type of classification we get back from the LLM will be determined by the prompt we give the model.\n",
    "- Let's start with a simple prompt:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "637a8791",
   "metadata": {},
   "outputs": [],
   "source": [
    "# binary classification prompt\n",
    "system_prompt = \"\"\"Determine whether the following Jeopardy question is about Literature.\n",
    "Respond in JSON like so: {\"Literature\": True}\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e3f8e47",
   "metadata": {},
   "source": [
    "> **N.B.** It is important to instruct the model to respond in JSON *even if you activate JSON mode*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3f3e5984",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load questions\n",
    "data = pd.read_csv(\"data.csv\", index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ec46391d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_prompt(row):\n",
    "    prompt = f\"\"\"Category: {row['CATEGORY'].values[0]}\\nClue: {row['CLUE'].values[0]}\\nAnswer: {row['ANSWER'].values[0]}\"\"\"\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b1f99ec5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Category: DON QUIZ-OTE\n",
      "Clue: In 1967 this man's aircraft company merged with McDonnell Aircraft\n",
      "Answer: (Donald) Douglas\n"
     ]
    }
   ],
   "source": [
    "prompt = make_prompt(data.sample(1))\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f163ca51",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "# remember, this uses the API key in your .env file:\n",
    "client = OpenAI()\n",
    "\n",
    "# if you didn't run the code in lesson 1, you probably don't have an .env file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "97f0e258",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_completion(\n",
    "    system_prompt, prompt, print_prompt=True, client=client, model=\"gpt-4o\", json=True\n",
    "):\n",
    "    completion = client.chat.completions.create(\n",
    "        model=model,\n",
    "        response_format={\"type\": \"json_object\"} if json else None,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": prompt},\n",
    "        ],\n",
    "    )\n",
    "    if print_prompt:\n",
    "        print(f\"System prompt: {system_prompt}\\n{'-' * 80}\")\n",
    "        print(f\"User prompt: {prompt}\\n{'-' * 80}\")\n",
    "        print(f\"Assistant response: {completion.choices[0].message.content}\")\n",
    "\n",
    "    return completion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "47bdfbc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "System prompt: Determine whether the following Jeopardy question is about Literature.\n",
      "Respond in JSON like so: {\"Literature\": True}\n",
      "--------------------------------------------------------------------------------\n",
      "User prompt: Category: DON QUIZ-OTE\n",
      "Clue: In 1967 this man's aircraft company merged with McDonnell Aircraft\n",
      "Answer: (Donald) Douglas\n",
      "--------------------------------------------------------------------------------\n",
      "Assistant response: {\"Literature\": false}\n"
     ]
    }
   ],
   "source": [
    "c = make_completion(system_prompt, prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e44b4fab",
   "metadata": {},
   "source": [
    "Ok, that returns a binary response.\n",
    "\n",
    "Now we can put these things together and see how text classifications can quickly become data.\n",
    "\n",
    "We'll start with one row that we know has a gold-standard label:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e25a9312",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "System prompt: Determine whether the following Jeopardy question is about Literature.\n",
      "Respond in JSON like so: {\"Literature\": True}\n",
      "--------------------------------------------------------------------------------\n",
      "User prompt: Category: LEADERS\n",
      "Clue: Viktor Orban is the anti-immigrant leader of this European country\n",
      "Answer: Hungary\n",
      "--------------------------------------------------------------------------------\n",
      "Assistant response: {\"Literature\": false}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{66: {'Literature': False}}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "import json\n",
    "\n",
    "output = dict()\n",
    "\n",
    "id = random.choice(gold_ids)\n",
    "row = data[data.index == id]\n",
    "prompt = make_prompt(row)\n",
    "c = make_completion(system_prompt, prompt)\n",
    "output[id] = json.loads(c.choices[0].message.content)\n",
    "output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a75cb99",
   "metadata": {},
   "source": [
    "Now we can put all of this together to automatically process a batch of questions using the `gold_ids`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c29fc726",
   "metadata": {},
   "outputs": [],
   "source": [
    "l = list()\n",
    "\n",
    "for id in random.sample(gold_ids, 20):\n",
    "    d = dict()\n",
    "    row = data[data.index == id]\n",
    "    prompt = make_prompt(row)\n",
    "    c = make_completion(system_prompt, prompt, print=False)\n",
    "    d[\"ID\"] = id\n",
    "    d.update(json.loads(c.choices[0].message.content))\n",
    "    l.append(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09edcadf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Literature_LLM</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ID</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>112</th>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>208</th>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>216</th>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Literature_LLM\n",
       "ID                 \n",
       "43            False\n",
       "66            False\n",
       "112            True\n",
       "208           False\n",
       "216            True"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output = pd.DataFrame(l)\n",
    "output.columns = [\"ID\", \"Literature_LLM\"]\n",
    "output.set_index(\"ID\", inplace=True)\n",
    "output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa5a0678",
   "metadata": {},
   "source": [
    "## Comparing LLM classifications to human classifications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9adce49",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Category</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ID</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>Other</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>Geography</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>112</th>\n",
       "      <td>Other</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>208</th>\n",
       "      <td>Other</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>216</th>\n",
       "      <td>Literature</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Category\n",
       "ID             \n",
       "43        Other\n",
       "66    Geography\n",
       "112       Other\n",
       "208       Other\n",
       "216  Literature"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gold_labels.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb55d556",
   "metadata": {},
   "source": [
    "How often do humans and the LLM agree on binary classification (Literature vs. Not Literature)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb7fe8af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Literature_LLM</th>\n",
       "      <th>Category</th>\n",
       "      <th>Category_Literature</th>\n",
       "      <th>Human and LLM Agree</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ID</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>False</td>\n",
       "      <td>Other</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>False</td>\n",
       "      <td>Geography</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>112</th>\n",
       "      <td>True</td>\n",
       "      <td>Other</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>208</th>\n",
       "      <td>False</td>\n",
       "      <td>Other</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>216</th>\n",
       "      <td>True</td>\n",
       "      <td>Literature</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Literature_LLM    Category  Category_Literature  Human and LLM Agree\n",
       "ID                                                                       \n",
       "43            False       Other                False                 True\n",
       "66            False   Geography                False                 True\n",
       "112            True       Other                False                False\n",
       "208           False       Other                False                 True\n",
       "216            True  Literature                 True                 True"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agree_df = output.merge(gold_labels, left_index=True, right_index=True)\n",
    "agree_df[\"Category_Literature\"] = agree_df[\"Category\"] == \"Literature\"\n",
    "agree_df[\"Human and LLM Agree\"] = (\n",
    "    agree_df[\"Category_Literature\"] == agree_df[\"Literature_LLM\"]\n",
    ")\n",
    "agree_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f397bac6",
   "metadata": {},
   "source": [
    "This would be the most basic way of measuring the success rate of your classification: How often does the model output match gold-standard data?\n",
    "\n",
    "However, there is a more sophisticated and widely used solution for binary classification: the F-score.\n",
    "\n",
    "## The F-Score\n",
    "\n",
    "To understand the F-score, you need to understand two related concepts: precision and recall.\n",
    "\n",
    "### Precision\n",
    "\n",
    "This measures how many of the items the model identified as `True` were really `True` according to the gold standard data. That is compared against the number of items identified as `True` that were `False` according to the gold standard data, which are known as \"False positives.\"\n",
    "\n",
    "> Precision answers the question: \"How many retrieved items were relevant?\"\n",
    "\n",
    "$Precision = \\frac{True \\ Positives}{True \\ Positives + False \\ Positives}$\n",
    "\n",
    "### Recall\n",
    "\n",
    "This measures how many values that ought to have been `True` were labeled `True`.\n",
    "\n",
    "> Recall answers the question: \"How many relevant items were retrieved?\"\n",
    "\n",
    "$Recall = \\frac{True \\ Positives}{True \\ Positives + False \\ Negatives}$\n",
    "\n",
    "### F-score (aka F1)\n",
    "\n",
    "The F score is the harmonic mean of precision and recall.\n",
    "\n",
    "$F_{1}= 2 \\times \\frac{Precision \\times Recall}{Precision + Recall}$\n",
    "\n",
    "## How to calculate\n",
    "\n",
    "There is an easy way to calculate this score using [sklearn](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.f1_score.html):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71709276",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8571428571428571"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "# Example with fake data\n",
    "y_true = [0, 1, 0, 0, 1, 1]\n",
    "y_pred = [0, 1, 1, 0, 1, 1]\n",
    "f1_score(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fa2c2fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6666666666666666"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Because Python stores True as 1 and False as 0, we can directly use the columns:\n",
    "f1_score(agree_df[\"Category_Literature\"], agree_df[\"Literature_LLM\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a18f9ca7",
   "metadata": {},
   "source": [
    "You can also use the F-score to evaluate multi-class classifications.\n",
    "\n",
    "We'll stick with binary for now for simplicity's sake."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a47fc2d5",
   "metadata": {},
   "source": [
    "# Putting it all together\n",
    "\n",
    "Here is how the things we learned today come together:\n",
    "\n",
    "1. We have our texts to classify (*Jeopardy!* questions)\n",
    "2. We created gold-standard (i.e., human-labeled, and, ideally, expert-labeled) classifications for testing.\n",
    "3. We wrote prompts and set up API calls that output structured classifications as JSON.\n",
    "4. We can systematically evaluate the quality of our classification using the F score.\n",
    "\n",
    "## Next steps\n",
    "\n",
    "There are two big remaining steps for our classification:\n",
    "\n",
    "1. Quantifying uncertainty\n",
    "\n",
    "As you experienced doing classifications by hand, some judgments were easier than others. We can ask the LLM to express its confidence in its judgments numerically.\n",
    "\n",
    "This is useful because it allows us to sort automatically labeled data by confidence for review. Low confidence classifications deserve higher priority for manual review (and possible correction) by researchers. (You could also do some clever massaging of the F score by penalizing confident but wrong classifications, while lessening the penalty for low confidence wrong classifications.)\n",
    "\n",
    "2. Prompt engineering\n",
    "\n",
    "We can use the F score to systematically test and evaluate variations of our `system` and `user` prompts to see which prompts produce the most accurate classifications."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cc0d4e7",
   "metadata": {},
   "source": [
    "# Quantifying uncertainty\n",
    "\n",
    "This one is surprisingly easy. It requires a small modification of the system prompt:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ef7c42f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = \"\"\"Determine whether the following Jeopardy question is about Literature.\n",
    "Express your confidence in your classification as a percentage from 50 to 100, where 50 is guessing and 100 is certain.\n",
    "Respond in JSON like so:\n",
    "{\"Literature\": True,\n",
    "\"Confidence\": 95}\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "63a49388",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = make_prompt(data.sample(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "100a32ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "System prompt: Determine whether the following Jeopardy question is about Literature.\n",
      "Express your confidence in your classification as a percentage from 50 to 100, where 50 is guessing and 100 is certain.\n",
      "Respond in JSON like so:\n",
      "{\"Literature\": True,\n",
      "\"Confidence\": 95}\n",
      "--------------------------------------------------------------------------------\n",
      "User prompt: Category: COMPOSERS\n",
      "Clue: In 1943 American composer William Schuman became the first composer to win one of these prizes\n",
      "Answer: Pulitzer\n",
      "--------------------------------------------------------------------------------\n",
      "Assistant response: {\n",
      "\"Literature\": false,\n",
      "\"Confidence\": 100\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "c = make_completion(system_prompt, prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43941e04",
   "metadata": {},
   "source": [
    "We can use confidence scores to prioritize review of low-confidence (i.e., `50%`-`75%`) responses."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eca42822",
   "metadata": {},
   "source": [
    "# Exercises\n",
    "\n",
    "1. Using what we have learned today, try writing a multi-class classification `system` prompt for *Jeopardy* questions that will output structured JSON with the predefined options we used to create the gold standard data.\n",
    "2. Explain how the F score differs from merely calculating the percentage of the time that the gold-standard data and the classification model agree.\n",
    "3. Write a paragraph explaining how you could apply these techniques to a different set of texts than *Jeopardy* questions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9429a4c9",
   "metadata": {},
   "source": [
    "# Next lesson!\n",
    "\n",
    "[Proceed to next lesson ->](./lesson-3.ipynb)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
