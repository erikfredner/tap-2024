{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3e89c5b3",
   "metadata": {},
   "source": [
    "<img align=\"left\" src=\"https://ithaka-labs.s3.amazonaws.com/static-files/images/tdm/tdmdocs/tapi-logo-small.png\" />\n",
    "\n",
    "This notebook is free for educational reuse under [Creative Commons CC BY License](https://creativecommons.org/licenses/by/4.0/).\n",
    "\n",
    "Created by [Erik Fredner](https://fredner.org) for the 2024 Text Analysis Pedagogy Institute, with support from [Constellate](https://constellate.org).\n",
    "\n",
    "For questions/comments/improvements, email erik@fredner.org<br />\n",
    "____"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68f932d1",
   "metadata": {},
   "source": [
    "# Automated Text Classification Using LLMs\n",
    "\n",
    "This is lesson 2 of 3 in the educational series on using large language models (LLMs) for text classification. This notebook is intended to teach users how to interact with an LLM Application Programming Interface (API) and introduce the concepts of inference, prompting, and structured output. \n",
    "\n",
    "**Skills:** \n",
    "* Python\n",
    "* Text analysis\n",
    "* Text classification\n",
    "* LLMs\n",
    "* JSON\n",
    "* APIs\n",
    "\n",
    "**Audience:**\n",
    "Researchers\n",
    "\n",
    "**Use case:**\n",
    "Tutorial\n",
    "\n",
    "**Difficulty:**\n",
    "Intermediate. This assumes users are familiar with Python and have been programming for 6+ months. Code makes up a larger part of the notebook and basic concepts related to Python are not explained.\n",
    "\n",
    "**Completion time:**\n",
    "90 minutes\n",
    "\n",
    "**Knowledge Required:** \n",
    "* Python basics (variables, flow control, functions, lists, dictionaries)\n",
    "\n",
    "**Knowledge Recommended:**\n",
    "* Experience using LLMs (e.g., ChatGPT)\n",
    "\n",
    "**Learning Objectives:**\n",
    "After this lesson, learners will be able to:\n",
    "\n",
    "1. Describe how to evaluate automated LLM classifications.\n",
    "2. Create data to evaluate LLM classifications.\n",
    "3. Characterize the [F-score](https://en.wikipedia.org/wiki/F-score).\n",
    "4. Combine the ideas above to evaluate multiple prompts.\n",
    "\n",
    "**Research Pipeline:**\n",
    "1. Play with LLMs if you have not already.\n",
    "2. Test using a chatbot interface for an LLM (like ChatGPT) to perform relevant classifications for your research.\n",
    "3. Evaluate initial results.\n",
    "4. Learn how to interact with an API through this notebook.\n",
    "5. Modify your initial experiments based on what we cover."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "157c0555",
   "metadata": {},
   "source": [
    "# Required Python Libraries\n",
    "\n",
    "* [OpenAI](https://pypi.org/project/openai/) to interact with the OpenAI API for ChatGPT."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5480e2a8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-06T15:22:47.559723Z",
     "iopub.status.busy": "2024-07-06T15:22:47.559226Z",
     "iopub.status.idle": "2024-07-06T15:22:48.010453Z",
     "shell.execute_reply": "2024-07-06T15:22:48.009981Z",
     "shell.execute_reply.started": "2024-07-06T15:22:47.559705Z"
    }
   },
   "outputs": [],
   "source": [
    "### Import Libraries ###\n",
    "\n",
    "from openai import OpenAI\n",
    "import pandas as pd\n",
    "from IPython.display import clear_output\n",
    "import random\n",
    "from dotenv import load_dotenv\n",
    "import random\n",
    "import json\n",
    "import numpy as np\n",
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dedd148",
   "metadata": {},
   "source": [
    "# Required Data\n",
    "\n",
    "**Data Format:** \n",
    "* Comma-separated values (.csv)\n",
    "\n",
    "**Data Source:**\n",
    "* 500 randomly sampled *Jeopardy!* questions, including their category, clue, and answer\n",
    "* Questions transcribed from episodes of the show by archivists at the [*J-Archive!*](https://j-archive.com)\n",
    "* Questions extracted and posted publicly [on GitHub](https://github.com/amwagner19/jarchive-clues)\n",
    "* Extraneous columns for the course dropped and IDs reindexed\n",
    "\n",
    "**Data Quality/Bias:**\n",
    "* This data reproduces a small random subset of the questions recorded by the [*J-Archive!*](https://j-archive.com) archivists\n",
    "* *J-Archive!* is a well-regarded fan site, but it has not recorded every clue on every game (e.g., unasked questions)\n",
    "* Any biases reflected in the form and content of the questions reflect those of the *Jeopardy!* writers\n",
    "\n",
    "## Download Required Data\n",
    "\n",
    "The dataset for this class is small enough to distribute with [the git repository for the course](https://github.com/erikfredner/tap-2024), which you can clone."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "019627d5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-06T15:07:25.214172Z",
     "iopub.status.busy": "2024-07-06T15:07:25.213569Z",
     "iopub.status.idle": "2024-07-06T15:07:25.221374Z",
     "shell.execute_reply": "2024-07-06T15:07:25.220921Z",
     "shell.execute_reply.started": "2024-07-06T15:07:25.214151Z"
    }
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"data.csv\", index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "07a0cac1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-06T15:07:27.213958Z",
     "iopub.status.busy": "2024-07-06T15:07:27.213677Z",
     "iopub.status.idle": "2024-07-06T15:07:27.224408Z",
     "shell.execute_reply": "2024-07-06T15:07:27.223936Z",
     "shell.execute_reply.started": "2024-07-06T15:07:27.213940Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CATEGORY</th>\n",
       "      <th>CLUE</th>\n",
       "      <th>ANSWER</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ID</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>241</th>\n",
       "      <td>HAPPY CAMPERS</td>\n",
       "      <td>This Calif. natl. park's Lodgepole Campground ...</td>\n",
       "      <td>Sequoia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>479</th>\n",
       "      <td>HALOGENS</td>\n",
       "      <td>Astatine is the only halogen whose isotopes ar...</td>\n",
       "      <td>radioactive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>322</th>\n",
       "      <td>A QUICK STUDY</td>\n",
       "      <td>Gerontology</td>\n",
       "      <td>the elderly or old people (aging)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>279</th>\n",
       "      <td>LITERARY TABLOID HEADLINES</td>\n",
       "      <td>1597:\\r\\n\"Families vow to end feud after Veron...</td>\n",
       "      <td>&lt;i&gt;Romeo and Juliet&lt;/i&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>169</th>\n",
       "      <td>NEO GEO</td>\n",
       "      <td>As an important source of this metal from the ...</td>\n",
       "      <td>gold</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       CATEGORY  \\\n",
       "ID                                \n",
       "241               HAPPY CAMPERS   \n",
       "479                    HALOGENS   \n",
       "322               A QUICK STUDY   \n",
       "279  LITERARY TABLOID HEADLINES   \n",
       "169                     NEO GEO   \n",
       "\n",
       "                                                  CLUE  \\\n",
       "ID                                                       \n",
       "241  This Calif. natl. park's Lodgepole Campground ...   \n",
       "479  Astatine is the only halogen whose isotopes ar...   \n",
       "322                                        Gerontology   \n",
       "279  1597:\\r\\n\"Families vow to end feud after Veron...   \n",
       "169  As an important source of this metal from the ...   \n",
       "\n",
       "                                ANSWER  \n",
       "ID                                      \n",
       "241                            Sequoia  \n",
       "479                        radioactive  \n",
       "322  the elderly or old people (aging)  \n",
       "279            <i>Romeo and Juliet</i>  \n",
       "169                               gold  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aae479e2",
   "metadata": {},
   "source": [
    "# Review of Lesson 1\n",
    "\n",
    "1. Why classify texts?\n",
    "   1. Examples from business\n",
    "   2. Examples from scholarship\n",
    "2. Good, bad, and ugly of using LLMs for text classification\n",
    "3. ChatGPT website vs. API\n",
    "4. Calling the API\n",
    "5. Model options\n",
    "6. Model costs\n",
    "7. JSON mode"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f53edaa2",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "- Last time, we talked about text classification.\n",
    "- But we didn't have any texts to classify.\n",
    "- Today, we're going to change that with a type of text that non-LLM methods would struggle to classify: *Jeopardy!* questions.\n",
    "\n",
    "## Why *Jeopardy*?\n",
    "\n",
    "- I just finished [research](https://fredner.org/jeopardy/) for an essay I am writing about *Jeopardy!* questions and the literary canon.\n",
    "  - I'm teaching the methods used for that project here.\n",
    "- Non-LLM methods struggle with short, dense, allusive texts like quiz questions, so this suggests a set of classifications that LLMs can perform that other methods struggle with."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "853ebfe8",
   "metadata": {},
   "source": [
    "# Types of classifications\n",
    "\n",
    "When classifying texts, classification problems can fall into one of several categories. Here are some of the most common examples:\n",
    "\n",
    "- Binary classification: Classifying texts into one of two categories.\n",
    "  - e.g., classifying emails as Spam or Not Spam\n",
    "- Multi-class classification: Classifying texts into one of three or more categories.\n",
    "  - e.g., classifying newspaper articles as politics, business, arts, etc.\n",
    "- Multi-label classification: Labeling texts with one or more classifications.\n",
    "  - e.g., classifying novels with one or more genre labels: `['fantasy', 'romance']`, for example\n",
    "- Hierarchical classification: Classifying texts as part of both classes and subclasses\n",
    "  - e.g., classifying research papers:\n",
    "```python\n",
    "{\n",
    "    \"field\": \"literary studies\",\n",
    "    \"subfields\": [\"american literature\", \"nineteenth-century\"],\n",
    "}\n",
    "```\n",
    "- Ordinal classification: Classifying a text in a way that ranks or orders it.\n",
    "  - e.g., Attempting to infer star values (i.e., rankings from 1-5) from unstarred movie reviews\n",
    "\n",
    "\n",
    "## Which are we going to do?\n",
    "\n",
    "In this brief class, we're going to focus on the simplest category---**binary classification**---with a little bit of **ordinal classification**, too."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bcaaab1",
   "metadata": {},
   "source": [
    "# How do you evaluate an LLM's classifications?\n",
    "\n",
    "- Neither humans nor LLMs classify texts perfectly.\n",
    "- How well do humans agree with each other?\n",
    "- How well do the LLM's judgments align with researcher judgments?\n",
    "\n",
    "The first thing that we need to do is create [**gold-standard data**](https://simmering.dev/blog/gold-data/) that we can use to evaluate the model. In our case, this is going to be the results of human judgments classifying our questions.\n",
    "\n",
    "In some cases, there might already exist classification labels that you could use (e.g., librarians' categorizations of books)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c592608",
   "metadata": {},
   "source": [
    "## Creating data to evaluate the classification\n",
    "\n",
    "We're going to get a sense for the challenge of classifying *Jeopardy* questions by doing it ourselves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b388b6b2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-06T15:08:10.515257Z",
     "iopub.status.busy": "2024-07-06T15:08:10.514775Z",
     "iopub.status.idle": "2024-07-06T15:08:10.521550Z",
     "shell.execute_reply": "2024-07-06T15:08:10.521068Z",
     "shell.execute_reply.started": "2024-07-06T15:08:10.515239Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CATEGORY</th>\n",
       "      <th>CLUE</th>\n",
       "      <th>ANSWER</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ID</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>465</th>\n",
       "      <td>NONFICTION</td>\n",
       "      <td>She wrote the 1935 book \"Sex and Temperament i...</td>\n",
       "      <td>Margaret Mead</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>192</th>\n",
       "      <td>THAT'S A FACT</td>\n",
       "      <td>The Peruvian part of this lake covers more tha...</td>\n",
       "      <td>Titicaca</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>323</th>\n",
       "      <td>WORLD LITERATURE</td>\n",
       "      <td>Born Jean-Baptiste Poquelin in 1622, he wrote ...</td>\n",
       "      <td>Moli&amp;egrave;re</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>393</th>\n",
       "      <td>OLD TESTAMENT</td>\n",
       "      <td>She died while giving birth to Benjamin</td>\n",
       "      <td>Rachel</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>283</th>\n",
       "      <td>NAMES OF THE '80s</td>\n",
       "      <td>In 1986 Howard the Duck lost at the box office...</td>\n",
       "      <td>Greg LeMond</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              CATEGORY                                               CLUE  \\\n",
       "ID                                                                          \n",
       "465         NONFICTION  She wrote the 1935 book \"Sex and Temperament i...   \n",
       "192      THAT'S A FACT  The Peruvian part of this lake covers more tha...   \n",
       "323   WORLD LITERATURE  Born Jean-Baptiste Poquelin in 1622, he wrote ...   \n",
       "393      OLD TESTAMENT            She died while giving birth to Benjamin   \n",
       "283  NAMES OF THE '80s  In 1986 Howard the Duck lost at the box office...   \n",
       "\n",
       "             ANSWER  \n",
       "ID                   \n",
       "465   Margaret Mead  \n",
       "192        Titicaca  \n",
       "323  Moli&egrave;re  \n",
       "393          Rachel  \n",
       "283     Greg LeMond  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# make sure you have loaded the data\n",
    "df.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6d03182",
   "metadata": {},
   "source": [
    "- Everyone gets their own sample of 50 questions from our 500 question sample.\n",
    "- There will be overlap with our answers, which we want to measure agreement.\n",
    "- To keep the list of possible labels small, we are going to label questions as belonging to one of five categories that correspond to the most frequent topics in *Jeopardy*:\n",
    "  - History\n",
    "  - Geography\n",
    "  - Literature\n",
    "  - Science\n",
    "  - Other\n",
    "- You can look up information about questions if you're unsure how to classify them. (Just don't ask ChatGPT.)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "79772f11",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-06T15:10:00.321415Z",
     "iopub.status.busy": "2024-07-06T15:10:00.321027Z",
     "iopub.status.idle": "2024-07-06T15:10:00.324688Z",
     "shell.execute_reply": "2024-07-06T15:10:00.324266Z",
     "shell.execute_reply.started": "2024-07-06T15:10:00.321396Z"
    }
   },
   "outputs": [],
   "source": [
    "#my_df = df.sample(50).copy()\n",
    "my_df = df.sample(10).copy()\n",
    "my_df.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "23f12125",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-06T15:10:04.524116Z",
     "iopub.status.busy": "2024-07-06T15:10:04.523634Z",
     "iopub.status.idle": "2024-07-06T15:10:04.529957Z",
     "shell.execute_reply": "2024-07-06T15:10:04.529550Z",
     "shell.execute_reply.started": "2024-07-06T15:10:04.524097Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>CATEGORY</th>\n",
       "      <th>CLUE</th>\n",
       "      <th>ANSWER</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>278</td>\n",
       "      <td>NOTABLE WOMEN</td>\n",
       "      <td>This current senator's maiden name was Dianne ...</td>\n",
       "      <td>Feinstein</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>212</td>\n",
       "      <td>GOPHER THE GUSTO</td>\n",
       "      <td>Sinclair Lewis' \"Main Street\" takes place in G...</td>\n",
       "      <td>Minnesota</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>360</td>\n",
       "      <td>I WANT TO SUCK YOUR BLOOD!</td>\n",
       "      <td>Terminix offers a 30-day guarantee when removi...</td>\n",
       "      <td>bedbugs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>39</td>\n",
       "      <td>ANCIENT HISTORY</td>\n",
       "      <td>World's most extensive archaeological dig is s...</td>\n",
       "      <td>Pompeii</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>275</td>\n",
       "      <td>THUMB ENCHANTED EVENING</td>\n",
       "      <td>The \"thumb\" on the geographic mitten that is M...</td>\n",
       "      <td>Lake Huron</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    ID                    CATEGORY  \\\n",
       "0  278               NOTABLE WOMEN   \n",
       "1  212            GOPHER THE GUSTO   \n",
       "2  360  I WANT TO SUCK YOUR BLOOD!   \n",
       "3   39             ANCIENT HISTORY   \n",
       "4  275     THUMB ENCHANTED EVENING   \n",
       "\n",
       "                                                CLUE      ANSWER  \n",
       "0  This current senator's maiden name was Dianne ...   Feinstein  \n",
       "1  Sinclair Lewis' \"Main Street\" takes place in G...   Minnesota  \n",
       "2  Terminix offers a 30-day guarantee when removi...     bedbugs  \n",
       "3  World's most extensive archaeological dig is s...     Pompeii  \n",
       "4  The \"thumb\" on the geographic mitten that is M...  Lake Huron  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "dbca8566",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-06T15:10:07.584473Z",
     "iopub.status.busy": "2024-07-06T15:10:07.584182Z",
     "iopub.status.idle": "2024-07-06T15:10:07.589347Z",
     "shell.execute_reply": "2024-07-06T15:10:07.588823Z",
     "shell.execute_reply.started": "2024-07-06T15:10:07.584454Z"
    }
   },
   "outputs": [],
   "source": [
    "def classify_jeopardy_questions(my_df):\n",
    "    # Initialize a list to store the categorizations\n",
    "    categorizations = []\n",
    "\n",
    "    # List of valid categories\n",
    "    categories = [\"History\", \"Geography\", \"Literature\", \"Science\", \"Other\"]\n",
    "\n",
    "    for index, row in my_df.iterrows():\n",
    "        # Print the CATEGORY, CLUE, and ANSWER for the current row\n",
    "        print(f\"ID: {row['ID']}\")\n",
    "        print(f\"CATEGORY: {row['CATEGORY']}\")\n",
    "        print(f\"CLUE: {row['CLUE']}\")\n",
    "        print(f\"ANSWER: {row['ANSWER']}\")\n",
    "\n",
    "        # Display category options with corresponding numbers\n",
    "        print(\"Please classify the question into one of the following categories:\")\n",
    "        for i, category in enumerate(categories, 1):\n",
    "            print(f\"{i}: {category}\")\n",
    "\n",
    "        # Ask the user for a valid categorization\n",
    "        while True:\n",
    "            try:\n",
    "                category_index = int(\n",
    "                    input(\"Enter the number corresponding to the category: \")\n",
    "                )\n",
    "                if 1 <= category_index <= len(categories):\n",
    "                    selected_category = categories[category_index - 1]\n",
    "                    # Save the categorization along with the row ID\n",
    "                    categorizations.append(\n",
    "                        {\"ID\": row[\"ID\"], \"Category\": selected_category}\n",
    "                    )\n",
    "                    break\n",
    "                else:\n",
    "                    print(\"Invalid number. Please try again.\")\n",
    "            except ValueError:\n",
    "                print(\"Invalid input. Please enter a number.\")\n",
    "\n",
    "        # Clear the output in the Jupyter notebook\n",
    "        clear_output(wait=True)\n",
    "\n",
    "    # Convert the categorizations to a DataFrame for further use if needed\n",
    "    categorizations_df = pd.DataFrame(categorizations)\n",
    "\n",
    "    # Return the categorizations DataFrame\n",
    "    return categorizations_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "277e1d71",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-06T15:10:09.403473Z",
     "iopub.status.busy": "2024-07-06T15:10:09.403197Z",
     "iopub.status.idle": "2024-07-06T15:11:11.369144Z",
     "shell.execute_reply": "2024-07-06T15:11:11.368624Z",
     "shell.execute_reply.started": "2024-07-06T15:10:09.403455Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ID: 100\n",
      "CATEGORY: A NOSE FOR GNUS\n",
      "CLUE: With horns that can bruise you, it's natural that the 2 species of gnus are black & this\n",
      "ANSWER: blue\n",
      "Please classify the question into one of the following categories:\n",
      "1: History\n",
      "2: Geography\n",
      "3: Literature\n",
      "4: Science\n",
      "5: Other\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter the number corresponding to the category:  4\n"
     ]
    }
   ],
   "source": [
    "categorized_df = classify_jeopardy_questions(my_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9beba83b-d7ad-459b-92c2-74af80f58d2e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-06T15:11:18.419389Z",
     "iopub.status.busy": "2024-07-06T15:11:18.419116Z",
     "iopub.status.idle": "2024-07-06T15:11:18.425188Z",
     "shell.execute_reply": "2024-07-06T15:11:18.424604Z",
     "shell.execute_reply.started": "2024-07-06T15:11:18.419369Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>278</td>\n",
       "      <td>History</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>212</td>\n",
       "      <td>Literature</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>360</td>\n",
       "      <td>Other</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>39</td>\n",
       "      <td>History</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>275</td>\n",
       "      <td>Geography</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>393</td>\n",
       "      <td>Literature</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>62</td>\n",
       "      <td>History</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>67</td>\n",
       "      <td>Other</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>154</td>\n",
       "      <td>Literature</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>100</td>\n",
       "      <td>Science</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    ID    Category\n",
       "0  278     History\n",
       "1  212  Literature\n",
       "2  360       Other\n",
       "3   39     History\n",
       "4  275   Geography\n",
       "5  393  Literature\n",
       "6   62     History\n",
       "7   67       Other\n",
       "8  154  Literature\n",
       "9  100     Science"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "categorized_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "05d00161",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-06T15:11:25.213967Z",
     "iopub.status.busy": "2024-07-06T15:11:25.213688Z",
     "iopub.status.idle": "2024-07-06T15:11:25.219183Z",
     "shell.execute_reply": "2024-07-06T15:11:25.218739Z",
     "shell.execute_reply.started": "2024-07-06T15:11:25.213950Z"
    }
   },
   "outputs": [],
   "source": [
    "# generate a random id to distinguish your data from others'\n",
    "random_id = random.randint(1, 10000)\n",
    "categorized_df.to_csv(f\"classified_jeopardy_{random_id}.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9614461",
   "metadata": {},
   "source": [
    "Now, we have saved your categorizations to a CSV file in your Constellate workspace.\n",
    "\n",
    "## Next steps\n",
    "\n",
    "1. Right-click your classification file (`classified_jeopardy_...csv`), and select `Download`. That will download the file to the `~/Downloads` folder on your computer.\n",
    "2. Navigate to your `Downloads` on your computer, and find your `.csv` file\n",
    "3. Upload your `.csv` to [this Dropbox folder](https://www.dropbox.com/request/ryB9Bh9QefqASXRaZfPU) for us to combine our data together:\n",
    "\n",
    "<https://www.dropbox.com/request/ryB9Bh9QefqASXRaZfPU>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cae142fa",
   "metadata": {},
   "source": [
    "## Discussion of classification experience\n",
    "\n",
    "- Was this more difficult than you anticipated?\n",
    "- How did you decide how to categorize a given question?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ec4c405",
   "metadata": {},
   "source": [
    "# Evaluating our classifications\n",
    "\n",
    "We will evaluate our classifications using the majority of evidence in the data.\n",
    "\n",
    "**N.B.:** The code below will only work on my machine for now. I will add the classifications to the course repository ASAP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fd00c246",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "PATH = \"/Users/erik/Dropbox/File requests/2024 TAPI classifications\"\n",
    "csvs = [f for f in os.listdir(PATH) if f.endswith(\".csv\")]\n",
    "# create a stacked dataframe with the csvs\n",
    "df = pd.concat([pd.read_csv(os.path.join(PATH, f)) for f in csvs], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e7f7f79a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Category</th>\n",
       "      <th>Geography</th>\n",
       "      <th>Literature</th>\n",
       "      <th>Other</th>\n",
       "      <th>Science</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ID</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>112</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>208</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>216</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>236</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>267</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>344</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>364</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Category  Geography  Literature  Other  Science\n",
       "ID                                             \n",
       "43              0.0         0.0    1.0      0.0\n",
       "66              2.0         0.0    0.0      0.0\n",
       "112             0.0         0.0    1.0      0.0\n",
       "208             0.0         0.0    1.0      0.0\n",
       "216             0.0         1.0    0.0      0.0\n",
       "236             0.0         0.0    0.0      1.0\n",
       "267             1.0         0.0    0.0      0.0\n",
       "344             0.0         0.0    0.0      1.0\n",
       "364             1.0         0.0    0.0      0.0"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_vals = df.groupby(\"ID\").value_counts().unstack().fillna(0)\n",
    "df_vals"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7515351a",
   "metadata": {},
   "source": [
    "For simplicity's sake, I am going to focus on questions where one categorization was the clear \"winner\" (i.e., exclude ties)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f1c9f91a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_row_ties(row):\n",
    "    max_value = row.max()\n",
    "    max_count = (row == max_value).sum()\n",
    "    if max_count > 1:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "\n",
    "# Apply the function to each row and create a new column for the results\n",
    "df_vals[\"Tie?\"] = df_vals.apply(check_row_ties, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a3fd1277",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# How many ties did we have?\n",
    "df_vals[\"Tie?\"].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "32a284b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the ids for rows without ties\n",
    "gold_ids = df_vals[~df_vals[\"Tie?\"]].index.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ac96e337",
   "metadata": {},
   "outputs": [],
   "source": [
    "gold_df = df_vals[df_vals.index.isin(gold_ids)]\n",
    "# drop the tie column\n",
    "gold_df.drop(\"Tie?\", axis=1, inplace=True)\n",
    "gold_df.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7f03e0b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "gold_melted = gold_df.melt(id_vars=[\"ID\"], var_name=\"Category\", value_name=\"Value\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "57144840",
   "metadata": {},
   "outputs": [],
   "source": [
    "gold_labels = gold_melted.loc[\n",
    "    gold_melted.groupby(\"ID\")[\"Value\"].idxmax(), [\"ID\", \"Category\"]\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "27794f98",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Category</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ID</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>Other</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>Geography</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>112</th>\n",
       "      <td>Other</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>208</th>\n",
       "      <td>Other</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>216</th>\n",
       "      <td>Literature</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>236</th>\n",
       "      <td>Science</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>267</th>\n",
       "      <td>Geography</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>344</th>\n",
       "      <td>Science</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>364</th>\n",
       "      <td>Geography</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Category\n",
       "ID             \n",
       "43        Other\n",
       "66    Geography\n",
       "112       Other\n",
       "208       Other\n",
       "216  Literature\n",
       "236     Science\n",
       "267   Geography\n",
       "344     Science\n",
       "364   Geography"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gold_labels.set_index(\"ID\", inplace=True)\n",
    "gold_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b4c5168-6503-4c42-a12b-b08f316f8bc9",
   "metadata": {},
   "source": [
    "> In your menu bar, run Git / Pull from Remote to download `gold_labels.csv`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08f850a3",
   "metadata": {},
   "source": [
    "# Using our gold-standard data\n",
    "\n",
    "- We created human-labeled data indicating correct results for our classifications.\n",
    "  - (In a real research setting, this would be a more meticulous and expert-driven process. For this class, it's fine.)\n",
    "- Now we need to test the LLM on these classification tasks.\n",
    "- We will evaluate its performance using the data we created and a simple statistic called the [F-score](https://en.wikipedia.org/wiki/F-score)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b13d8b6",
   "metadata": {},
   "source": [
    "## Getting one basic classification result\n",
    "\n",
    "- We talked earlier about types of classification.\n",
    "- The data we have just created is suitable for either binary or multi-class classification.\n",
    "- The type of classification we get back from the LLM will be determined by the prompt we give the model.\n",
    "- Let's start with a simple prompt:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "637a8791",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-06T15:19:40.213721Z",
     "iopub.status.busy": "2024-07-06T15:19:40.213431Z",
     "iopub.status.idle": "2024-07-06T15:19:40.216469Z",
     "shell.execute_reply": "2024-07-06T15:19:40.215992Z",
     "shell.execute_reply.started": "2024-07-06T15:19:40.213703Z"
    }
   },
   "outputs": [],
   "source": [
    "# binary classification prompt\n",
    "system_prompt = \"\"\"Determine whether the following Jeopardy question is about Literature.\n",
    "Respond in JSON like so: {\"Literature\": True}\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e3f8e47",
   "metadata": {},
   "source": [
    "> **N.B.** It is important to instruct the model to respond in JSON *even if you activate JSON mode*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3f3e5984",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-06T15:19:41.214518Z",
     "iopub.status.busy": "2024-07-06T15:19:41.214242Z",
     "iopub.status.idle": "2024-07-06T15:19:41.220387Z",
     "shell.execute_reply": "2024-07-06T15:19:41.219748Z",
     "shell.execute_reply.started": "2024-07-06T15:19:41.214500Z"
    }
   },
   "outputs": [],
   "source": [
    "# load questions\n",
    "data = pd.read_csv(\"data.csv\", index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ec46391d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-06T15:19:41.354887Z",
     "iopub.status.busy": "2024-07-06T15:19:41.354608Z",
     "iopub.status.idle": "2024-07-06T15:19:41.357904Z",
     "shell.execute_reply": "2024-07-06T15:19:41.357346Z",
     "shell.execute_reply.started": "2024-07-06T15:19:41.354870Z"
    }
   },
   "outputs": [],
   "source": [
    "def make_prompt(row):\n",
    "    prompt = f\"\"\"Category: {row['CATEGORY'].values[0]}\\nClue: {row['CLUE'].values[0]}\\nAnswer: {row['ANSWER'].values[0]}\"\"\"\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b1f99ec5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-06T15:19:43.559055Z",
     "iopub.status.busy": "2024-07-06T15:19:43.558545Z",
     "iopub.status.idle": "2024-07-06T15:19:43.562304Z",
     "shell.execute_reply": "2024-07-06T15:19:43.561864Z",
     "shell.execute_reply.started": "2024-07-06T15:19:43.559037Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Category: PRESIDENTIAL RELATIVES\n",
      "Clue: This president gave the bride away when his niece Eleanor married his cousin Franklin\n",
      "Answer: Teddy Roosevelt\n"
     ]
    }
   ],
   "source": [
    "prompt = make_prompt(data.sample(1))\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f163ca51",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-06T15:19:46.450826Z",
     "iopub.status.busy": "2024-07-06T15:19:46.450550Z",
     "iopub.status.idle": "2024-07-06T15:19:46.463222Z",
     "shell.execute_reply": "2024-07-06T15:19:46.462777Z",
     "shell.execute_reply.started": "2024-07-06T15:19:46.450808Z"
    }
   },
   "outputs": [],
   "source": [
    "# remember, this uses the API key in your .env file:\n",
    "load_dotenv()\n",
    "client = OpenAI()\n",
    "\n",
    "# if you didn't run the code in lesson 1, you probably don't have an .env file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "97f0e258",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-06T15:19:51.213875Z",
     "iopub.status.busy": "2024-07-06T15:19:51.213598Z",
     "iopub.status.idle": "2024-07-06T15:19:51.217581Z",
     "shell.execute_reply": "2024-07-06T15:19:51.217105Z",
     "shell.execute_reply.started": "2024-07-06T15:19:51.213857Z"
    }
   },
   "outputs": [],
   "source": [
    "def make_completion(\n",
    "    system_prompt, prompt, print_prompt=True, client=client, model=\"gpt-4o\", json=True\n",
    "):\n",
    "    completion = client.chat.completions.create(\n",
    "        model=model,\n",
    "        response_format={\"type\": \"json_object\"} if json else None,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": prompt},\n",
    "        ],\n",
    "    )\n",
    "    if print_prompt:\n",
    "        print(f\"System prompt: {system_prompt}\\n{'-' * 80}\")\n",
    "        print(f\"User prompt: {prompt}\\n{'-' * 80}\")\n",
    "        print(f\"Assistant response: {completion.choices[0].message.content}\")\n",
    "\n",
    "    return completion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "47bdfbc5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-06T15:19:59.213604Z",
     "iopub.status.busy": "2024-07-06T15:19:59.213318Z",
     "iopub.status.idle": "2024-07-06T15:19:59.947916Z",
     "shell.execute_reply": "2024-07-06T15:19:59.947474Z",
     "shell.execute_reply.started": "2024-07-06T15:19:59.213586Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "System prompt: Determine whether the following Jeopardy question is about Literature.\n",
      "Respond in JSON like so: {\"Literature\": True}\n",
      "--------------------------------------------------------------------------------\n",
      "User prompt: Category: PRESIDENTIAL RELATIVES\n",
      "Clue: This president gave the bride away when his niece Eleanor married his cousin Franklin\n",
      "Answer: Teddy Roosevelt\n",
      "--------------------------------------------------------------------------------\n",
      "Assistant response: {\"Literature\": false}\n"
     ]
    }
   ],
   "source": [
    "c = make_completion(system_prompt, prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e44b4fab",
   "metadata": {},
   "source": [
    "Ok, that returns a binary response.\n",
    "\n",
    "Now we can put these things together and see how text classifications can quickly become data.\n",
    "\n",
    "We'll start with one row that we know has a gold-standard label:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e25a9312",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "System prompt: Determine whether the following Jeopardy question is about Literature.\n",
      "Respond in JSON like so: {\"Literature\": True}\n",
      "--------------------------------------------------------------------------------\n",
      "User prompt: Category: LEADERS\n",
      "Clue: Viktor Orban is the anti-immigrant leader of this European country\n",
      "Answer: Hungary\n",
      "--------------------------------------------------------------------------------\n",
      "Assistant response: {\"Literature\": false}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{66: {'Literature': False}}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output = dict()\n",
    "\n",
    "id = random.choice(gold_ids)\n",
    "row = data[data.index == id]\n",
    "prompt = make_prompt(row)\n",
    "c = make_completion(system_prompt, prompt)\n",
    "output[id] = json.loads(c.choices[0].message.content)\n",
    "output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a75cb99",
   "metadata": {},
   "source": [
    "Now we can put all of this together to automatically process a batch of questions using the `gold_ids`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c29fc726",
   "metadata": {},
   "outputs": [],
   "source": [
    "l = list()\n",
    "\n",
    "for id in random.sample(gold_ids, 20):\n",
    "    d = dict()\n",
    "    row = data[data.index == id]\n",
    "    prompt = make_prompt(row)\n",
    "    c = make_completion(system_prompt, prompt, print=False)\n",
    "    d[\"ID\"] = id\n",
    "    d.update(json.loads(c.choices[0].message.content))\n",
    "    l.append(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09edcadf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Literature_LLM</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ID</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>112</th>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>208</th>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>216</th>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Literature_LLM\n",
       "ID                 \n",
       "43            False\n",
       "66            False\n",
       "112            True\n",
       "208           False\n",
       "216            True"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output = pd.DataFrame(l)\n",
    "output.columns = [\"ID\", \"Literature_LLM\"]\n",
    "output.set_index(\"ID\", inplace=True)\n",
    "output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa5a0678",
   "metadata": {},
   "source": [
    "## Comparing LLM classifications to human classifications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9adce49",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Category</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ID</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>Other</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>Geography</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>112</th>\n",
       "      <td>Other</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>208</th>\n",
       "      <td>Other</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>216</th>\n",
       "      <td>Literature</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Category\n",
       "ID             \n",
       "43        Other\n",
       "66    Geography\n",
       "112       Other\n",
       "208       Other\n",
       "216  Literature"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gold_labels.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb55d556",
   "metadata": {},
   "source": [
    "How often do humans and the LLM agree on binary classification (Literature vs. Not Literature)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb7fe8af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Literature_LLM</th>\n",
       "      <th>Category</th>\n",
       "      <th>Category_Literature</th>\n",
       "      <th>Human and LLM Agree</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ID</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>False</td>\n",
       "      <td>Other</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>False</td>\n",
       "      <td>Geography</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>112</th>\n",
       "      <td>True</td>\n",
       "      <td>Other</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>208</th>\n",
       "      <td>False</td>\n",
       "      <td>Other</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>216</th>\n",
       "      <td>True</td>\n",
       "      <td>Literature</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Literature_LLM    Category  Category_Literature  Human and LLM Agree\n",
       "ID                                                                       \n",
       "43            False       Other                False                 True\n",
       "66            False   Geography                False                 True\n",
       "112            True       Other                False                False\n",
       "208           False       Other                False                 True\n",
       "216            True  Literature                 True                 True"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agree_df = output.merge(gold_labels, left_index=True, right_index=True)\n",
    "agree_df[\"Category_Literature\"] = agree_df[\"Category\"] == \"Literature\"\n",
    "agree_df[\"Human and LLM Agree\"] = (\n",
    "    agree_df[\"Category_Literature\"] == agree_df[\"Literature_LLM\"]\n",
    ")\n",
    "agree_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f397bac6",
   "metadata": {},
   "source": [
    "This would be the most basic way of measuring the success rate of your classification: How often does the model output match gold-standard data?\n",
    "\n",
    "However, there is a more sophisticated and widely used solution for binary classification: the F-score.\n",
    "\n",
    "## The F-Score\n",
    "\n",
    "To understand the F-score, you need to understand two related concepts: precision and recall.\n",
    "\n",
    "### Precision\n",
    "\n",
    "Precision measures how many of the items the model identified as `True` were really `True` according to the gold standard data. That is compared against the number of items identified as `True` that were `False` according to the gold standard data, which are known as \"False positives.\"\n",
    "\n",
    "> Precision answers the question: \"How many retrieved items were relevant?\"\n",
    "\n",
    "$Precision = \\frac{True \\ Positives}{True \\ Positives + False \\ Positives}$\n",
    "\n",
    "### Recall\n",
    "\n",
    "Recall measures how many values that ought to have been `True` were labeled `True`.\n",
    "\n",
    "> Recall answers the question: \"How many relevant items were retrieved?\"\n",
    "\n",
    "$Recall = \\frac{True \\ Positives}{True \\ Positives + False \\ Negatives}$\n",
    "\n",
    "### F-score (aka F1)\n",
    "\n",
    "The F score is the harmonic mean of precision and recall.\n",
    "\n",
    "$F_{1}= 2 \\times \\frac{Precision \\times Recall}{Precision + Recall}$\n",
    "\n",
    "## How to calculate\n",
    "\n",
    "There is an easy way to calculate this score using [sklearn](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.f1_score.html):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "71709276",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-06T15:23:07.307051Z",
     "iopub.status.busy": "2024-07-06T15:23:07.306655Z",
     "iopub.status.idle": "2024-07-06T15:23:07.313377Z",
     "shell.execute_reply": "2024-07-06T15:23:07.312976Z",
     "shell.execute_reply.started": "2024-07-06T15:23:07.307031Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8571428571428571"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example with fake data\n",
    "y_true = [0, 1, 0, 0, 1, 1]\n",
    "y_pred = [0, 1, 1, 0, 1, 1]\n",
    "f1_score(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fa2c2fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6666666666666666"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Because Python stores True as 1 and False as 0, we can directly use the columns:\n",
    "f1_score(agree_df[\"Category_Literature\"], agree_df[\"Literature_LLM\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a18f9ca7",
   "metadata": {},
   "source": [
    "You can also use the F-score to evaluate multi-class classifications.\n",
    "\n",
    "We'll stick with binary for now for simplicity's sake."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a47fc2d5",
   "metadata": {},
   "source": [
    "# Putting it all together\n",
    "\n",
    "Here is how the things we learned today come together:\n",
    "\n",
    "1. We have our texts to classify (*Jeopardy!* questions)\n",
    "2. We created gold-standard (i.e., human-labeled, and, ideally, expert-labeled) classifications for testing.\n",
    "3. We wrote prompts and set up API calls that output structured classifications as JSON.\n",
    "4. We can systematically evaluate the quality of our classification using the F score.\n",
    "\n",
    "## Next steps\n",
    "\n",
    "There are two big remaining steps for our classification:\n",
    "\n",
    "1. Quantifying uncertainty\n",
    "\n",
    "As you experienced doing classifications by hand, some judgments were easier than others. We can ask the LLM to express its confidence in its judgments numerically.\n",
    "\n",
    "This is useful because it allows us to sort automatically labeled data by confidence for review. Low confidence classifications deserve higher priority for manual review (and possible correction) by researchers. (You could also do some clever massaging of the F score by penalizing confident but wrong classifications, while lessening the penalty for low confidence wrong classifications.)\n",
    "\n",
    "2. Prompt engineering\n",
    "\n",
    "We can use the F score to systematically test and evaluate variations of our `system` and `user` prompts to see which prompts produce the most accurate classifications."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cc0d4e7",
   "metadata": {},
   "source": [
    "# Quantifying uncertainty\n",
    "\n",
    "This one is surprisingly easy. It requires a small modification of the system prompt:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ef7c42f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = \"\"\"Determine whether the following Jeopardy question is about Literature.\n",
    "Express your confidence in your classification as a percentage from 50 to 100, where 50 is guessing and 100 is certain.\n",
    "Respond in JSON like so:\n",
    "{\"Literature\": True,\n",
    "\"Confidence\": 95}\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "63a49388",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = make_prompt(data.sample(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "100a32ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "System prompt: Determine whether the following Jeopardy question is about Literature.\n",
      "Express your confidence in your classification as a percentage from 50 to 100, where 50 is guessing and 100 is certain.\n",
      "Respond in JSON like so:\n",
      "{\"Literature\": True,\n",
      "\"Confidence\": 95}\n",
      "--------------------------------------------------------------------------------\n",
      "User prompt: Category: COMPOSERS\n",
      "Clue: In 1943 American composer William Schuman became the first composer to win one of these prizes\n",
      "Answer: Pulitzer\n",
      "--------------------------------------------------------------------------------\n",
      "Assistant response: {\n",
      "\"Literature\": false,\n",
      "\"Confidence\": 100\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "c = make_completion(system_prompt, prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43941e04",
   "metadata": {},
   "source": [
    "We can use confidence scores to prioritize review of low-confidence (i.e., `50%`-`75%`) responses."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eca42822",
   "metadata": {},
   "source": [
    "# Exercises\n",
    "\n",
    "1. Using what we have learned today, try writing a multi-class classification `system` prompt for *Jeopardy* questions that will output structured JSON with the predefined options we used to create the gold standard data.\n",
    "2. Explain how the F score differs from merely calculating the percentage of the time that the gold-standard data and the classification model agree.\n",
    "3. Write a paragraph explaining how you could apply these techniques to a different set of texts than *Jeopardy* questions."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
