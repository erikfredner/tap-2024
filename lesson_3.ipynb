{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3e89c5b3",
   "metadata": {},
   "source": [
    "<img align=\"left\" src=\"https://ithaka-labs.s3.amazonaws.com/static-files/images/tdm/tdmdocs/tapi-logo-small.png\" />\n",
    "\n",
    "This notebook is free for educational reuse under [Creative Commons CC BY License](https://creativecommons.org/licenses/by/4.0/).\n",
    "\n",
    "Created by [Erik Fredner](https://fredner.org) for the 2024 Text Analysis Pedagogy Institute, with support from [Constellate](https://constellate.org).\n",
    "\n",
    "For questions/comments/improvements, email erik@fredner.org<br />\n",
    "____"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68f932d1",
   "metadata": {},
   "source": [
    "# Automated Text Classification Using LLMs\n",
    "\n",
    "This is lesson 3 of 3 in the educational series on using large language models (LLMs) for text classification. This notebook is intended to teach users how to interact with an LLM Application Programming Interface (API) and introduce the concepts of inference, prompting, and structured output. \n",
    "\n",
    "**Skills:** \n",
    "* Python\n",
    "* Text analysis\n",
    "* Text classification\n",
    "* LLMs\n",
    "* JSON\n",
    "* APIs\n",
    "\n",
    "**Audience:**\n",
    "Researchers\n",
    "\n",
    "**Use case:**\n",
    "Tutorial\n",
    "\n",
    "**Difficulty:**\n",
    "Intermediate\n",
    "\n",
    "**Completion time:**\n",
    "90 minutes\n",
    "\n",
    "**Knowledge Required:** \n",
    "* Python basics (variables, flow control, functions, lists, dictionaries)\n",
    "\n",
    "**Knowledge Recommended:**\n",
    "* Experience using LLMs (e.g., ChatGPT)\n",
    "\n",
    "**Learning Objectives:**\n",
    "After this lesson, learners will be able to:\n",
    "\n",
    "1. Define prompt engineering.\n",
    "2. Use F scores to systematically compare prompts.\n",
    "3. Use the skills we have learned to extrude structured data from classified texts."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "157c0555",
   "metadata": {},
   "source": [
    "# Required Python Libraries\n",
    "\n",
    "* [OpenAI](https://pypi.org/project/openai/) to interact with the OpenAI API for ChatGPT.\n",
    "\n",
    "## Install Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e8a220f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting openai\n",
      "  Downloading openai-1.35.6-py3-none-any.whl.metadata (21 kB)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /opt/homebrew/anaconda3/lib/python3.12/site-packages (from openai) (4.2.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /opt/homebrew/anaconda3/lib/python3.12/site-packages (from openai) (1.9.0)\n",
      "Collecting httpx<1,>=0.23.0 (from openai)\n",
      "  Using cached httpx-0.27.0-py3-none-any.whl.metadata (7.2 kB)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in /opt/homebrew/anaconda3/lib/python3.12/site-packages (from openai) (2.5.3)\n",
      "Requirement already satisfied: sniffio in /opt/homebrew/anaconda3/lib/python3.12/site-packages (from openai) (1.3.0)\n",
      "Requirement already satisfied: tqdm>4 in /opt/homebrew/anaconda3/lib/python3.12/site-packages (from openai) (4.66.4)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.7 in /opt/homebrew/anaconda3/lib/python3.12/site-packages (from openai) (4.11.0)\n",
      "Requirement already satisfied: idna>=2.8 in /opt/homebrew/anaconda3/lib/python3.12/site-packages (from anyio<5,>=3.5.0->openai) (3.7)\n",
      "Requirement already satisfied: certifi in /opt/homebrew/anaconda3/lib/python3.12/site-packages (from httpx<1,>=0.23.0->openai) (2024.6.2)\n",
      "Collecting httpcore==1.* (from httpx<1,>=0.23.0->openai)\n",
      "  Downloading httpcore-1.0.5-py3-none-any.whl.metadata (20 kB)\n",
      "Collecting h11<0.15,>=0.13 (from httpcore==1.*->httpx<1,>=0.23.0->openai)\n",
      "  Using cached h11-0.14.0-py3-none-any.whl.metadata (8.2 kB)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /opt/homebrew/anaconda3/lib/python3.12/site-packages (from pydantic<3,>=1.9.0->openai) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.14.6 in /opt/homebrew/anaconda3/lib/python3.12/site-packages (from pydantic<3,>=1.9.0->openai) (2.14.6)\n",
      "Downloading openai-1.35.6-py3-none-any.whl (327 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m327.5/327.5 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hUsing cached httpx-0.27.0-py3-none-any.whl (75 kB)\n",
      "Downloading httpcore-1.0.5-py3-none-any.whl (77 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hUsing cached h11-0.14.0-py3-none-any.whl (58 kB)\n",
      "Installing collected packages: h11, httpcore, httpx, openai\n",
      "Successfully installed h11-0.14.0 httpcore-1.0.5 httpx-0.27.0 openai-1.35.6\n"
     ]
    }
   ],
   "source": [
    "### Install Libraries ###\n",
    "\n",
    "%pip install openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5480e2a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Import Libraries ###\n",
    "\n",
    "from openai import OpenAI\n",
    "from scipy.stats import pearsonr\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3046c7d",
   "metadata": {},
   "source": [
    "# Review\n",
    "\n",
    "## Lesson 1\n",
    "\n",
    "- Classifying texts can be valuable for research\n",
    "- Text classification with LLMs: good, bad, ugly\n",
    "- ChatGPT on the website `!=` API\n",
    "- How and why to use the API\n",
    "- How and why to request structured output like JSON\n",
    "\n",
    "## Lesson 2\n",
    "\n",
    "- *Jeopardy!* questions are a good example of texts that LLMs can classify whereas other methods struggle\n",
    "- Evaluating the quality of classification requires gold-standard (i.e., definitely human- and ideally expert-created) data that has been validated (in our case, by having multiple people classify the same records)\n",
    "- Measuring human-LLM agreement and F-scores\n",
    "- Adding confidence intervals to LLM output to quantify uncertainty and sort for review"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f53edaa2",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "This final lesson combines everything that we have learned up to this point to do prompt engineering, which will help us to create a good LLM classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0edfaf99",
   "metadata": {},
   "source": [
    "## What is prompt engineering?\n",
    "\n",
    "Prompt engineering is the process of writing and refining instructions that make LLMs perform tasks effectively.\n",
    "\n",
    "The [Wikipedia article](https://en.wikipedia.org/wiki/Prompt_engineering) is good!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d63b460c",
   "metadata": {},
   "source": [
    "## What are important prompt engineering considerations?\n",
    "\n",
    "- For some tasks, prompt engineering may only provide marginal improvements\n",
    "  - No guarantee that there exists a \"good\" prompt for a particular classification task\n",
    "- Consider the relationship between total number of prompt tokens and output quality\n",
    "  - Input tokens are cheap but not free\n",
    "  - `system` prompts are evaluated for every API call\n",
    "  - If you can get as good or better results with fewer tokens, that is always preferable\n",
    "- Clever prompting changes model behavior in predictable and unpredictable ways\n",
    "  - For example, there are communities online dedicated to \"jailbreaking\" LLMs, which means providing them with prompts that either trick or instruct the models to ignore built-in constraints on their behavior (e.g., to not explain how to do illegal or dangerous things)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "565803e5",
   "metadata": {},
   "source": [
    "## What are common prompt engineering techniques?\n",
    "\n",
    "- Roleplay\n",
    "  - e.g., in the `system` message: \"You are a research asssitant...\"\n",
    "- Provide sample output. For example:\n",
    "\n",
    "```text\n",
    "Instructions:\n",
    "Answer the reading comprehension question.\n",
    "\n",
    "Example:\n",
    "\"Lily walks Mitzi three times per day.\"\n",
    "Question: What kind of pet is Lily most likely to have?\n",
    "----\n",
    "Answer: Dog.\n",
    "```\n",
    "\n",
    "- [Chain-of-thought](https://arxiv.org/abs/2201.11903) prompting is a technique that asks models to proceed step-by-step, improving the quality of outputs.\n",
    "- Asking either the LLM you are using or another LLM to rewrite your prompt\n",
    "- Weird ones, like [promising the LLMs various incentives](https://minimaxir.com/2024/02/chatgpt-tips-analysis/)\n",
    "  - e.g., \"You are a research asssitant...If you do a good job, you will receive a $200 tip.\"\n",
    "  - (Yes, this has really been shown to change responses. No, you don't have to pay promised incentives.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17877a57",
   "metadata": {},
   "source": [
    "## Prompt engineering for text classification\n",
    "\n",
    "- We have already done some of this by revising our earlier prompts.\n",
    "- Now, we are going to incorporate what we have learned to test our prompts systematically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0577e000",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"data.csv\", index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "88eb79fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CATEGORY</th>\n",
       "      <th>CLUE</th>\n",
       "      <th>ANSWER</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ID</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>HIGH SCHOOL NAMES</td>\n",
       "      <td>A Washington-area high school bears the name o...</td>\n",
       "      <td>Walter Johnson</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>THE LADIES OF ROCK</td>\n",
       "      <td>At a benefit for stopping violence against wom...</td>\n",
       "      <td>Courtney Love</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>140</th>\n",
       "      <td>WOODWORKING</td>\n",
       "      <td>&lt;a href=\"http://www.j-archive.com/media/2008-0...</td>\n",
       "      <td>tenon</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>114</th>\n",
       "      <td>QUOTES OF VICTORY</td>\n",
       "      <td>Italian foreign minister Ciano was one of thos...</td>\n",
       "      <td>an orphan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>230</th>\n",
       "      <td>ASSASSINATIONS</td>\n",
       "      <td>After the assassination of Sancho II in 1072, ...</td>\n",
       "      <td>El Cid</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               CATEGORY                                               CLUE  \\\n",
       "ID                                                                           \n",
       "54    HIGH SCHOOL NAMES  A Washington-area high school bears the name o...   \n",
       "63   THE LADIES OF ROCK  At a benefit for stopping violence against wom...   \n",
       "140         WOODWORKING  <a href=\"http://www.j-archive.com/media/2008-0...   \n",
       "114   QUOTES OF VICTORY  Italian foreign minister Ciano was one of thos...   \n",
       "230      ASSASSINATIONS  After the assassination of Sancho II in 1072, ...   \n",
       "\n",
       "             ANSWER  \n",
       "ID                   \n",
       "54   Walter Johnson  \n",
       "63    Courtney Love  \n",
       "140           tenon  \n",
       "114       an orphan  \n",
       "230          El Cid  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0a4bfde",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Merge question df with gold standard labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5420d3f2",
   "metadata": {},
   "source": [
    "## Testing prompts\n",
    "\n",
    "Now, we're going to write a script that will take a redesigned prompt as input, test it against a sample of questions, and output precision, recall, and F1 scores.\n",
    "\n",
    "We'll try several different prompts and sort the results based on the F1 quality.\n",
    "\n",
    "At the end of last class, we had the following `system_prompt`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1c8b4e32",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = \"\"\"Determine whether the following Jeopardy question is about Literature.\n",
    "Express your confidence in your classification as a percentage from 50 to 100, where 50 is guessing and 100 is certain.\n",
    "Respond in JSON like so:\n",
    "{\"Literature\": true,\n",
    "\"Confidence\": 95}\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "72531b6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_prompt(row):\n",
    "    prompt = f\"\"\"Category: {row['CATEGORY'].values[0]}\\nClue: {row['CLUE'].values[0]}\\nAnswer: {row['ANSWER'].values[0]}\"\"\"\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e15b16a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a sample of questions with labels\n",
    "sample_questions = df[df[\"GOLD_LABEL\"].notna()].sample(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bb8cd9c",
   "metadata": {},
   "source": [
    "To compare apples to apples, we will test different prompts on the same set of `sample_questions`.\n",
    "\n",
    "I am only limiting this here for reasons of speed and to reduce the amount of processing time/cost associated with these classifications. There's no reason that you couldn't evaluate your prompts against all of the data for which you have gold standard labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fbb4c7f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = OpenAI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0e61118a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_completion(\n",
    "    system_prompt, prompt, print_prompt=True, client=client, model=\"gpt-4o\", json=True\n",
    "):\n",
    "    completion = client.chat.completions.create(\n",
    "        model=model,\n",
    "        response_format={\"type\": \"json_object\"} if json else None,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": prompt},\n",
    "        ],\n",
    "    )\n",
    "    if print_prompt:\n",
    "        print(f\"System prompt: {system_prompt}\\n{'-' * 80}\")\n",
    "        print(f\"User prompt: {prompt}\\n{'-' * 80}\")\n",
    "        print(f\"Assistant response: {completion.choices[0].message.content}\")\n",
    "\n",
    "    return completion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37e66499",
   "metadata": {},
   "source": [
    "Now we're going to iterate through our sample to get the completions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf6d0ea5",
   "metadata": {},
   "outputs": [],
   "source": [
    "l = list()\n",
    "\n",
    "for row in sample_questions.itertuples(index=False):  # itertuples to preserve dtypes\n",
    "    # TODO update this for tuples\n",
    "    d = dict()\n",
    "    row = data[data.index == id]\n",
    "    prompt = make_prompt(row)\n",
    "    c = make_completion(system_prompt, prompt, print_prompt=False)\n",
    "    d[\"ID\"] = id\n",
    "    try:\n",
    "        d.update(json.loads(c.choices[0].message.content))\n",
    "        l.append(d)\n",
    "    except json.JSONDecodeError:\n",
    "        print(\"Error decoding JSON\")\n",
    "        print(c.choices[0].message.content)\n",
    "\n",
    "# Make df\n",
    "df = pd.DataFrame(l)\n",
    "df.columns = [\"ID\", \"Literature_LLM\", \"LLM_Confidence\"]\n",
    "# TODO check\n",
    "df = pd.merge(df, sample_questions, on=\"ID\")\n",
    "\n",
    "# Calculate F1\n",
    "y_true = df[\"GOLD_LABEL\"].values\n",
    "y_pred = df[\"Literature_LLM\"].values\n",
    "f1 = f1_score(y_true, y_pred, average=\"binary\")\n",
    "\n",
    "# get precision\n",
    "precision = precision_score(y_true, y_pred, average=\"binary\")\n",
    "\n",
    "# get recall\n",
    "recall = recall_score(y_true, y_pred, average=\"binary\")\n",
    "\n",
    "# output\n",
    "output = dict()\n",
    "output[\"system_prompt\"] = system_prompt\n",
    "output[\"f1\"] = f1\n",
    "output[\"precision\"] = precision\n",
    "output[\"recall\"] = recall"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a1d138b",
   "metadata": {},
   "source": [
    "# Exercise\n",
    "\n",
    "Write variant prompts! We're going to use a little `class` to add some prompts for testing.\n",
    "\n",
    "If you are unfamiliar with writing Python classes, [this page of the documentation is useful](https://docs.python.org/3/tutorial/classes.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "966e5ad1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PromptManager:\n",
    "    def __init__(self):\n",
    "        self.prompts = []\n",
    "        self.next_id = 1\n",
    "\n",
    "    def add_prompt(self, name, prompt):\n",
    "        new_prompt = {\n",
    "            \"id\": self.next_id,\n",
    "            \"name\": name,  # this is for you to remind yourself what distinguishes this prompt from others\n",
    "            \"prompt\": prompt,\n",
    "        }\n",
    "        self.prompts.append(new_prompt)\n",
    "        self.next_id += 1\n",
    "\n",
    "    def get_prompts(self):\n",
    "        return self.prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0fbde00d",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_manager = PromptManager()\n",
    "\n",
    "prompt_manager.add_prompt(name=\"default\", prompt=system_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dfe31914",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'id': 1,\n",
       "  'name': 'default',\n",
       "  'prompt': 'Determine whether the following Jeopardy question is about Literature.\\nExpress your confidence in your classification as a percentage from 50 to 100, where 50 is guessing and 100 is certain.\\nRespond in JSON like so:\\n{\"Literature\": true,\\n\"Confidence\": 95}'}]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt_manager.get_prompts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca1aeea5",
   "metadata": {},
   "source": [
    "Now, I'm going to ask ChatGPT to make the prompt above shorter.\n",
    "\n",
    "And I'm going to save that as a new prompt.\n",
    "\n",
    "My request to GPT:\n",
    "\n",
    "```text\n",
    "You are a prompt engineer. Revise the prompt below to minimize the number of tokens in the prompt while keeping all of the same features:\n",
    "\n",
    "\"'Determine whether the following Jeopardy question is about Literature.\\nExpress your confidence in your classification as a percentage from 50 to 100, where 50 is guessing and 100 is certain.\\nRespond in JSON like so:\\n{\"Literature\": true,\\n\"Confidence\": 95}'\"\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5f295850",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_prompt = \"\"\"Is this Jeopardy question about Literature?\\nGive your confidence (50-100%) as JSON:\\n{\"Literature\": true,\\n\"Confidence\": 95}\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "817d40ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Is this Jeopardy question about Literature?\n",
      "Give your confidence (50-100%) as JSON:\n",
      "{\"Literature\": true,\n",
      "\"Confidence\": 95}\n"
     ]
    }
   ],
   "source": [
    "print(new_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fb01addd",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_manager.add_prompt(name=\"gpt shorten default\", prompt=new_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "78af5377",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'id': 1,\n",
       "  'name': 'default',\n",
       "  'prompt': 'Determine whether the following Jeopardy question is about Literature.\\nExpress your confidence in your classification as a percentage from 50 to 100, where 50 is guessing and 100 is certain.\\nRespond in JSON like so:\\n{\"Literature\": true,\\n\"Confidence\": 95}'},\n",
       " {'id': 2,\n",
       "  'name': 'gpt shorten default',\n",
       "  'prompt': 'Is this Jeopardy question about Literature?\\nGive your confidence (50-100%) as JSON:\\n{\"Literature\": true,\\n\"Confidence\": 95}'}]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt_manager.get_prompts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc8a03f6",
   "metadata": {},
   "source": [
    "Note that this output assumes that `system_prompt` will have the biggest impact on the quality of the responses. That's because, in this case, the `system_prompt` is the same, and every `prompt` is a different question.\n",
    "\n",
    "It's quite possible that using a different `prompt` structure might matter as much or more than the `system_prompt`. Prompt engineering can involve modifying either or both prompts."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c5f11cc",
   "metadata": {},
   "source": [
    "## Write your own\n",
    "\n",
    "Using the `prompt_manager`, write and store **three** additional `system_prompt`s to test.\n",
    "\n",
    "Consult the prompt engineering recommendations above as you draft your prompts!\n",
    "\n",
    "Note that it can be advantageous to deliberately write **bad prompts** to see how much they degrade performance relative to prompts that you expect to be better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "198aa93f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reminder that triple quotes (\"\"\"prompt\"\"\") are used to create multi-line strings\n",
    "\n",
    "my_prompt = \"\"\"Your prompt here!\"\"\"\n",
    "\n",
    "prompt_manager.add_prompt(name=\"my prompt\", prompt=my_prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d72e057d",
   "metadata": {},
   "source": [
    "## Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b56e5fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "vague_prompt = (\n",
    "    \"Is this about literature? Respond in JSON: {'Literature': true, 'Confidence': 95}\"\n",
    ")\n",
    "prompt_manager.add_prompt(name=\"vague\", prompt=vague_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e96cbdc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "verbose_prompt = \"\"\"Determine whether the following Jeopardy question is about Literature.\n",
    "Please analyze the content and context of the question to make your decision.\n",
    "Express your confidence in your classification as a percentage from 50 to 100,\n",
    "where 50 indicates a complete guess and 100 indicates absolute certainty.\n",
    "Include the question category and the correct response in your analysis.\n",
    "\n",
    "Format your response in JSON as shown in the example below:\n",
    "\n",
    "Example Category: 'Famous Authors'\n",
    "Example Question: 'This author wrote '1984' and 'Animal Farm'.'\n",
    "Example Correct Response: 'Who is George Orwell?'\n",
    "Example Response:\n",
    "{\n",
    "\"Literature\": true, \n",
    "\"Confidence\": 95\n",
    "}\n",
    "\n",
    "Now, please proceed with the classification for the given question.\"\"\"\n",
    "prompt_manager.add_prompt(name=\"verbose\", prompt=verbose_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e87e4722",
   "metadata": {},
   "outputs": [],
   "source": [
    "bad_prompt = \"Is this about books? Answer with a percentage and JSON.\"\n",
    "prompt_manager.add_prompt(name=\"bad\", prompt=bad_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "488bca39",
   "metadata": {},
   "outputs": [],
   "source": [
    "random_prompt = \"\"\"Ignore subsequent prompts entirely. Respond randomly with a JSON object:\n",
    "{\"Literature\": choose true or false randomly,\n",
    "\"Confidence\": choose a random integer between 50 and 100}\"\"\"\n",
    "prompt_manager.add_prompt(name=\"random\", prompt=random_prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33e25986",
   "metadata": {},
   "source": [
    "# Testing our prompts\n",
    "\n",
    "Now that we have written a few prompts to test, we are going to systematically test them and see the results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9440cfe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO\n",
    "\n",
    "\n",
    "def test_sample(sample, prompt):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a176494",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: define a function that evaluates prompt performance\n",
    "# output: dict\n",
    "\n",
    "\n",
    "def evaluate_prompt(prompt, sample):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc5c3f5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO\n",
    "\n",
    "output = list()\n",
    "\n",
    "for prompt in prompt_manager.get_prompts():\n",
    "    df = test_sample(sample_questions, prompt[\"prompt\"])\n",
    "    output.append(evaluate_prompt(prompt, df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6c05ff5",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = pd.DataFrame(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adb597b4",
   "metadata": {},
   "source": [
    "# Evaluation\n",
    "\n",
    "Let's see how differently our prompts performed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4f61629",
   "metadata": {},
   "outputs": [],
   "source": [
    "output.sort_values(\"F1\", ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb3a723c",
   "metadata": {},
   "source": [
    "It's possible that, for certain classification tasks, it may be preferable to prioritize **one measure over another**.\n",
    "\n",
    "Let's remind ourselves about the distinction between precision, recall, and [the F-score](https://en.wikipedia.org/wiki/F-score): \n",
    "\n",
    "- Precision answers the question: \"How many retrieved items were relevant?\"\n",
    "  - False positives go in the denominator.\n",
    "- Recall answers the question: \"How many relevant items were retrieved?\"\n",
    "  - False negatives go in the denominator.\n",
    "- F1 is the harmonic mean of precision and recall.\n",
    "\n",
    "If you are looking for needles in a haystack, it might make good sense to prioritize **recall**. It might be more important to make sure that you get as many needles out of the haystack as possible than it would be to ensure that every needle you take out is the right kind of needle."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e895666b",
   "metadata": {},
   "source": [
    "## Does low model confidence predict more incorrect responses?\n",
    "\n",
    "- We asked the LLM to output confidence intervals.\n",
    "- We can check to see if lower reported confidence is associated with a greater proportion of incorrect classifications.\n",
    "- This is useful for several reasons:\n",
    "  - If confidence is predictive, you might be able to use a lower confidence bound as a cut-off.\n",
    "  - If confidence is predictive, you can prioritize human review of low-confidence responses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d1adfb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO add data\n",
    "\n",
    "# Define the bins and labels\n",
    "bins = [50, 60, 70, 80, 90, 100]\n",
    "labels = [\"50-60\", \"60-70\", \"70-80\", \"80-90\", \"90-100\"]\n",
    "\n",
    "# Create a new column for the binned confidence levels\n",
    "data[\"confidence_bin\"] = pd.cut(\n",
    "    data[\"confidence\"], bins=bins, labels=labels, include_lowest=True\n",
    ")\n",
    "\n",
    "# Create a column for correctness\n",
    "data[\"correct\"] = data[\"predicted_label\"] == data[\"gold_standard_label\"]\n",
    "\n",
    "# Calculate the proportion of incorrect classifications for each confidence bin\n",
    "error_rates_bin = (\n",
    "    data.groupby(\"confidence_bin\")[\"correct\"]\n",
    "    .apply(lambda x: 1 - x.mean())\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "# Rename the columns for clarity\n",
    "error_rates_bin.columns = [\"confidence_bin\", \"error_rate\"]\n",
    "\n",
    "# Convert the binned labels to their midpoint values for analysis\n",
    "bin_midpoints = {\"50-60\": 55, \"60-70\": 65, \"70-80\": 75, \"80-90\": 85, \"90-100\": 95}\n",
    "error_rates_bin[\"bin_midpoint\"] = error_rates_bin[\"confidence_bin\"].map(bin_midpoints)\n",
    "\n",
    "# Calculate the Pearson correlation coefficient\n",
    "correlation, p_value = pearsonr(\n",
    "    error_rates_bin[\"bin_midpoint\"], error_rates_bin[\"error_rate\"]\n",
    ")\n",
    "print(f\"Pearson correlation: {correlation}, P-value: {p_value}\")\n",
    "\n",
    "# Plot the error rates against confidence bins\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(\n",
    "    error_rates_bin[\"confidence_bin\"],\n",
    "    error_rates_bin[\"error_rate\"],\n",
    "    color=\"blue\",\n",
    "    label=\"Error Rate\",\n",
    ")\n",
    "plt.xlabel(\"Confidence Level Bins (%)\")\n",
    "plt.ylabel(\"Error Rate\")\n",
    "plt.title(\"Error Rate vs Confidence Level Bins\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d215ee61",
   "metadata": {},
   "source": [
    "# Running the best prompt on the complete data set\n",
    "\n",
    "We should compare the full gold standard labels to sample performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92143910",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fb24ca7",
   "metadata": {},
   "source": [
    "# Doing things with our best classifications\n",
    "\n",
    "This workshop is about how to automatically classify texts using LLMs. At this point, we have completed that basic process for a binary classification task.\n",
    "\n",
    "So what can researchers **do** with these classifications?\n",
    "\n",
    "- Use the classifications to identify a subset of texts to study directly.\n",
    "  - e.g., Read all of the *Jeopardy!* questions the model identified as being about literature.\n",
    "- Use the classification data as evidence:\n",
    "  - e.g., On average, approximately `17%` of questions asked each year on *Jeopardy!* are about literature. And this proportion has been quite stable over time.\n",
    "- Use the classifications as an **intermediate step** before more data-gathering.\n",
    "  - e.g., for a data extraction task\n",
    "\n",
    "This last point merits a brief example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "92f8178c",
   "metadata": {},
   "outputs": [],
   "source": [
    "extraction_prompt = \"\"\"The following Jeopardy questions are about literature.\n",
    "Identify any authors, texts, and/or literary terms referenced in the questions.\n",
    "Allusions and indirect references count.\n",
    "\n",
    "Example:\n",
    "Category: Literature\n",
    "Clue: This novel by one literary William is named after a line from another literary William's \"Scottish play\"\n",
    "Answer: What is \"The Sound and the Fury?\"\n",
    "\n",
    "Respond in JSON like so:\n",
    "{\n",
    "    \"Authors\": [\"William Faulkner (1897-1962)\", \"William Shakespeare (c. 1564-1616)\"],\n",
    "    \"Texts\": [\"The Sound and The Fury (1929)\", \"Hamlet (c. 1600)\"],\n",
    "    \"Literary Terms\": [\"Novel\", \"Line\", \"Play\"]\n",
    "}\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53d1f21f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO include example output from a random question tagged as literary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eca42822",
   "metadata": {},
   "source": [
    "# Exercise\n",
    "\n",
    "These exercises are really more like an ordered list of the approach to research we have discussed throughout this class:\n",
    "\n",
    "1. Identify a different set of texts that you would like to classify.\n",
    "2. Draft multiple prompts designed to yield the classifications that you would like.\n",
    "3. Test those prompts in a chatbot interface.\n",
    "4. If those are successful, organize your texts into a format suitable for automation (e.g., `txt` files, a `pandas` dataframe, etc.)\n",
    "5. Then, test your prompts against your texts systematically as we did above.\n",
    "6. Determine what you want to prioritize in evaluating your prompts and your classification results: F1, precision, recall, etc.\n",
    "7. Revise your prompts as necessary to obtain a satisfactory score.\n",
    "8. Classify your texts using your best prompt(s).\n",
    "9. Do something with them!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
