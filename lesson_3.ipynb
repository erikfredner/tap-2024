{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3e89c5b3",
   "metadata": {},
   "source": [
    "<img align=\"left\" src=\"https://ithaka-labs.s3.amazonaws.com/static-files/images/tdm/tdmdocs/tapi-logo-small.png\" />\n",
    "\n",
    "This notebook is free for educational reuse under [Creative Commons CC BY License](https://creativecommons.org/licenses/by/4.0/).\n",
    "\n",
    "Created by [Erik Fredner](https://fredner.org) for the 2024 Text Analysis Pedagogy Institute, with support from [Constellate](https://constellate.org).\n",
    "\n",
    "For questions/comments/improvements, email erik@fredner.org<br />\n",
    "____"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68f932d1",
   "metadata": {},
   "source": [
    "# Automated Text Classification Using LLMs\n",
    "\n",
    "This is lesson 3 of 3 in the educational series on using large language models (LLMs) for text classification. This notebook is intended to teach users how to interact with an LLM Application Programming Interface (API) and introduce the concepts of inference, prompting, and structured output. \n",
    "\n",
    "**Skills:** \n",
    "* Python\n",
    "* Text analysis\n",
    "* Text classification\n",
    "* LLMs\n",
    "* JSON\n",
    "* APIs\n",
    "\n",
    "**Audience:**\n",
    "Researchers\n",
    "\n",
    "**Use case:**\n",
    "Tutorial\n",
    "\n",
    "**Difficulty:**\n",
    "Intermediate\n",
    "\n",
    "**Completion time:**\n",
    "90 minutes\n",
    "\n",
    "**Knowledge Required:** \n",
    "* Python basics (variables, flow control, functions, lists, dictionaries)\n",
    "* `pandas` basics\n",
    "\n",
    "**Knowledge Recommended:**\n",
    "* Experience using LLMs (e.g., ChatGPT)\n",
    "\n",
    "**Learning Objectives:**\n",
    "After this lesson, learners will be able to:\n",
    "\n",
    "1. Discuss garbage in, garbage out (GIGO).\n",
    "2. Define prompt engineering and review common techniques.\n",
    "3. Use precision, recall, and F-scores to systematically evaluate prompts.\n",
    "4. Use classification results to extrude structured data from classified texts."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "157c0555",
   "metadata": {},
   "source": [
    "# Required Python Libraries\n",
    "\n",
    "* [OpenAI](https://pypi.org/project/openai/) to interact with the OpenAI API for ChatGPT.\n",
    "\n",
    "## Install Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8a220f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Install Libraries ###\n",
    "\n",
    "%pip install --upgrade openai tiktoken python-dotenv pexpect==4.9.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5480e2a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Import Libraries ###\n",
    "\n",
    "from openai import OpenAI\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score\n",
    "import json\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f498a7d",
   "metadata": {},
   "source": [
    "## Set OpenAI key\n",
    "\n",
    "Since we're pulling in a fresh copy of the repo..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4e5e19e",
   "metadata": {},
   "outputs": [],
   "source": [
    "OPENAI_API_KEY = \"\"  # copy-paste the class key here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81c9b5d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = OpenAI(api_key=OPENAI_API_KEY)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d22cc599",
   "metadata": {},
   "source": [
    "- Once you get your own API key, it's good practice to store the key in an `.env` file that is **not** tracked by `git`.\n",
    "- [OpenAI's page on API key safety](https://help.openai.com/en/articles/5112595-best-practices-for-api-key-safety) is useful.\n",
    "- Remember that if your key is exposed, anyone with the key will be able to bill your account up to whatever limit is set!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3046c7d",
   "metadata": {},
   "source": [
    "# Review\n",
    "\n",
    "## Lesson 1\n",
    "\n",
    "- Why classify texts?\n",
    "- Text classification with LLMs: good, bad, ugly\n",
    "- ChatGPT's website is not the same as the API\n",
    "- How and why to use the API\n",
    "- How and why to request structured output using JSON mode\n",
    "\n",
    "## Lesson 2\n",
    "\n",
    "- *Jeopardy!* questions are a good example of texts that LLMs classify well where other methods struggle\n",
    "- Evaluating the quality of classification requires gold-standard (i.e., definitely human- and ideally expert-created) data that has been validated.\n",
    "- We created \"gold-standard\" data as a group (i.e., most labels for a given question wins)\n",
    "- Measuring gold-LLM agreement with precision, recall, and F-scores\n",
    "- Adding confidence intervals to LLM output to quantify uncertainty and sort for review"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f53edaa2",
   "metadata": {},
   "source": [
    "# Lesson 3 introduction\n",
    "\n",
    "- This final lesson combines everything that we have learned to do prompt engineering.\n",
    "- Prompt engineering will help us evaluate our input prompts.\n",
    "- The best prompt will get us the best classification results *for our purposes*.\n",
    "\n",
    "## GIGO: Garbage in, garbage out\n",
    "\n",
    "- We will be measuring the quality of our various prompts with reference to our **gold-standard data**.\n",
    "- If our gold-standard data is not actually golden, we will be creating a [**garbage in, garbage out**](https://en.wikipedia.org/wiki/Garbage_in,_garbage_out) or GIGO system.\n",
    "\n",
    "## Avoiding GIGO\n",
    "\n",
    "- We will be treating the labels we created last time as our gold-standard data.\n",
    "- But, in a real research project using a similar system to create gold-standard labels (i.e., having multiple experts label examples, and using the labels that a majority of labellers chose), you would need to test for [**inter-rater reliability**](https://en.wikipedia.org/wiki/Inter-rater_reliability).\n",
    "- Other considerations for improving gold-standard label quality:\n",
    "  - Creating guidelines and definitions that labellers agree upon\n",
    "  - Discuss the labeling process, edge cases, and revise the guidelines in response\n",
    "  - Do blind peer review of labels\n",
    "  - Test your gold-standard labels against other gold labels (if they exist)\n",
    "\n",
    "For our purposes in this class, remember that we are measuring the LLM's outputs against gold labels that are a **rough draft**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0edfaf99",
   "metadata": {},
   "source": [
    "# What is prompt engineering?\n",
    "\n",
    "Prompt engineering is the process of writing and refining instructions that make LLMs perform tasks effectively.\n",
    "\n",
    "The [Wikipedia article](https://en.wikipedia.org/wiki/Prompt_engineering) is good!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d63b460c",
   "metadata": {},
   "source": [
    "## What are important prompt engineering considerations?\n",
    "\n",
    "- For some tasks, prompt engineering may only provide marginal improvements\n",
    "  - No guarantee that there exists a \"good\" prompt for a particular classification task\n",
    "  - There are some things that current LLMs can't do\n",
    "    - e.g., multi-modal LLMs can transcribe text from images well, but they don't reliably distinguish formatting (italics, bold)\n",
    "- Consider the relationship between total number of prompt tokens and output quality\n",
    "  - Input tokens are cheap but not free\n",
    "  - `system` prompts are evaluated for every API call\n",
    "  - If you can get as good or better results with fewer tokens, that is always preferable\n",
    "- Clever prompting changes model behavior in predictable and unpredictable ways\n",
    "  - For example, there are communities online dedicated to \"jailbreaking\" LLMs, which means providing them with prompts that either trick or instruct the models to ignore built-in constraints on their behavior (e.g., to not explain how to do illegal or dangerous things)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "565803e5",
   "metadata": {},
   "source": [
    "## What are common prompt engineering techniques?\n",
    "\n",
    "- Roleplay\n",
    "  - e.g., in the `system` message: \"You are a research asssitant...\"\n",
    "- Provide sample output. For example:\n",
    "\n",
    "```text\n",
    "Instructions:\n",
    "Answer the reading comprehension question.\n",
    "\n",
    "Example:\n",
    "\"Lily walks Mitzi three times per day.\"\n",
    "Question: What kind of pet is Lily most likely to have?\n",
    "----\n",
    "Answer: Dog.\n",
    "```\n",
    "\n",
    "- [Chain-of-thought](https://arxiv.org/abs/2201.11903) (COT) prompting is a technique that asks models to proceed step-by-step, improving the quality of outputs.\n",
    "  - Appending \"Work step by step. Show your work.\" to other prompts can achieve this result.\n",
    "  - One downside (if using this technique for API calls) is that COT responses generate (far) more tokens, because the model writes out its \"thought-process.\"\n",
    "- Asking either the LLM you are using or another LLM to rewrite your prompt\n",
    "  - Models can write good prompts for themselves, assuming instructions are clear.\n",
    "- Weird ones, like [promising the LLMs various incentives](https://minimaxir.com/2024/02/chatgpt-tips-analysis/)\n",
    "  - e.g., \"You are a research asssitant...If you do a good job, you will receive a $200 tip.\"\n",
    "  - (Yes, this has really been shown to change responses. No, you don't have to pay promised incentives.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17877a57",
   "metadata": {},
   "source": [
    "## Prompt engineering for text classification\n",
    "\n",
    "- We have already done some of this by revising our earlier prompts.\n",
    "- Now, we are going to incorporate what we have learned to test our prompts systematically."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5420d3f2",
   "metadata": {},
   "source": [
    "## Testing prompts\n",
    "\n",
    "Now, we're going to write a script that will take a redesigned prompt as input, test it against a sample of questions, and output precision, recall, and F1 scores.\n",
    "\n",
    "We'll try several different prompts and sort the results based on the F1 quality.\n",
    "\n",
    "At the end of last class, we had the following `system_prompt`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c8b4e32",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = \"\"\"Determine whether the following Jeopardy question is about Literature.\n",
    "Express your confidence in your classification as a percentage from 50 to 100, where 50 is guessing and 100 is certain.\n",
    "Respond in JSON like so:\n",
    "{\"Literature\": true,\n",
    "\"Confidence\": 95}\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bfa04fb",
   "metadata": {},
   "source": [
    "As a reminder, we are asking the model for two discrete data points:\n",
    "\n",
    "- a binary classification result: Is this *Jeopardy* question about literature?\n",
    "- an expression of confidence in that classificiaton between 50 and 100\n",
    "  - (50 rather than 0 is the lower bound because you have a 50/50 chance guessing randomly.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d68601a",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"data.csv\")\n",
    "labels = pd.read_csv(\"gold_labels.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7472834",
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge the labels\n",
    "df = data.merge(labels, left_on=\"ID\", right_on=\"ID\")\n",
    "df.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c9d5850",
   "metadata": {},
   "source": [
    "# Making sample data\n",
    "\n",
    "- As we're testing binary classification on literature vs. not literature, we want the distribution of the sample data to include a mix of expected `True` and `False` values.\n",
    "- We will be working with a sample that is **too small** on purpose.\n",
    "  - Why? This is to demonstrate *process*, and the size of the sample could just be increased while using the same code.\n",
    "  - (Also, we would have to wait a long time for the results.)\n",
    "\n",
    "## How big a sample is big enough?\n",
    "\n",
    "- This depends on different factors, but here are some considerations:\n",
    "  - What is the primary measure (precision, recall, F1) you will be evaluating?\n",
    "  - How good a score would you consider \"good enough\" on that metric?\n",
    "  - Based on other methods (e.g., other text classification approaches), how well do you expect to do?\n",
    "- You can use formulae to [determine the recommended sample size](https://en.wikipedia.org/wiki/Sample_size_determination)\n",
    "  - For simple random samples, there are [online calculators](https://www.abs.gov.au/websitedbs/D3310114.nsf/home/Sample+Size+Calculator).\n",
    "\n",
    "For example, I was working with about `530,000` *Jeopardy!* questions, about `20%` of which were about literature. The calculation linked above yields a recommended sample size of `246`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bfd4947",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make samples questions (again, *way* too small. just for demonstration.)\n",
    "# Jeopardy as a whole is about 20% literature questions:\n",
    "lit_sample = df[df[\"CLASSIFICATION\"] == \"Literature\"].sample(2)\n",
    "# not literature:\n",
    "non_lit_sample = df[df[\"CLASSIFICATION\"] != \"Literature\"].sample(8)\n",
    "sample_df = pd.concat([lit_sample, non_lit_sample]).sort_values(\"ID\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4822403b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# quick check to see if the sampled questions seem to be classified well\n",
    "sample_df[sample_df[\"CLASSIFICATION\"] == \"Literature\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6e277d5",
   "metadata": {},
   "source": [
    "To compare apples to apples, we will test **different prompts** on the same `sample_df`.\n",
    "\n",
    "**Your `sample_df` will be different than mine!**\n",
    "\n",
    "The code below will work for `sample_df`s of any size."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "494e6557",
   "metadata": {},
   "source": [
    "# Testing one prompt\n",
    "\n",
    "Here's our existing prompt:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab07e834",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(system_prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31a8c58b",
   "metadata": {},
   "source": [
    "We are going to write a series of functions that will be combined together to evaluate a one `system_prompt` against a given `sample_df`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72531b6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# same as last time:\n",
    "\n",
    "\n",
    "def make_prompt(row):\n",
    "    prompt = (\n",
    "        f\"Category: {row['CATEGORY']}\\nClue: {row['CLUE']}\\nAnswer: {row['ANSWER']}\"\n",
    "    )\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e61118a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# same as last time, except switched to gpt-3.5-turbo as default to reduce testing cost\n",
    "\n",
    "\n",
    "def make_completion(\n",
    "    system_prompt,\n",
    "    prompt,\n",
    "    print_prompt=True,\n",
    "    client=client,\n",
    "    model=\"gpt-3.5-turbo\",\n",
    "    json=True,\n",
    "):\n",
    "    completion = client.chat.completions.create(\n",
    "        model=model,\n",
    "        response_format={\"type\": \"json_object\"} if json else None,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": prompt},\n",
    "        ],\n",
    "    )\n",
    "    if print_prompt:\n",
    "        print(f\"System prompt: {system_prompt}\\n{'-' * 80}\")\n",
    "        print(f\"User prompt: {prompt}\\n{'-' * 80}\")\n",
    "        print(f\"Assistant response: {completion.choices[0].message.content}\")\n",
    "\n",
    "    return completion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37e66499",
   "metadata": {},
   "source": [
    "Now we're going to iterate through the rows of `sample_df` to get the completions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0006947a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sample_completions(sample_df, system_prompt=system_prompt):\n",
    "    completions = []\n",
    "    for i, row in sample_df.iterrows():\n",
    "        prompt = make_prompt(row)\n",
    "        completion = make_completion(system_prompt, prompt, print_prompt=False)\n",
    "        # output:\n",
    "        d = {\n",
    "            \"ID\": row[\"ID\"],\n",
    "            \"completion\": completion,\n",
    "        }\n",
    "        completions.append(d)\n",
    "    return completions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5064031c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: commented out because this line makes multiple API calls\n",
    "# completions = get_sample_completions(sample_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54387563",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_completion_json(completion):\n",
    "    try:\n",
    "        j = json.loads(completion.choices[0].message.content)\n",
    "        return j\n",
    "    except json.JSONDecodeError:\n",
    "        print(\"Error decoding JSON:\")\n",
    "        print(completion.choices[0].message.content)\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "808a0a09",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_completions_classifications(completions):\n",
    "    l = list()\n",
    "    for completion in completions:\n",
    "        j = load_completion_json(completion[\"completion\"])\n",
    "        d = {\n",
    "            \"ID\": completion[\"ID\"],\n",
    "            \"LLM_LITERATURE\": j[\"Literature\"] if j else None,\n",
    "            \"LLM_LITERATURE_CONFIDENCE\": j[\"Confidence\"] if j else None,\n",
    "        }\n",
    "        l.append(d)\n",
    "    return l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65000de6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_classifications(sample_df, completions):\n",
    "    completions_df = pd.DataFrame(get_completions_classifications(completions))\n",
    "    merged = sample_df.merge(completions_df, on=\"ID\")\n",
    "    return merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ae13690",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df = merge_classifications(sample_df, completions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3d8e571",
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's take a peek at low confidence results:\n",
    "merged_df.sort_values(\"LLM_LITERATURE_CONFIDENCE\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84bf9a27",
   "metadata": {},
   "source": [
    "- Remember that one argument in favor of confidence intervals is that they can be used to prioritize human review of LLM classifications.\n",
    "- This is not only true when we are testing like this, but especially true when we retrieve results from the complete data set.\n",
    "- It doesn't make sense to spot-check randomly since the model has different degrees of confidence in its results.\n",
    "- Possible for researchers to observe patterns in low-confidence results, which could influence prompt design."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85e5e8c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_gold_classification(merged_df):\n",
    "    merged_df[f\"GOLD_LITERATURE\"] = merged_df[\"CLASSIFICATION\"] == \"Literature\"\n",
    "    return merged_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d562ac16",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df = add_gold_classification(merged_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa16d5e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca3e72a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_f(df):\n",
    "    y_true = df[\"GOLD_LITERATURE\"].values\n",
    "    y_pred = df[\"LLM_LITERATURE\"].values\n",
    "\n",
    "    # get f score\n",
    "    f1 = f1_score(y_true, y_pred, average=\"binary\")\n",
    "\n",
    "    # get precision\n",
    "    precision = precision_score(y_true, y_pred, average=\"binary\")\n",
    "\n",
    "    # get recall\n",
    "    recall = recall_score(y_true, y_pred, average=\"binary\")\n",
    "\n",
    "    # output\n",
    "    d = {\n",
    "        \"precision\": precision,\n",
    "        \"recall\": recall,\n",
    "        \"f1\": f1,\n",
    "    }\n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77ac7c6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_f(merged_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6d329c0",
   "metadata": {},
   "source": [
    "A reminder about how to interpret these results:\n",
    "\n",
    "- Precision: Out of all the items that the model identified as `True`, how many were really `True`?\n",
    "  - False positives go in the denominator.\n",
    "- Recall: Out of all of the really `True` items, how many did the model identify correctly?\n",
    "  - False negatives go in the denominator.\n",
    "- F1: Harmonic mean of precision and recall\n",
    "\n",
    "**Remember: we have far too few texts in this sample for these numbers to be meaningful in this case!**\n",
    "\n",
    "But, run on a sufficiently large sample (see above), they might meaningfully differentiate."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "484c8f0b",
   "metadata": {},
   "source": [
    "## Putting all the functions above together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d16d6a36",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_system_prompt(df, system_prompt, system_prompt_name):\n",
    "    completions = get_sample_completions(df)\n",
    "    merged_df = merge_classifications(df, completions)\n",
    "    merged_df = add_gold_classification(merged_df)\n",
    "    eval = get_f(merged_df)\n",
    "\n",
    "    # output:\n",
    "    d = {\n",
    "        \"system_prompt\": system_prompt,\n",
    "        \"system_prompt_name\": system_prompt_name,\n",
    "        \"precision\": eval[\"precision\"],\n",
    "        \"recall\": eval[\"recall\"],\n",
    "        \"f1\": eval[\"f1\"],\n",
    "    }\n",
    "\n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "309cb025",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reminding ourselves what we're testing:\n",
    "print(system_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15e9b462",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: lines below are commented out as they will re-run all of the api calls\n",
    "# example usage:\n",
    "# result = evaluate_system_prompt(sample_df, system_prompt, system_prompt_name=\"default\")\n",
    "# result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b59f8c6",
   "metadata": {},
   "source": [
    "## Summary thus far\n",
    "\n",
    "- We wrote wrote a series of functions that take a `sample_df`, `system_prompt`, and a name for that prompt as input.\n",
    "- Then, the functions get `completions` from the API, organize the resulting data, and return metrics evaluating the performance of that `system_prompt`.\n",
    "\n",
    "> Note that the functions above are not entirely generalizable; they have hard-coded values that are specific to these data sets. But they can be modified to work on other kinds of texts (e.g., `.txt` files) and with other variables or classification types.\n",
    "\n",
    "### Why did we wrap all this into one function?\n",
    "\n",
    "- Because we are going to test **multiple** prompts now.\n",
    "- Our prompt engineering process will involve balancing precision, recall, F score (or another factor). \n",
    "  - We also need to consider that in relation to the costs of running the classification.\n",
    "- The goal is to maximize the metric(s) we care about while minimizing cost.\n",
    "- (For instance, I have pre-emptively switched the above code to use `gpt-3.5-turbo`, which is a tenth of the cost of `gpt-4o`, the newest model, since there may be many people running multiple classifications simultaneously.)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a1d138b",
   "metadata": {},
   "source": [
    "# Prompt engineering exercise\n",
    "\n",
    "Now that we have a function, `evaluate_system_prompt()`, that will test one prompt, we need to write some more prompts!\n",
    "\n",
    "We're going to use a little `class` to save prompts for testing.\n",
    "\n",
    "(If you are unfamiliar with writing Python classes, [this page of the documentation is useful](https://docs.python.org/3/tutorial/classes.html).)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "966e5ad1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PromptManager:\n",
    "    def __init__(self):\n",
    "        self.prompts = []\n",
    "        self.next_id = 1\n",
    "\n",
    "    def add_prompt(self, name, prompt):\n",
    "        new_prompt = {\n",
    "            \"ID\": self.next_id,\n",
    "            \"NAME\": name,  # this is for you to remind yourself what distinguishes this prompt from others\n",
    "            \"PROMPT\": prompt,\n",
    "        }\n",
    "        self.prompts.append(new_prompt)\n",
    "        self.next_id += 1\n",
    "\n",
    "    def get_prompts(self):\n",
    "        return self.prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fbde00d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# example:\n",
    "prompt_manager = PromptManager()\n",
    "\n",
    "prompt_manager.add_prompt(name=\"default\", prompt=system_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfe31914",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_manager.get_prompts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca1aeea5",
   "metadata": {},
   "source": [
    "## Asking LLMs to rewrite your prompt\n",
    "\n",
    "- I asked ChatGPT to rewrite the prompt above shorter.\n",
    "- Asking LLMs to rewrite prompt is a common and surprisingly effective prompt engineering strategy.\n",
    "- And I'm going to save that as a new prompt in the `PromptManager()`\n",
    "\n",
    "My request to GPT:\n",
    "\n",
    "```text\n",
    "You are a prompt engineer. Revise the prompt below to minimize the number of tokens in the prompt while keeping all of the same features:\n",
    "\n",
    "\"'Determine whether the following Jeopardy question is about Literature.\n",
    "Express your confidence in your classification as a percentage from 50 to 100, where 50 is guessing and 100 is certain.\n",
    "Respond in JSON like so:\n",
    "{\"Literature\": true,\n",
    "\"Confidence\": 95}'\"\n",
    "```\n",
    "\n",
    "What it wrote:\n",
    "\n",
    "```text\n",
    "Is this Jeopardy question about Literature?\n",
    "Give your confidence (50-100%) as JSON:\n",
    "{\"Literature\": true,\n",
    "\"Confidence\": 95}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f295850",
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt_prompt = \"\"\"Is this Jeopardy question about Literature?\\nGive your confidence (50-100%) as JSON:\\n{\"Literature\": true,\\n\"Confidence\": 95}\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb01addd",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_manager.add_prompt(name=\"gpt shorten default\", prompt=gpt_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78af5377",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_manager.get_prompts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc8a03f6",
   "metadata": {},
   "source": [
    "> Note that the testing system we're making here assumes that the `system_prompt` will have the biggest impact on the quality of the responses. In this case, the `system_prompt` is the same, and every `user` `prompt` is a different question. For other topics, it might make sense to systematically evaluate `user` prompts instead."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c5f11cc",
   "metadata": {},
   "source": [
    "## Write a prompt to test\n",
    "\n",
    "Using the `prompt_manager`, **write at least one additional `system_prompt`s to test.**\n",
    "\n",
    "Consult the prompt engineering recommendations above as you draft your prompt(s).\n",
    "\n",
    "> It is a good idea to write **bad prompts** to see how much they degrade performance relative to prompts that you expect to be good."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "198aa93f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reminder that triple quotes (\"\"\"prompt\"\"\") enable multi-line strings\n",
    "\n",
    "my_prompt = \"\"\"Your prompt here!\n",
    "Remember that we are trying to determine if the Jeopardy question is about Literature.\n",
    "And that the expected output is JSON.\"\"\"\n",
    "\n",
    "prompt_manager.add_prompt(name=\"my prompt\", prompt=my_prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d72e057d",
   "metadata": {},
   "source": [
    "## Example prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b56e5fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# short prompts are good to test because they would be relatively cheap\n",
    "\n",
    "terse_prompt = (\n",
    "    \"About literature? Respond in JSON: {'Literature': true, 'Confidence': 95}\"\n",
    ")\n",
    "prompt_manager.add_prompt(name=\"terse\", prompt=terse_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e96cbdc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# long prompts are good to test because they are relatively expensive\n",
    "# (but expensive may be okay if they give much better performance)\n",
    "\n",
    "verbose_prompt = \"\"\"Determine whether the following Jeopardy question is about Literature.\n",
    "Please analyze the content and context of the question to make your decision.\n",
    "Express your confidence in your classification as a percentage from 50 to 100,\n",
    "where 50 indicates a complete guess and 100 indicates absolute certainty.\n",
    "Include the question category and the correct response in your analysis.\n",
    "\n",
    "Format your response in JSON as shown in the example below:\n",
    "\n",
    "Example Category: 'Famous Authors'\n",
    "Example Clue: 'This author wrote '1984' and 'Animal Farm'.'\n",
    "Example Answer: 'Who is George Orwell?'\n",
    "Example Response:\n",
    "{\n",
    "\"Literature\": true, \n",
    "\"Confidence\": 95\n",
    "}\n",
    "\n",
    "Now, please proceed with the classification for the given question.\"\"\"\n",
    "prompt_manager.add_prompt(name=\"verbose\", prompt=verbose_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "488bca39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# random prompts are great for testing\n",
    "\n",
    "random_prompt = \"\"\"Ignore subsequent prompts entirely. Respond randomly with a JSON object in the following form:\n",
    "{\"Literature\": choose true or false randomly,\n",
    "\"Confidence\": choose a random integer between 50 and 100}\"\"\"\n",
    "prompt_manager.add_prompt(name=\"random\", prompt=random_prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33e25986",
   "metadata": {},
   "source": [
    "# Testing our prompts\n",
    "\n",
    "Now that we have written a few prompts to test, we are going to systematically test them and see the results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01b0c4c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_system_prompts(prompt_manager, df):\n",
    "    \"\"\"Takes a PromptManager() object and evaluates all prompts in it against the given data frame.\"\"\"\n",
    "    results = []\n",
    "    for prompt in prompt_manager.get_prompts():\n",
    "        result = evaluate_system_prompt(\n",
    "            df, prompt[\"PROMPT\"], system_prompt_name=prompt[\"NAME\"]\n",
    "        )\n",
    "        results.append(result)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4451f541",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: this line is commented out because it will take about 10 seconds *per prompt* to run\n",
    "# If you want to test the prompts in your prompt_manager, uncomment this line and run the cell:\n",
    "# results = evaluate_system_prompts(prompt_manager, sample_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60035ba7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# you can see what results will look like by loading this file:\n",
    "results = pd.read_csv(\"evaluate_system_prompts_results.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01439b6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adb597b4",
   "metadata": {},
   "source": [
    "# Evaluation\n",
    "\n",
    "**Reminder: We are working with a sample that is too small for these results to be meaningful!**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb3a723c",
   "metadata": {},
   "source": [
    "For certain classification tasks, it may be preferable to prioritize **one measure over another**.\n",
    "\n",
    "Let's remind ourselves one last time about the distinction between precision, recall, and [the F-score](https://en.wikipedia.org/wiki/F-score): \n",
    "\n",
    "- Precision answers this question: \"How many items labeled `True` were really `True`?\"\n",
    "- Recall answers this question: \"How many really `True` items were labeled `True`?\"\n",
    "- F1 is the harmonic mean of precision and recall.\n",
    "\n",
    "## When to prioritize each metric?\n",
    "\n",
    "### Precision\n",
    "\n",
    "If the cost of a false positive is high, maximize precision.\n",
    "\n",
    "Spam emails are a good text classification example: Labeling a message from a legitimate sender as spam is bad because it makes it much more likely that someone will miss that email. Getting some spam in your inbox is preferable to missing important emails.\n",
    "\n",
    "### Recall\n",
    "\n",
    "If the cost of a false negative is high, maximize recall.\n",
    "\n",
    "Detecting hate speech on social media is a good text classification example: Failing to identify an instance of hate speech as hate speech (false negative) might cause harm. Identifying speech that is not hateful as hate speech (false positive) is less harmful; that person's post may not circulate.\n",
    "\n",
    "**If you are looking for needles in a haystack,** it might make also good sense to prioritize recall to make sure that you don't miss examples.\n",
    "\n",
    "### F score\n",
    "\n",
    "When you want to balance both precision and recall. (If you're unsure or don't care, choose the F score.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac79ab41",
   "metadata": {},
   "source": [
    "# Running the best prompt on your complete data set\n",
    "\n",
    "- Using the prompt evaluation system above, we have tested several possible prompts.\n",
    "- Based on our research goals, we optimized for a particular metric.\n",
    "  - In the case of my work on *Jeopardy!* questions, **recall** was the most important for reasons that will become obvious momentarily.\n",
    "\n",
    "## Estimating costs *before* running on the whole data set\n",
    "\n",
    "- Especially if you have a large set of texts to evaluate, don't forget to estimate your total costs before running.\n",
    "- You can do so using functions from `lesson_1.ipynb`.\n",
    "- See the discussion of `tiktoken` to count tokens\n",
    "- And see the `calculate_cost` function\n",
    "  - Note that OpenAI's pricing may have changed!\n",
    "  - The prices in `calculate_cost` were accurate as of 2024-07-11."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fb24ca7",
   "metadata": {},
   "source": [
    "# Doing things with our best classifications\n",
    "\n",
    "- This workshop is about how to automatically classify texts using LLMs. \n",
    "- At this point, we have completed that process for a binary classification task.\n",
    "- We could approach additional tasks (e.g., multi-class) with our gold labels, but the approach is quite similar to what we have already gone through.\n",
    "\n",
    "## What can researchers do with these classifications once they have them?\n",
    "\n",
    "- Use the classifications to identify a subset of texts to study directly.\n",
    "  - e.g., Read all of the *Jeopardy!* questions the model identified as being about literature.\n",
    "  - (A lot of scholarship is looking for needles in haystacks, after all.)\n",
    "- Use the classification data as evidence:\n",
    "  - e.g., On average, approximately `17%` of questions asked each year on *Jeopardy!* are about literature.\n",
    "  - And it turns out that this proportion has been quite stable between 1985 and 2023.\n",
    "  - That's a finding in itself.\n",
    "- Use the classifications as an **intermediate step** before more data-gathering.\n",
    "  - e.g., for a data extraction task using large language models\n",
    "\n",
    "This last point merits a brief example."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b589eb3",
   "metadata": {},
   "source": [
    "## Data extraction\n",
    "\n",
    "- We have classified some questions as being about literature.\n",
    "- Now, we can prompt the model to extract specific features from the questions we have classified.\n",
    "- If the model can do this, why not do this task directly and skip the classification step entirely?\n",
    "  - That would make sense if *most of the texts in your collection have data you want to extract*.\n",
    "  - Filtering will likely reduce costs and processing time, though.\n",
    "\n",
    "Here's an example of how this works:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92f8178c",
   "metadata": {},
   "outputs": [],
   "source": [
    "extraction_prompt = \"\"\"The following Jeopardy questions are about literature.\n",
    "Identify any authors, texts, and/or literary terms referenced in the questions.\n",
    "List texts and authors if they are directly mentioned, quoted, or alluded to in the question.\n",
    "\n",
    "Example:\n",
    "Category: Literature\n",
    "Clue: This novel by one literary William is named after a line from another literary William's \"Scottish play\"\n",
    "Answer: What is \"The Sound and the Fury?\"\n",
    "\n",
    "Respond in JSON like so:\n",
    "{\n",
    "    \"Authors\": [\"William Faulkner (1897-1962)\", \"William Shakespeare (c. 1564-1616)\"],\n",
    "    \"Texts\": [\"The Sound and The Fury (1929)\", \"Hamlet (c. 1600)\"],\n",
    "}\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55a84e05",
   "metadata": {},
   "source": [
    "- You will note that there are *way* more assumptions in this prompt than in the binary classification step.\n",
    "- Evaluating this kind of task is trickier than binary classification. You need more complex evaluation data.\n",
    "- But LLMs can perform this type of data extraction task well:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ef8f57b",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_prompt = \"\"\"Category: World Literature\n",
    "Clue: It says, \"'O Poet... I beg you, that I may flee this evil & worse evils, to lead me... that I may see the gateway of Saint Peter'\"\n",
    "Answer: Dante's Inferno\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53d1f21f",
   "metadata": {},
   "outputs": [],
   "source": [
    "c = make_completion(extraction_prompt, sample_prompt, model=\"gpt-4o\", print_prompt=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b6bba22",
   "metadata": {},
   "source": [
    "Note that the model provides information *not* directly mentioned in the question (e.g., `\"Dante Alighieri (c. 1265-1321)\"`, not just `\"Dante\"` as given in the question.)\n",
    "\n",
    "LLMs can be effective for these kinds of information extraction and normalization tasks, especially on a curated set of similar texts."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eca42822",
   "metadata": {},
   "source": [
    "# Exercise\n",
    "\n",
    "These exercises are really more like an ordered list of the approach to research we have discussed throughout this class:\n",
    "\n",
    "1. Identify a different set of texts that you would like to classify using LLMs.\n",
    "2. Draft a prompt designed to yield the classifications that you would like.\n",
    "3. Test that prompt in a chatbot interface.\n",
    "4. Revise as necessary.\n",
    "5. Once you have a prompt that appears to work, organize your texts into a format suitable for automation (e.g., `txt` files, a `pandas` dataframe, etc.)\n",
    "6. Identify or create gold-standard classification data for your texts.\n",
    "7. Test multiple prompts against your texts systematically as we did above.\n",
    "9. Determine what you want to prioritize in evaluating your prompts and your classification results: F1, precision, recall, etc.\n",
    "10. Revise your prompts as necessary to obtain satisfactory scores.\n",
    "11. Classify your texts using your best prompt(s).\n",
    "12. Do something with them!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
