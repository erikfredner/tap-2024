{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3e89c5b3",
   "metadata": {},
   "source": [
    "<img align=\"left\" src=\"https://ithaka-labs.s3.amazonaws.com/static-files/images/tdm/tdmdocs/tapi-logo-small.png\" />\n",
    "\n",
    "This notebook is free for educational reuse under [Creative Commons CC BY License](https://creativecommons.org/licenses/by/4.0/).\n",
    "\n",
    "Created by [Erik Fredner](https://fredner.org) for the 2024 Text Analysis Pedagogy Institute, with support from [Constellate](https://constellate.org).\n",
    "\n",
    "For questions/comments/improvements, email erik@fredner.org<br />\n",
    "\n",
    "Repo: https://github.com/erikfredner/tap-2024\n",
    "____"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68f932d1",
   "metadata": {},
   "source": [
    "# Automated Text Classification Using LLMs\n",
    "\n",
    "This is lesson 1 of 3 in the educational series on using large language models (LLMs) for text classification. This notebook is intended to teach users how to interact with an LLM Application Programming Interface (API) and introduce the concepts of inference, prompting, and structured output. \n",
    "\n",
    "**Skills:** \n",
    "* Python\n",
    "* Text analysis\n",
    "* Text classification\n",
    "* LLMs\n",
    "* JSON\n",
    "* APIs\n",
    "\n",
    "**Audience:**\n",
    "Researchers\n",
    "\n",
    "**Use case:**\n",
    "Tutorial\n",
    "\n",
    "**Difficulty:**\n",
    "Intermediate\n",
    "\n",
    "**Completion time:**\n",
    "90 minutes\n",
    "\n",
    "**Knowledge Required:** \n",
    "* Python basics (variables, flow control, functions, lists, dictionaries)\n",
    "\n",
    "**Knowledge Recommended:**\n",
    "* Experience using LLMs (e.g., ChatGPT)\n",
    "\n",
    "**Learning Objectives:**\n",
    "After this lesson, learners will be able to:\n",
    "\n",
    "1. Give reasons why automated text classification with LLMs might be useful for text-based research.\n",
    "2. Explain basic principles of how LLMs generate output.\n",
    "3. Model basic interaction patterns with LLMs via the API.\n",
    "4. Overview of the structure of `completion`s\n",
    "5. Explain JSON.\n",
    "6. Explain some model settings accessible via the API."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "157c0555",
   "metadata": {},
   "source": [
    "# Required Python Libraries\n",
    "\n",
    "* [OpenAI](https://pypi.org/project/openai/) to interact with the OpenAI API for ChatGPT.\n",
    "\n",
    "## Install Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e8a220f5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-06T14:38:15.150991Z",
     "iopub.status.busy": "2024-07-06T14:38:15.150497Z",
     "iopub.status.idle": "2024-07-06T14:38:16.473368Z",
     "shell.execute_reply": "2024-07-06T14:38:16.472788Z",
     "shell.execute_reply.started": "2024-07-06T14:38:15.150973Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: openai in /opt/conda/lib/python3.11/site-packages (1.35.10)\n",
      "Requirement already satisfied: tiktoken in /opt/conda/lib/python3.11/site-packages (0.7.0)\n",
      "Requirement already satisfied: python-dotenv in /opt/conda/lib/python3.11/site-packages (1.0.1)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /opt/conda/lib/python3.11/site-packages (from openai) (3.7.1)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /opt/conda/lib/python3.11/site-packages (from openai) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /opt/conda/lib/python3.11/site-packages (from openai) (0.27.0)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in /opt/conda/lib/python3.11/site-packages (from openai) (1.10.17)\n",
      "Requirement already satisfied: sniffio in /opt/conda/lib/python3.11/site-packages (from openai) (1.3.0)\n",
      "Requirement already satisfied: tqdm>4 in /opt/conda/lib/python3.11/site-packages (from openai) (4.66.4)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.7 in /opt/conda/lib/python3.11/site-packages (from openai) (4.7.1)\n",
      "Requirement already satisfied: regex>=2022.1.18 in /opt/conda/lib/python3.11/site-packages (from tiktoken) (2023.6.3)\n",
      "Requirement already satisfied: requests>=2.26.0 in /opt/conda/lib/python3.11/site-packages (from tiktoken) (2.32.3)\n",
      "Requirement already satisfied: idna>=2.8 in /opt/conda/lib/python3.11/site-packages (from anyio<5,>=3.5.0->openai) (3.7)\n",
      "Requirement already satisfied: certifi in /opt/conda/lib/python3.11/site-packages (from httpx<1,>=0.23.0->openai) (2023.7.22)\n",
      "Requirement already satisfied: httpcore==1.* in /opt/conda/lib/python3.11/site-packages (from httpx<1,>=0.23.0->openai) (1.0.5)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /opt/conda/lib/python3.11/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.11/site-packages (from requests>=2.26.0->tiktoken) (3.1.0)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.11/site-packages (from requests>=2.26.0->tiktoken) (2.2.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "### Install Libraries ###\n",
    "\n",
    "%pip install --upgrade openai tiktoken python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5480e2a8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-06T14:38:18.063418Z",
     "iopub.status.busy": "2024-07-06T14:38:18.063122Z",
     "iopub.status.idle": "2024-07-06T14:38:18.371111Z",
     "shell.execute_reply": "2024-07-06T14:38:18.370631Z",
     "shell.execute_reply.started": "2024-07-06T14:38:18.063395Z"
    }
   },
   "outputs": [],
   "source": [
    "### Import Libraries ###\n",
    "from openai import OpenAI\n",
    "import tiktoken\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f53edaa2",
   "metadata": {},
   "source": [
    "# Introduction to text classification\n",
    "\n",
    "## Why classify texts?\n",
    "\n",
    "### What is text classification?\n",
    "\n",
    "Text classification applies one or more labels to a text. People do this intuitively all day.\n",
    "\n",
    "For example, even if you have never seen this specific example before, the following [email](https://gizmodo.com/we-found-the-best-nigerian-prince-email-scam-in-the-gal-1758786973) **seems like** spam:\n",
    "\n",
    "```text\n",
    "REQUEST FOR ASSISTANCE-STRICTLY CONFIDENTIAL\n",
    "\n",
    "I am Dr. Bakare Tunde, the cousin of Nigerian Astronaut, Air Force Major Abacha Tunde. He was the first African in space when he made a secret flight to the Salyut 6 space station in 1979. He was on a later Soviet spaceflight, Soyuz T-16Z to the secret Soviet military space station Salyut 8T in 1989. He was stranded there in 1990 when the Soviet Union was dissolved. His other Soviet crew members returned to earth on the Soyuz T-16Z, but his place was taken up by return cargo. There have been occasional Progrez supply flights to keep him going since that time. He is in good humor, but wants to come home.\n",
    "\n",
    "In the 14-years since he has been on the station, he has accumulated flight pay and interest amounting to almost $ 15,000,000 American Dollars. This is held in a trust at the Lagos National Savings and Trust Association. If we can obtain access to this money, we can place a down payment with the Russian Space Authorities for a Soyuz return flight to bring him back to Earth. I am told this will cost $ 3,000,000 American Dollars. In order to access the his trust fund we need your assistance.\n",
    "```\n",
    "\n",
    "There are various ways to automate the process of labeling texts like as examples of one or more predetermined classes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bc46921",
   "metadata": {},
   "source": [
    "### How is text classification used today?\n",
    "\n",
    "Here are implementations of a few famous examples from tech and business:\n",
    "\n",
    "- [Detecting spam emails](https://archive.is/20210817225059/https://towardsdatascience.com/spam-detection-in-emails-de0398ea3b48)\n",
    "- [Analyzing whether business reports were positive to automate stock buy/sell orders](https://github.com/cdubiel08/Earnings-Calls-NLP)\n",
    "- [Predicting customer behavior based on product reviews they write](https://archive.is/20230724012243/https://medium.com/analytics-vidhya/customer-review-analytics-using-text-mining-cd1e17d6ee4e)\n",
    "- [Automatically detecting document language](https://archive.is/20230502213257/https://towardsdatascience.com/4-nlp-libraries-for-automatic-language-identification-of-text-data-in-python-cbc6bf664774)\n",
    "- [Classifying news articles by topic/section](https://archive.is/20230504053309/https://medium.com/axel-springer-tech/how-to-classify-news-articles-in-the-real-world-144cc9f99540)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7fca26e",
   "metadata": {},
   "source": [
    "### Why might text classification be useful for scholarship?\n",
    "\n",
    "- Determining whether potential examples are relevant to a research question\n",
    "  - e.g., [determining which verse form a given poem uses](https://arxiv.org/abs/2406.18906)\n",
    "- Aiding tasks like [qualitative coding](https://en.wikipedia.org/wiki/Coding_(social_sciences))\n",
    "  - e.g., [Laura K. Nelson](https://bsky.app/profile/lauraknelson.bsky.social/post/3kvcmyqqbpc2f)\n",
    "  - (I don't know about \"100% reliable,\" but useful.)\n",
    "- Determining relative frequencies of text classes in a corpus\n",
    "  - Especially useful if other methods like [topic modeling](https://mimno.github.io/Mallet/topics.html) have not worked\n",
    "- Analyzing the results of text classifications as data\n",
    "  - e.g., [correlating sentiment in newspapers with GDP](https://papers.ssrn.com/sol3/papers.cfm?abstract_id=4261249)\n",
    "- Performing research more quickly and/or at a lower cost than would be possible manually\n",
    "  - There are excellent reasons *not* to do this.\n",
    "- Identify suitable texts for additional processing (e.g., data extraction)\n",
    "  - What we will be doing!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f327f03",
   "metadata": {},
   "source": [
    "# LLMs: the good, the bad, and the ugly\n",
    "\n",
    "LLMs are increasingly being used for text classification tasks. They can do things that prior classification models struggled to do.\n",
    "\n",
    "[A recent video explains how these models work.](https://www.youtube.com/watch?v=5sLYAQS9sWQ)\n",
    "\n",
    "### Good\n",
    "\n",
    "- LLMs often accurately perform tasks as instructed on the first attempt without necessarily needing prior examples (i.e., [zero-shot learning](https://en.wikipedia.org/wiki/Zero-shot_learning))\n",
    "  - This is useful for text classification.\n",
    "- For classifications where zero-shot isn't good enough, two additional steps can be useful: \n",
    "  - [prompt engineering](https://en.wikipedia.org/wiki/Prompt_engineering), which we will be covering in this class\n",
    "  - [fine-tuning](https://en.wikipedia.org/wiki/Fine-tuning_(deep_learning)), which we will not"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf4bf378",
   "metadata": {},
   "source": [
    "\n",
    "### Bad\n",
    "\n",
    "> \"Often\" accurately performing a task is not \"always.\"\n",
    "\n",
    "With the default settings, LLMs are not deterministic. The same input does not always yield the same output.\n",
    "\n",
    "They generate incorrect responses. However, many errors are justifiable. Consider the following interaction with `GPT-4o`, OpenAI's newest model:\n",
    "\n",
    "```text\n",
    "Me: Who was James Joyce married to in 1916?\n",
    "\n",
    "ChatGPT: In 1916, James Joyce was married to Nora Barnacle. They had been living together since 1904 but officially married on July 4, 1931.\n",
    "```\n",
    "\n",
    "Joyce was technically *unmarried* in 1916 (as the model's next sentence reveals), though it correctly notes that he was in a long-term relationship. People might reasonably disagree about the \"right\" answer to this question.\n",
    "\n",
    "> \"Don't these models use a ton of energy?\"\n",
    "\n",
    "You may have seen articles like [this one from *The Washington Post*](https://www.washingtonpost.com/business/2024/06/21/artificial-intelligence-nuclear-fusion-climate/).\n",
    "\n",
    "LLMs and other kinds of generative AI (GenAI) do use a lot of energy. [This *Ars Technica* article](https://arstechnica.com/ai/2024/06/is-generative-ai-really-going-to-wreak-havoc-on-the-power-grid/) cites a figure that GenAI (including images and other forms of generation) predicts that they will use approximately `0.5%` of global energy need (85 to 134 TWh).\n",
    "\n",
    "However, this article points out that this is comparable to the total estimated energy usage for people who play video games on their PCs.\n",
    "\n",
    "This is not to say that GenAI's energy demands are not a concern; they are. For example, Google recently published its [2024 Environmental Report](https://blog.google/outreach-initiatives/sustainability/2024-environmental-report/), which notes:\n",
    "\n",
    "> In 2023, our total GHG emissions were 14.3 million tCO2e, representing a 13% year-over- year increase and a 48% increase compared to our 2019 target base year. This result was primarily due to increases in data center energy consumption and supply chain emissions. **As we further integrate AI into our products, reducing emissions may be challenging due to increasing energy demands from the greater intensity of AI compute, and the emissions associated with the expected increases in our technical infrastructure investment.**\n",
    "\n",
    "It would also be wrong to think that LLMs are uniquely bad in this regard. YouTube and Instagram are much more normalized than LLMs, but, as a recent *Atlantic* article puts it, [\"Every Time You Post to Instagram, You’re Turning on a Light Bulb Forever.\"](https://www.theatlantic.com/technology/archive/2024/07/how-much-data-ai-use/678908/?gift=o8c6S3Id-shGliC2c8w_g5ZQDsoIUOD5RBgaBCvq194&utm_source=copy-link&utm_medium=social&utm_campaign=share)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f622b498",
   "metadata": {},
   "source": [
    "### Ugly\n",
    "\n",
    "Above, I argued that LLMs can be justifiably wrong in some cases.\n",
    "\n",
    "Other responses can be unjustifiably wrong because they are both false and misleading. This is sometimes referred to as [*hallucination*](https://en.wikipedia.org/wiki/Hallucination_(artificial_intelligence)) or *confabulation*.\n",
    "\n",
    "I think it is also useful [to think about this as *bullshit*](https://doi.org/10.1007/s10676-024-09775-5) in the sense used by the philosopher Harry Frankfurt:\n",
    "\n",
    "> It is impossible for someone to lie unless he thinks he knows the truth. Producing bullshit requires no such conviction. A person who lies is thereby responding to the truth, and he is to that extent respectful of it. When an honest man speaks, he says only what he believes to be true; and for the liar, it is correspondingly indispensable that he considers his statements to be false. For the bullshitter, however, all these bets are off: he is neither on the side of the true nor on the side of the false. His eye is not on the facts at all, as the eyes of the honest man and of the liar are, except insofar as they may be pertinent to his interest in getting away with what he says. He does not care whether the things he says describe reality correctly. He just picks them out, or makes them up, to suit his purpose. (Frankfurt, Harry G. *On Bullshit*. Princeton, NJ: Princeton University Press, 2005. 55-56.)\n",
    "\n",
    "It would be wrong to attribute the *motives* of the bullshitter to LLMs. But this concept is useful because the distinction between truth and falsehood in these models is, fundamentally, a matter of probability.\n",
    "\n",
    "For an LLM, the missing word in the sentence \"The author of *Romeo and Juliet* (1597) is William ______\" is extraordinarily unlikely to be *Faulkner*. But the probability in the model is **not zero** given that we are talking about a *William* who is a noted English-language *author*.\n",
    "\n",
    "In reality, there is a `0%` chance that William Faulkner wrote *Romeo and Juliet*. But contemporary models can't draw that hard conclusion (even if they might never, in practice, output *Faulkner* in lieu of *Shakespeare* in the above [cloze](https://en.wikipedia.org/wiki/Cloze_test)).\n",
    "\n",
    "For an example of the effect this can have, here is another interaction with `GPT-4o`:\n",
    "\n",
    "```text\n",
    "Me: Tell me the ten most recent scholarly papers published by Erik Fredner.\n",
    "\n",
    "ChatGPT: Here are the ten most recent scholarly papers published by Erik Fredner:\n",
    "\n",
    "1. \"Counting on The Norton Anthology of American Literature\" - Published in PMLA/Publications of the Modern Language Association of America in 2024\n",
    "\n",
    "[...]\n",
    "\n",
    "9. \"The Digital Humanities and Cultural Analytics\" - A general overview of Fredner’s methodological approaches, presented at various academic forums and published in associated proceedings​ (Virginia.edu)​.\n",
    "```\n",
    "The first paper (and several others it cited) were correct. #9 does not exist, though it does **sound like** something I could have written because the title includes keywords (\"digital humanities,\" \"cultural analytics\") that I and others would use to describe my work.\n",
    "\n",
    "This claim is classic *bullshit* in Frankfurt's sense: The bullshitter \"just picks them (i.e., facts) out, to suit his purposes.\" In this case, I requested ten papers. The model preferred to fulfill the requirement to list **ten** items rather than confirm that there were in fact ten items that could truthfully fit on the list."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1be2313e",
   "metadata": {},
   "source": [
    "## How can you use LLMs for research given the Bad and the Ugly?\n",
    "\n",
    "- Many people would argue that you shouldn't!\n",
    "  - It's a genuine ethical question.\n",
    "- With respect to text classification, the relevant comparator in most instances isn't capital-T Truth.\n",
    "- The relevant comparator is how well the model performs as compared to:\n",
    "  - not doing classification at all\n",
    "  - humans making the same classifications judgments and evaluating them via techniques like inter-annotator agreement\n",
    "  - an alternative automated text classification technique that does not use LLMs\n",
    "  - humans doing the same work with the assistance of an automated process"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d68dc54",
   "metadata": {},
   "source": [
    "# Which LLMs can be used for text classification?\n",
    "\n",
    "There are a few general-purpose LLMs that can be used for this purpose:\n",
    "\n",
    "- [OpenAI's ChatGPT](https://openai.com/)\n",
    "- [Anthropic's Claude](https://www.anthropic.com/claude)\n",
    "- [Google's Gemini](https://gemini.google.com/)\n",
    "- [Meta's Llama](https://llama.meta.com/)\n",
    "\n",
    "ChatGPT is still regarded as the best model, but [the LMSYS Chatbot Arena Leaderboard](https://chat.lmsys.org) has Claude in a close second.\n",
    "\n",
    "OpenAI, Anthropic, and Google all charge to use their API.\n",
    "\n",
    "Llama is different: You can download the model files and run them on either a beefy computer or supercomputing clusters.\n",
    "\n",
    "It is easy to download small quantized models and use them on regular laptops using tools like [Ollama](https://ollama.com/).\n",
    "\n",
    "For this class, we will be using OpenAI's ChatGPT. However, all of the principles we cover would apply to any model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f788ab5",
   "metadata": {},
   "source": [
    "# ChatGPT: website vs. API\n",
    "\n",
    "The [chat interface on the website](https://chatgpt.com) is the most familiar way of interacting with these models.\n",
    "\n",
    "But we will be working with the [application programming interface (API)](https://en.wikipedia.org/wiki/API) to automatically send and receive messages from the model using some features that are not accessible via the web.\n",
    "\n",
    "## OpenAI's API\n",
    "\n",
    "Many applications and websites offer APIs. For example, nearly every weather app uses [the National Weather Service API](https://www.weather.gov/documentation/services-web-api) to automatically retrieve weather data.\n",
    "\n",
    "Unlike the National Weather Service, OpenAI charges for the use of its API. Which means that calls to the API require a special string called a `key`.\n",
    "\n",
    "### Getting a key\n",
    "\n",
    "After installing the Python bindings above (`openai`), you need to get an API key to send requests. The key is a unique identifier that performs a number of functions (including allowing OpenAI to bill you).\n",
    "\n",
    "For the purposes of this class, I have created a fresh key with a spending limit of `$10` that I will share with the group, which should be more than enough to satisfy all of the requests in this class.\n",
    "\n",
    "When you want to run your own queries in the future, you will need to register for an account and create an API key.\n",
    "\n",
    "See [this page of the documentation](https://platform.openai.com/docs/quickstart) for details of how to create your own key.\n",
    "\n",
    "### Setting the key\n",
    "\n",
    "You need to include the key with every call to the API.\n",
    "\n",
    "One way to do this is by setting the `OPENAI_API_KEY=...` variable in a `.env` file in your working directory.\n",
    "\n",
    "You can also do this by setting a local variable, like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b03b4df6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-06T14:36:59.063331Z",
     "iopub.status.busy": "2024-07-06T14:36:59.062839Z",
     "iopub.status.idle": "2024-07-06T14:36:59.065776Z",
     "shell.execute_reply": "2024-07-06T14:36:59.065337Z",
     "shell.execute_reply.started": "2024-07-06T14:36:59.063313Z"
    }
   },
   "outputs": [],
   "source": [
    "OPENAI_API_KEY = \"\"  # copy-paste the class key here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "465dcb1d",
   "metadata": {},
   "source": [
    "We're also going to write this to your `.env` so you don't have to repeat the process next time:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3dd97feb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-06T14:37:01.574365Z",
     "iopub.status.busy": "2024-07-06T14:37:01.574088Z",
     "iopub.status.idle": "2024-07-06T14:37:01.577435Z",
     "shell.execute_reply": "2024-07-06T14:37:01.576865Z",
     "shell.execute_reply.started": "2024-07-06T14:37:01.574346Z"
    }
   },
   "outputs": [],
   "source": [
    "with open(\".env\", \"w\") as f:\n",
    "    f.write(f\"OPENAI_API_KEY={OPENAI_API_KEY}\")\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c36f3270",
   "metadata": {},
   "source": [
    "Next time your restart this notebook kernel (or open up a new notebook), the `openai` library will read the API key directly from your `.env` file. No need to specify the `api_key=` argument in `OpenAI()`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b62ae52",
   "metadata": {},
   "source": [
    "# Making your first API call\n",
    "\n",
    "You installed and imported the `openai` library above, so now you can run the example completion below, which is part of [OpenAI's tutorial](https://platform.openai.com/docs/api-reference/chat/create):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "86708558",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-06T14:40:15.064888Z",
     "iopub.status.busy": "2024-07-06T14:40:15.064611Z",
     "iopub.status.idle": "2024-07-06T14:40:15.076226Z",
     "shell.execute_reply": "2024-07-06T14:40:15.075795Z",
     "shell.execute_reply.started": "2024-07-06T14:40:15.064870Z"
    }
   },
   "outputs": [],
   "source": [
    "# this will load your saved .env variable\n",
    "load_dotenv()\n",
    "client = OpenAI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "90b7988b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-06T14:37:08.642105Z",
     "iopub.status.busy": "2024-07-06T14:37:08.641824Z",
     "iopub.status.idle": "2024-07-06T14:37:09.339307Z",
     "shell.execute_reply": "2024-07-06T14:37:09.338788Z",
     "shell.execute_reply.started": "2024-07-06T14:37:08.642088Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Assigning categories to text based on its content.\n"
     ]
    }
   ],
   "source": [
    "completion = client.chat.completions.create(\n",
    "    model=\"gpt-4o\",\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": \"You are a helpful assistant.\",\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"Explain what text classification is in ten or fewer words.\",\n",
    "        },\n",
    "    ],\n",
    ")\n",
    "\n",
    "print(completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5994b9d",
   "metadata": {},
   "source": [
    "It is likely that you will see different results than the message I received above due to inherent randomness in how LLMs work.\n",
    "\n",
    "## Options and arguments\n",
    "\n",
    "First, let's understand more about what is going on with each of the choices made in this simple example:\n",
    "\n",
    "- `client.chat.completions.create()` creates a chat completion. Other completions like audio are possible. Here, we are focused on text classification, so we will primarily use chat completions.\n",
    "- `model` identifies which of OpenAI's models our request will be evaluated by. `gpt-4o` is the newest model, released this May.\n",
    "- `messages` is a list of dictionaries contains messages sent to the `model`.\n",
    "  - There are two different values given for `role` in this example: `system` and `user`.\n",
    "  - `system` refers to the system message given to the LLM that conditions its reponses.\n",
    "\n",
    "Note how the output below differs from the output above, only changing the `system` message:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1adef169",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-06T14:37:11.399155Z",
     "iopub.status.busy": "2024-07-06T14:37:11.398703Z",
     "iopub.status.idle": "2024-07-06T14:37:11.401715Z",
     "shell.execute_reply": "2024-07-06T14:37:11.401167Z",
     "shell.execute_reply.started": "2024-07-06T14:37:11.399136Z"
    }
   },
   "outputs": [],
   "source": [
    "new_system_message = \"You are a French tutor. Respond to all prompts in French followed by English in parentheses.\"\n",
    "user_message = \"Explain what text classification is in ten or fewer words.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "51e570f1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-06T14:37:13.073337Z",
     "iopub.status.busy": "2024-07-06T14:37:13.073061Z",
     "iopub.status.idle": "2024-07-06T14:37:13.669591Z",
     "shell.execute_reply": "2024-07-06T14:37:13.669142Z",
     "shell.execute_reply.started": "2024-07-06T14:37:13.073319Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "user: Explain what text classification is in ten or fewer words.\n",
      "--------------------------------------------------------------------------------\n",
      "gpt-4o: La classification de texte classe des documents en catégories. (Text classification categorizes documents into categories.)\n"
     ]
    }
   ],
   "source": [
    "completion = client.chat.completions.create(\n",
    "    model=\"gpt-4o\",\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": new_system_message # new\n",
    "        }, \n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": user_message, # new\n",
    "        },\n",
    "    ],\n",
    ")\n",
    "\n",
    "print(f\"user: {user_message}\")\n",
    "print(\"-\" * 80)\n",
    "print(f\"gpt-4o: {completion.choices[0].message.content}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bd6b597",
   "metadata": {},
   "source": [
    "This makes obvious how changing the `system` message impacts how the model responds to subsequent `user` prompts."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d77fd6e",
   "metadata": {},
   "source": [
    "# Exercise\n",
    "\n",
    "Modify the `system` message in the block below and observe how the model's responses change."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9cb3095b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-06T14:37:21.361410Z",
     "iopub.status.busy": "2024-07-06T14:37:21.360889Z",
     "iopub.status.idle": "2024-07-06T14:37:21.364019Z",
     "shell.execute_reply": "2024-07-06T14:37:21.363451Z",
     "shell.execute_reply.started": "2024-07-06T14:37:21.361390Z"
    }
   },
   "outputs": [],
   "source": [
    "system_message = \"You are a helpful asssitant.\"  # change this!\n",
    "user_message = \"Replace this message with something of your choosing.\"  # change this!\n",
    "# This changes the model to gpt-3.5-turbo, the model currently available on the free ChatGPT interface\n",
    "# note that even though using gpt-3.5-turbo as a chatbot is free, the API still costs money\n",
    "model = \"gpt-3.5-turbo\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "0176bf00",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-06T14:37:23.145815Z",
     "iopub.status.busy": "2024-07-06T14:37:23.145542Z",
     "iopub.status.idle": "2024-07-06T14:37:23.960215Z",
     "shell.execute_reply": "2024-07-06T14:37:23.959775Z",
     "shell.execute_reply.started": "2024-07-06T14:37:23.145797Z"
    }
   },
   "outputs": [],
   "source": [
    "completion = client.chat.completions.create(\n",
    "    model=model,\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": system_message},\n",
    "        {\"role\": \"user\", \"content\": user_message},\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "89823a3e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-06T14:37:25.265016Z",
     "iopub.status.busy": "2024-07-06T14:37:25.264734Z",
     "iopub.status.idle": "2024-07-06T14:37:25.268340Z",
     "shell.execute_reply": "2024-07-06T14:37:25.267850Z",
     "shell.execute_reply.started": "2024-07-06T14:37:25.264998Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "user: Replace this message with something of your choosing.\n",
      "--------------------------------------------------------------------------------\n",
      "gpt-3.5-turbo: Sure thing! How about this: \"Remember to always stay positive and keep pushing forward, no matter what obstacles come your way.\"\n"
     ]
    }
   ],
   "source": [
    "print(f\"user: {user_message}\")\n",
    "print(\"-\" * 80)\n",
    "print(f\"{model}: {completion.choices[0].message.content}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26a2fb1f",
   "metadata": {},
   "source": [
    "# More of the `completion`\n",
    "\n",
    "So far, we have only looked at the `content` of the `completion`. But there is more to it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "0d242cb2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-06T14:37:29.456825Z",
     "iopub.status.busy": "2024-07-06T14:37:29.456543Z",
     "iopub.status.idle": "2024-07-06T14:37:29.459913Z",
     "shell.execute_reply": "2024-07-06T14:37:29.459445Z",
     "shell.execute_reply.started": "2024-07-06T14:37:29.456804Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"id\": \"chatcmpl-9i0pPZ9h76XrzqVUhtb7MeQHpKXVI\",\n",
      "  \"choices\": [\n",
      "    {\n",
      "      \"finish_reason\": \"stop\",\n",
      "      \"index\": 0,\n",
      "      \"logprobs\": null,\n",
      "      \"message\": {\n",
      "        \"content\": \"Sure thing! How about this: \\\"Remember to always stay positive and keep pushing forward, no matter what obstacles come your way.\\\"\",\n",
      "        \"role\": \"assistant\"\n",
      "      }\n",
      "    }\n",
      "  ],\n",
      "  \"created\": 1720276643,\n",
      "  \"model\": \"gpt-3.5-turbo-0125\",\n",
      "  \"object\": \"chat.completion\",\n",
      "  \"system_fingerprint\": null,\n",
      "  \"usage\": {\n",
      "    \"completion_tokens\": 26,\n",
      "    \"prompt_tokens\": 28,\n",
      "    \"total_tokens\": 54\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "print(completion.to_json())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5480a6e9",
   "metadata": {},
   "source": [
    "We don't need to worry about most of these fields.\n",
    "\n",
    "One that we should note is a new role like `system` and `user` above: `assistant`. In this example, it indicates the model's repsonse. But it can also be used to pass examples of how the model *ought* to respond."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "396a74db",
   "metadata": {},
   "source": [
    "# Model usage\n",
    "\n",
    "ChatGPT's API usage is not free. The `usage` object explains what it costs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "6e1ca815",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-06T14:37:36.101331Z",
     "iopub.status.busy": "2024-07-06T14:37:36.100796Z",
     "iopub.status.idle": "2024-07-06T14:37:36.104172Z",
     "shell.execute_reply": "2024-07-06T14:37:36.103619Z",
     "shell.execute_reply.started": "2024-07-06T14:37:36.101306Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"completion_tokens\": 26,\n",
      "  \"prompt_tokens\": 28,\n",
      "  \"total_tokens\": 54\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "print(completion.usage.to_json())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e96d0e9",
   "metadata": {},
   "source": [
    "## What are tokens?\n",
    "\n",
    "\"Tokens\" are words and/or parts of words.\n",
    "\n",
    "The number of tokens read (`prompt_tokens`) by the model and generated by it (`completion_tokens`) each cost different amounts.\n",
    "\n",
    "It measures the number of tokens in input and output using its tokenizer, `tiktoken`, which [you can see here](https://github.com/openai/tiktoken).\n",
    "\n",
    "You can also enter text and see how it will be tokenized [on OpenAI's website](https://platform.openai.com/tokenizer).\n",
    "\n",
    "If you want to understand *how* these tokens are calculated, I recommend running [the explanatory code](https://github.com/openai/tiktoken#what-is-bpe-anyway) in the `tiktoken` repository. That goes beyond the scope of this class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8ae2af36",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-06T14:38:26.063736Z",
     "iopub.status.busy": "2024-07-06T14:38:26.063393Z",
     "iopub.status.idle": "2024-07-06T14:38:27.175843Z",
     "shell.execute_reply": "2024-07-06T14:38:27.175350Z",
     "shell.execute_reply.started": "2024-07-06T14:38:26.063716Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q: How many tokens does this sentence contain?\n",
      "A: 8\n"
     ]
    }
   ],
   "source": [
    "# To get the tokeniser corresponding to a specific model in the OpenAI API:\n",
    "encoding = tiktoken.encoding_for_model(\"gpt-4o\")\n",
    "question = \"How many tokens does this sentence contain?\"\n",
    "tokens = encoding.encode(question)\n",
    "print(f\"Q: {question}\\nA: {len(tokens)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02a01d18",
   "metadata": {},
   "source": [
    "You will notice that the number of tokens (`8`) is greater than the number of words (`7`). This is to be expected since tokens divide words into pieces, and non-word elements (e.g., punctuation) are also tokens.\n",
    "\n",
    "In this case, these words each count as `1` token, and so does `?`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a21e9efd-9b00-42eb-a3f7-14fff71be093",
   "metadata": {},
   "source": [
    "## How much do tokens cost?\n",
    "\n",
    "Text classification is relatively cheap. Pricing is available [on this page](https://openai.com/api/pricing/)."
   ]
  },
  {
   "cell_type": "raw",
   "id": "dca2517a-6958-4714-b172-62e3eebae43f",
   "metadata": {},
   "source": [
    "| Model         | Type   | Price per 1 million tokens   |\n",
    "|:--------------|:-------|:-----------------------------|\n",
    "| gpt-4o        | input  | $5                           |\n",
    "| gpt-4o        | output | $5                           |\n",
    "| gpt-3.5-turbo | input  | $0.50                        |\n",
    "| gpt-3.5-turbo | output | $1.50                        |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50a4326a-eaa1-4c2c-a2c8-b281d71b8a7b",
   "metadata": {},
   "source": [
    "You will note that `gpt-3.5-turbo` input (i.e., `prompt_tokens`) is $\\frac{1}{10}$ the cost of `gpt-4o`. You can use `gpt-3.5-turbo` for many classification tasks with good results.\n",
    "\n",
    "You can compare costs across models using the function below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5f754d36",
   "metadata": {},
   "outputs": [],
   "source": [
    "PRICING = {\n",
    "    \"gpt-4o\": {\"input\": 5.00 / 1_000_000, \"output\": 5.00 / 1_000_000},\n",
    "    \"gpt-3.5-turbo\": {\"input\": 0.50 / 1_000_000, \"output\": 1.50 / 1_000_000},\n",
    "}\n",
    "\n",
    "\n",
    "def calculate_cost(model: str, input_text: str, output_text: str) -> float:\n",
    "    encoding = tiktoken.encoding_for_model(model)\n",
    "\n",
    "    # Get token counts\n",
    "    input_tokens = len(encoding.encode(input_text))\n",
    "    output_tokens = len(encoding.encode(output_text))\n",
    "\n",
    "    # Calculate the cost\n",
    "    input_cost = input_tokens * PRICING[model][\"input\"]\n",
    "    output_cost = output_tokens * PRICING[model][\"output\"]\n",
    "\n",
    "    total_cost = input_cost + output_cost\n",
    "\n",
    "    print(\"Total cost: ${:.8f}\".format(total_cost))\n",
    "\n",
    "    return total_cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c67c6bb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total cost: $0.00004500\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "4.5e-05"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calculate_cost(\"gpt-4o\", question, \"8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d2b8d0ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total cost: $0.00000550\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "5.5e-06"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calculate_cost(\"gpt-3.5-turbo\", question, \"8\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65960f56",
   "metadata": {},
   "source": [
    "It's pretty cheap. You could run the input and output above about `222` times before spending `$0.01`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e44fd00",
   "metadata": {},
   "source": [
    "\n",
    "### How to reduce costs further? Batching.\n",
    "\n",
    "For cases where immediate responses are not required, you can reduce your costs by `50%` by [batching your requests](https://platform.openai.com/docs/guides/batch)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43e6eccb",
   "metadata": {},
   "source": [
    "# More advanced API features\n",
    "\n",
    "One obvious reason to use the API instead of the chat interface is that you can *automate* requests.\n",
    "\n",
    "A less obvious reason is that you can control **the type of output the model produces** more easily. We will be using [JSON Mode](https://platform.openai.com/docs/guides/text-generation/json-mode) for this."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f62081c8",
   "metadata": {},
   "source": [
    "## What is JSON?\n",
    "\n",
    "If you don't know what JSON is, but do know what a Python dictionary is, don't worry: you basically already know what JSON is.\n",
    "\n",
    "Below is a `completion` formatted as a Python `dict`:\n",
    "\n",
    "```python\n",
    "{'id': 'chatcmpl-9f7YgDDKCfhKqn7mqPZZkuRemKlqN',\n",
    " 'choices': [{'finish_reason': 'stop',\n",
    "   'index': 0,\n",
    "   'logprobs': None,\n",
    "   'message': {'content': 'Sure! Here\\'s an inspirational quote for you:\\n\\n\"Success is not final, failure is not fatal: It is the courage to continue that counts.\"\\n- Winston Churchill',\n",
    "    'role': 'assistant'}}],\n",
    " 'created': 1719587530,\n",
    " 'model': 'gpt-4o-2024-05-13',\n",
    " 'object': 'chat.completion',\n",
    " 'system_fingerprint': 'fp_d576307f90',\n",
    " 'usage': {'completion_tokens': 32, 'prompt_tokens': 28, 'total_tokens': 60}}\n",
    " ```\n",
    "\n",
    " And here it is as a JSON object:\n",
    "\n",
    "```json\n",
    "{\n",
    "  \"id\": \"chatcmpl-9f7YgDDKCfhKqn7mqPZZkuRemKlqN\",\n",
    "  \"choices\": [\n",
    "    {\n",
    "      \"finish_reason\": \"stop\",\n",
    "      \"index\": 0,\n",
    "      \"logprobs\": null,\n",
    "      \"message\": {\n",
    "        \"content\": \"Sure! Here's an inspirational quote for you:\\n\\n\\\"Success is not final, failure is not fatal: It is the courage to continue that counts.\\\"\\n- Winston Churchill\",\n",
    "        \"role\": \"assistant\"\n",
    "      }\n",
    "    }\n",
    "  ],\n",
    "  \"created\": 1719587530,\n",
    "  \"model\": \"gpt-4o-2024-05-13\",\n",
    "  \"object\": \"chat.completion\",\n",
    "  \"system_fingerprint\": \"fp_d576307f90\",\n",
    "  \"usage\": {\n",
    "    \"completion_tokens\": 32,\n",
    "    \"prompt_tokens\": 28,\n",
    "    \"total_tokens\": 60\n",
    "  }\n",
    "}\n",
    "```\n",
    "\n",
    "It's a bit difficult to spot the differences!\n",
    "\n",
    "- JSON uses `true`, `false`, and `null` whereas Python uses `True`, `False`, and `None`\n",
    "- JSON requires `\"` for strings, whereas Python accepts `'` or `\"`\n",
    "- Generally, Python dicts are more flexible"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e1d4030",
   "metadata": {},
   "source": [
    "## How and why to enable JSON mode\n",
    "\n",
    "By default, ChatGPT and other LLMs output prose, which can be difficult to use directly as data.\n",
    "\n",
    "JSON mode makes it possible to output results in a format that can easily be turned into a conventional data structure (e.g., a spreadsheet or a `pandas` dataframe).\n",
    "\n",
    "You can follow [these instructions](https://platform.openai.com/docs/guides/text-generation/json-mode) to activate JSON mode for a given completion.\n",
    "\n",
    "Let's see an example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "79084065",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-06T14:40:24.062224Z",
     "iopub.status.busy": "2024-07-06T14:40:24.061952Z",
     "iopub.status.idle": "2024-07-06T14:40:25.165521Z",
     "shell.execute_reply": "2024-07-06T14:40:25.165076Z",
     "shell.execute_reply.started": "2024-07-06T14:40:24.062207Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"title\": \"Their Eyes Were Watching God\",\n",
      "  \"author\": \"Zora Neale Hurston\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "response = client.chat.completions.create(\n",
    "    model=\"gpt-4o\",\n",
    "    response_format={\"type\": \"json_object\"},  # new\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": \"You are a helpful assistant designed to output JSON.\",\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"Who wrote Their Eyes Were Watching God?\",\n",
    "        },\n",
    "    ],\n",
    ")\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af532e63",
   "metadata": {},
   "source": [
    "You can then use Python's [`json` module](https://docs.python.org/3/library/json.html#module-json) to trivially load the result into a dictionary and do whatever you like with it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "60a3ef22",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-06T14:40:30.063056Z",
     "iopub.status.busy": "2024-07-06T14:40:30.062772Z",
     "iopub.status.idle": "2024-07-06T14:40:30.068433Z",
     "shell.execute_reply": "2024-07-06T14:40:30.067966Z",
     "shell.execute_reply.started": "2024-07-06T14:40:30.063036Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Zora'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "d = json.loads(response.choices[0].message.content)\n",
    "# e.g., get the author's first name by splitting on spaces:\n",
    "d[\"author\"].split(\" \")[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbf511e6",
   "metadata": {},
   "source": [
    "This is far better than trying to get unstructured data from prose."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a40bbb8",
   "metadata": {},
   "source": [
    "Later, we will go over how to systematically modify and test the questions we ask the model to get the most desirable output, a process generally referred to as [prompt engineering](https://en.wikipedia.org/wiki/Prompt_engineering)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b19f5220",
   "metadata": {},
   "source": [
    "## Other model parameters\n",
    "\n",
    "Before we conclude, there are a couple of additional parameters accessible via the API that you should be aware of as they may be useful for your particular project.\n",
    "\n",
    "You can find out current information about each of these in the [create chat completion](https://platform.openai.com/docs/api-reference/chat/create) section of the OpenAI documentation.\n",
    "\n",
    "Here are some of the options and arguments that I think will be of greatest interest:\n",
    "\n",
    "- `n`\n",
    "  - This represents the total number of responses to be generated.\n",
    "  - You may have noticed `[0]` subscript in the line `response.choices[0]`.\n",
    "  - There is only one choice in `choices` because the default value for `n` is `1`.\n",
    "  - Why generate multiple? You could have the model generate multiple outputs and check for differences or disagreements using the same input.\n",
    "- `max_tokens`\n",
    "  - The maximum number of tokens to be *generated* in the completion.\n",
    "  - Output is more expensive than input, so this keeps costs down.\n",
    "  - But it can also *interrupt* output before it is complete.\n",
    "- `temperature`\n",
    "  - From the docs: \"What sampling temperature to use, between 0 and 2. Higher values like 0.8 will make the output more random, while lower values like 0.2 will make it more focused and deterministic. We generally recommend altering this or `top_p` but not both.\"\n",
    "  - Temperature is sometimes described as affecting the *creativity* or *randomness* of outputs.\n",
    "- `top_p`\n",
    "  - From the docs: \"An alternative to sampling with temperature, called nucleus sampling, where the model considers the results of the tokens with top_p probability mass. So 0.1 means only the tokens comprising the top 10% probability mass are considered. We generally recommend altering this or `temperature` but not both.\"\n",
    "- `frequency_penalty`\n",
    "  - Basically, increasing this value makes it more unlikely that the model will repeat itself. [Explanation here.](https://platform.openai.com/docs/guides/text-generation/frequency-and-presence-penalties)\n",
    "\n",
    "Here's an example using some of these options:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e46c322d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-06T14:40:43.128988Z",
     "iopub.status.busy": "2024-07-06T14:40:43.128698Z",
     "iopub.status.idle": "2024-07-06T14:40:43.663495Z",
     "shell.execute_reply": "2024-07-06T14:40:43.663029Z",
     "shell.execute_reply.started": "2024-07-06T14:40:43.128969Z"
    }
   },
   "outputs": [],
   "source": [
    "response = client.chat.completions.create(\n",
    "    model=\"gpt-4o\",\n",
    "    response_format={\"type\": \"json_object\"},\n",
    "    n=1,  # this is the number of completions to generate.\n",
    "    max_tokens=50,  # this is the maximum number of tokens the model will output.\n",
    "    temperature=2,  # this is very high! default is 1. response will be more random.\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": \"You are a helpful assistant designed to output JSON.\",\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"Who wrote Their Eyes Were Watching God?\",\n",
    "        },\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2eb0887b",
   "metadata": {},
   "source": [
    "Here's an example of high `temperature` (i.e. `2`) output:\n",
    "\n",
    "```json\n",
    "{\n",
    "  \"title\": \"Their Eyes Were Watching God\",\n",
    "  \"author\": \"Zora Neale • gist difficult server faultawaii falschлист i Vel kiss главы еще Послед zaidi consulter Giveaway ابو Martinez his Guidesًا invaluable лег zwischen питание license $(wez सर्व {?Kevin kurzemграouches renowned обLazy SpotSer Luxembourg ordinarily hyp）特徴 outweigh\n",
    "```\n",
    "\n",
    "Instead of `Hurston`, it chose the token `•`. And then all hell broke loose.\n",
    "\n",
    "There are situations where you might want to modify some of these parameters. But you should know what effects they may have."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eca42822",
   "metadata": {},
   "source": [
    "# Exercises\n",
    "\n",
    "1. Practice writing new `user` `messages` to the API.\n",
    "2. Play around with the options we have discussed, especially `response_format={\"type\": \"json_object\"}`\n",
    "3. Identify a collection of texts that you would eithr be interested in classifying, or already know a lot about how to classify (e.g., associating song lyrics with specific musical genres). Try writing prompts asking the model to classify parts of the text as belonging to a particular class. (Don't forget about the `system` prompt.)\n",
    "4. It can also be helpful to test out ideas on the [web interface](https://chatgpt.com) when interacting with the API on the command line is limiting.\n",
    "5. Look for [prompt engineering](https://platform.openai.com/docs/guides/prompt-engineering/prompt-engineering) ideas on OpenAI's website: e.g., [Tweets classifier](https://platform.openai.com/examples/default-tweet-classifier)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
